This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: src/core/provenance_manager.py, src/monitoring/performance_tracker.py, src/core/sla_monitor.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
src/
  core/
    provenance_manager.py
    sla_monitor.py
  monitoring/
    performance_tracker.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/core/provenance_manager.py">
"""
Provenance Manager for tracking citation sources and modifications.
Ensures every citation has a verifiable source and maintains a complete
audit trail of all modifications.
"""
import asyncio
import uuid
import hashlib
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
import json
import logging
logger = logging.getLogger(__name__)
@dataclass(frozen=True)
class AuditEntry:
    """Immutable audit entry with cryptographic chaining."""
    timestamp: str
    operation: str
    actor: str
    data: Dict[str, Any]
    previous_hash: str
    entry_hash: str = field(init=False)
    def __post_init__(self):
        """Calculate entry hash including previous hash for chaining."""
        content = json.dumps({
            "timestamp": self.timestamp,
            "operation": self.operation,
            "actor": self.actor,
            "data": self.data,
            "previous_hash": self.previous_hash
        }, sort_keys=True)
        # For frozen dataclass, use object.__setattr__
        object.__setattr__(self, 'entry_hash', hashlib.sha256(content.encode()).hexdigest())
class ImmutableAuditTrail:
    """Append-only audit trail with cryptographic verification."""
    def __init__(self):
        self._chain: List[AuditEntry] = []
        self._genesis_hash = "0" * 64  # Initial hash
    def append(self, operation: str, actor: str, data: Dict[str, Any]) -> str:
        """Append new entry to chain. Returns entry hash."""
        previous_hash = self._chain[-1].entry_hash if self._chain else self._genesis_hash
        entry = AuditEntry(
            timestamp=datetime.now().isoformat(),
            operation=operation,
            actor=actor,
            data=data,
            previous_hash=previous_hash
        )
        self._chain.append(entry)
        return entry.entry_hash
    def verify_integrity(self) -> bool:
        """Verify entire chain integrity."""
        if not self._chain:
            return True
        # Check first entry
        if self._chain[0].previous_hash != self._genesis_hash:
            return False
        # Check chain continuity
        for i in range(1, len(self._chain)):
            if self._chain[i].previous_hash != self._chain[i-1].entry_hash:
                return False
        return True
    def get_entries(self) -> List[Dict[str, Any]]:
        """Get all entries (read-only)."""
        return [
            {
                "timestamp": e.timestamp,
                "operation": e.operation,
                "actor": e.actor,
                "data": e.data,
                "hash": e.entry_hash
            }
            for e in self._chain
        ]
class ProvenanceManager:
    """
    Manages provenance tracking for citations and content transformations.
    Features:
    - Source document registration with content hashing
    - Citation creation with source verification
    - Modification audit trails
    - Provenance chain tracking
    - Content integrity verification
    """
    def __init__(self):
        """Initialize the provenance manager."""
        self._sources: Dict[str, Dict[str, Any]] = {}
        self._citations: Dict[str, Dict[str, Any]] = {}
        self._audit_trails: Dict[str, ImmutableAuditTrail] = {}
        self._derived_content: Dict[str, Dict[str, Any]] = {}
        self._usage_tracking: Dict[str, List[str]] = {}
        self._lock = asyncio.Lock()
    async def register_source(self, source_doc: Dict[str, Any]) -> str:
        """
        Register a source document with content hashing.
        Args:
            source_doc: Source document with id, content, and optional metadata
        Returns:
            Source ID
        """
        async with self._lock:
            source_id = source_doc.get("id", str(uuid.uuid4()))
            # Calculate content hash if not provided
            if "hash" not in source_doc:
                content = source_doc.get("content", "")
                source_doc["hash"] = hashlib.sha256(content.encode()).hexdigest()
            # Store source
            self._sources[source_id] = {
                **source_doc,
                "registered_at": datetime.now().isoformat(),
                "type": "source"
            }
            logger.info(f"Registered source: {source_id}")
            return source_id
    async def create_citation(self, source_id: str, text: str, 
                            start_pos: int, end_pos: int,
                            context: Optional[str] = None,
                            metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Create a citation with source verification.
        Args:
            source_id: ID of the source document
            text: The cited text
            start_pos: Start position in source
            end_pos: End position in source
            context: Optional surrounding context
            metadata: Optional additional metadata
        Returns:
            Citation record
        Raises:
            ValueError: If source not found or text not in source
        """
        async with self._lock:
            # Verify source exists
            if source_id not in self._sources and source_id not in self._derived_content:
                raise ValueError(f"Source not found: {source_id}")
            # Get source content
            if source_id in self._sources:
                source = self._sources[source_id]
                content = source.get("content", "")
            else:
                source = self._derived_content[source_id]
                content = source.get("output_text", "")
            # Verify text exists in source
            if text not in content:
                raise ValueError(f"Text not found in source: '{text}'")
            # Verify positions
            if start_pos < 0 or end_pos > len(content) or start_pos >= end_pos:
                raise ValueError(f"Invalid text positions: {start_pos}-{end_pos}")
            # Create citation
            citation_id = str(uuid.uuid4())
            citation = {
                "id": citation_id,
                "source_id": source_id,
                "text": text,
                "start_pos": start_pos,
                "end_pos": end_pos,
                "context": context or content[max(0, start_pos-50):min(len(content), end_pos+50)],
                "metadata": metadata or {},
                "created_at": datetime.now().isoformat(),
                "type": "citation",
                "provenance_chain": await self._build_provenance_chain(source_id)
            }
            self._citations[citation_id] = citation
            # Initialize audit trail with immutable chain
            trail = ImmutableAuditTrail()
            trail.append("create", "system", {
                "text": text,
                "source_id": source_id,
                "metadata": metadata or {}
            })
            self._audit_trails[citation_id] = trail
            logger.info(f"Created citation: {citation_id}")
            return citation
    async def modify_citation(self, citation_id: str, new_text: str,
                            reason: str, modifier: str) -> Dict[str, Any]:
        """
        Modify a citation with audit trail.
        Args:
            citation_id: ID of citation to modify
            new_text: New citation text
            reason: Reason for modification
            modifier: ID of user/system making modification
        Returns:
            Modified citation record
        """
        async with self._lock:
            if citation_id not in self._citations:
                raise ValueError(f"Citation not found: {citation_id}")
            citation = self._citations[citation_id]
            old_text = citation["text"]
            # Update citation
            citation["text"] = new_text
            citation["modified_at"] = datetime.now().isoformat()
            citation["last_modifier"] = modifier
            # Add to audit trail with cryptographic chaining
            trail = self._audit_trails.get(citation_id)
            if not trail:
                raise ValueError(f"No audit trail for citation {citation_id}")
            trail.append("modify", modifier, {
                "old_text": old_text,
                "new_text": new_text,
                "reason": reason
            })
            logger.info(f"Modified citation: {citation_id}")
            return citation
    async def get_audit_trail(self, citation_id: str) -> List[Dict[str, Any]]:
        """Get complete audit trail for a citation."""
        trail = self._audit_trails.get(citation_id)
        return trail.get_entries() if trail else []
    async def get_source(self, source_id: str) -> Optional[Dict[str, Any]]:
        """Get source document by ID."""
        return self._sources.get(source_id)
    async def create_derived_content(self, source_id: str, operation: str,
                                   input_text: str, output_text: str,
                                   tool: str) -> Dict[str, Any]:
        """
        Track derived content from transformations.
        Args:
            source_id: ID of source content
            operation: Type of operation (extract, summarize, etc.)
            input_text: Input to the operation
            output_text: Output from the operation
            tool: Tool/model used for transformation
        Returns:
            Derived content record
        """
        async with self._lock:
            derived_id = str(uuid.uuid4())
            derived = {
                "id": derived_id,
                "source_id": source_id,
                "operation": operation,
                "input_text": input_text,
                "output_text": output_text,
                "tool": tool,
                "created_at": datetime.now().isoformat(),
                "type": "derived",
                "provenance_chain": await self._build_provenance_chain(source_id)
            }
            self._derived_content[derived_id] = derived
            logger.info(f"Created derived content: {derived_id}")
            return derived
    async def _build_provenance_chain(self, source_id: str) -> List[str]:
        """Build provenance chain from source."""
        chain = []
        current_id = source_id
        while current_id:
            chain.append(current_id)
            # Check if it's derived content
            if current_id in self._derived_content:
                current_id = self._derived_content[current_id].get("source_id")
            else:
                # Reached original source
                break
        return list(reversed(chain))
    async def get_provenance_chain(self, citation_id: str) -> List[Dict[str, Any]]:
        """Get complete provenance chain for a citation."""
        if citation_id not in self._citations:
            return []
        citation = self._citations[citation_id]
        chain_ids = citation.get("provenance_chain", [])
        chain = []
        for node_id in chain_ids:
            if node_id in self._sources:
                chain.append(self._sources[node_id])
            elif node_id in self._derived_content:
                chain.append(self._derived_content[node_id])
        # Add the citation itself
        chain.append(citation)
        return chain
    async def verify_source_integrity(self, source_id: str) -> bool:
        """Verify source content hasn't been tampered with."""
        if source_id not in self._sources:
            return False
        source = self._sources[source_id]
        content = source.get("content", "")
        stored_hash = source.get("hash", "")
        # Calculate current hash
        current_hash = hashlib.sha256(content.encode()).hexdigest()
        return current_hash == stored_hash
    async def track_citation_usage(self, citation_id: str, used_in: str) -> None:
        """Track where a citation is used."""
        async with self._lock:
            if citation_id not in self._usage_tracking:
                self._usage_tracking[citation_id] = []
            self._usage_tracking[citation_id].append(used_in)
    async def get_citation_statistics(self) -> Dict[str, Any]:
        """Get statistics about citations and sources."""
        stats = {
            "total_sources": len(self._sources),
            "total_citations": len(self._citations),
            "total_derived": len(self._derived_content),
            "citations_by_source": {},
            "usage_count": {}
        }
        # Count citations per source
        for citation in self._citations.values():
            source_id = citation["source_id"]
            if source_id not in stats["citations_by_source"]:
                stats["citations_by_source"][source_id] = 0
            stats["citations_by_source"][source_id] += 1
        # Usage counts
        for citation_id, uses in self._usage_tracking.items():
            stats["usage_count"][citation_id] = len(uses)
        return stats
    async def verify_audit_integrity(self, citation_id: str) -> bool:
        """Verify audit trail hasn't been tampered with."""
        trail = self._audit_trails.get(citation_id)
        if not trail:
            return False
        return trail.verify_integrity()
    # Test helper methods (should not be in production)
    async def _tamper_source_content(self, source_id: str, new_content: str) -> None:
        """FOR TESTING ONLY: Tamper with source content."""
        if source_id in self._sources:
            self._sources[source_id]["content"] = new_content
    async def _corrupt_citation(self, citation_id: str) -> None:
        """FOR TESTING ONLY: Corrupt a citation."""
        if citation_id in self._citations:
            self._citations[citation_id]["source_id"] = "corrupted_source"
</file>

<file path="src/core/sla_monitor.py">
"""
SLA (Service Level Agreement) monitoring for KGAS.
Defines and enforces performance thresholds for all operations.
"""
import asyncio
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import logging
import json
from pathlib import Path
import aiofiles
from ..monitoring.performance_tracker import get_performance_tracker, PerformanceTracker
logger = logging.getLogger(__name__)
class SLASeverity(Enum):
    """SLA violation severity levels."""
    WARNING = "warning"      # Approaching threshold
    VIOLATION = "violation"  # Exceeded threshold
    CRITICAL = "critical"    # Severely exceeded
@dataclass
class SLAThreshold:
    """Performance threshold for an operation."""
    operation: str
    max_duration: float           # Maximum acceptable duration (seconds)
    warning_duration: float       # Warning threshold (seconds)
    critical_duration: float      # Critical threshold (seconds)
    max_error_rate: float        # Maximum acceptable error rate (0-1)
    min_success_rate: float      # Minimum success rate (0-1)
    evaluation_window: int       # Window size for rate calculations
    def check_duration(self, duration: float) -> Optional[SLASeverity]:
        """Check if duration violates SLA."""
        if duration >= self.critical_duration:
            return SLASeverity.CRITICAL
        elif duration >= self.max_duration:
            return SLASeverity.VIOLATION
        elif duration >= self.warning_duration:
            return SLASeverity.WARNING
        return None
    def check_error_rate(self, error_rate: float) -> Optional[SLASeverity]:
        """Check if error rate violates SLA."""
        if error_rate > self.max_error_rate:
            return SLASeverity.VIOLATION
        elif error_rate > self.max_error_rate * 0.8:  # 80% of threshold
            return SLASeverity.WARNING
        return None
@dataclass
class SLAViolation:
    """Record of an SLA violation."""
    operation: str
    severity: SLASeverity
    violation_type: str  # "duration" or "error_rate"
    actual_value: float
    threshold_value: float
    timestamp: str
    metadata: Dict[str, Any] = field(default_factory=dict)
class SLAMonitor:
    """
    Monitors operations against defined SLA thresholds.
    Features:
    - Configurable thresholds per operation
    - Real-time violation detection
    - Alert callbacks
    - Historical violation tracking
    - Automatic threshold recommendations
    """
    # Default SLA thresholds for common operations
    DEFAULT_SLAS = {
        "tool_execution": SLAThreshold(
            operation="tool_execution",
            max_duration=5.0,
            warning_duration=4.0,
            critical_duration=10.0,
            max_error_rate=0.05,
            min_success_rate=0.95,
            evaluation_window=100
        ),
        "database_query": SLAThreshold(
            operation="database_query",
            max_duration=1.0,
            warning_duration=0.8,
            critical_duration=3.0,
            max_error_rate=0.01,
            min_success_rate=0.99,
            evaluation_window=1000
        ),
        "api_request": SLAThreshold(
            operation="api_request",
            max_duration=2.0,
            warning_duration=1.5,
            critical_duration=5.0,
            max_error_rate=0.02,
            min_success_rate=0.98,
            evaluation_window=500
        ),
        "document_processing": SLAThreshold(
            operation="document_processing",
            max_duration=30.0,
            warning_duration=25.0,
            critical_duration=60.0,
            max_error_rate=0.1,
            min_success_rate=0.9,
            evaluation_window=50
        ),
        "pipeline_execution": SLAThreshold(
            operation="pipeline_execution",
            max_duration=120.0,
            warning_duration=100.0,
            critical_duration=300.0,
            max_error_rate=0.15,
            min_success_rate=0.85,
            evaluation_window=20
        )
    }
    def __init__(self,
                 performance_tracker: Optional[PerformanceTracker] = None,
                 config_path: Optional[Path] = None):
        """
        Initialize SLA monitor.
        Args:
            performance_tracker: Performance tracker instance
            config_path: Path to SLA configuration file
        """
        self.tracker = performance_tracker or get_performance_tracker()
        self.config_path = config_path or Path("sla_config.json")
        # SLA thresholds
        self._thresholds: Dict[str, SLAThreshold] = self.DEFAULT_SLAS.copy()
        # Violation history
        self._violations: List[SLAViolation] = []
        self._violation_counts: Dict[str, int] = {}
        # Alert callbacks
        self._alert_handlers: List[Callable] = []
        # Statistics
        self._stats = {
            "total_checks": 0,
            "total_violations": 0,
            "critical_violations": 0,
            "operations_monitored": set()
        }
        # Lock for thread safety
        self._lock = asyncio.Lock()
        # Flag to track if config is loaded
        self._config_loaded = False
        # Monitoring task (will be started when needed)
        self._monitoring_task = None
    async def _load_config(self):
        """Load SLA configuration from file."""
        try:
            if self.config_path.exists():
                async with aiofiles.open(self.config_path, 'r') as f:
                    config = json.loads(await f.read())
                    for op, threshold_data in config.get("thresholds", {}).items():
                        self._thresholds[op] = SLAThreshold(**threshold_data)
                logger.info(f"Loaded SLA config for {len(self._thresholds)} operations")
        except Exception as e:
            logger.error(f"Failed to load SLA config: {e}")
    async def _ensure_config_loaded(self):
        """Ensure configuration is loaded."""
        if not self._config_loaded:
            await self._load_config()
            self._config_loaded = True
    async def _ensure_monitoring_started(self):
        """Ensure monitoring task is started."""
        if self._monitoring_task is None:
            self._monitoring_task = asyncio.create_task(self._monitoring_loop())
    async def _save_config(self):
        """Save current SLA configuration."""
        try:
            config = {
                "thresholds": {
                    op: {
                        "operation": t.operation,
                        "max_duration": t.max_duration,
                        "warning_duration": t.warning_duration,
                        "critical_duration": t.critical_duration,
                        "max_error_rate": t.max_error_rate,
                        "min_success_rate": t.min_success_rate,
                        "evaluation_window": t.evaluation_window
                    }
                    for op, t in self._thresholds.items()
                },
                "stats": {
                    "total_checks": self._stats["total_checks"],
                    "total_violations": self._stats["total_violations"],
                    "critical_violations": self._stats["critical_violations"]
                }
            }
            async with aiofiles.open(self.config_path, 'w') as f:
                await f.write(json.dumps(config, indent=2))
        except Exception as e:
            logger.error(f"Failed to save SLA config: {e}")
    async def set_sla(self, operation: str, threshold: SLAThreshold):
        """Set SLA threshold for an operation."""
        async with self._lock:
            self._thresholds[operation] = threshold
            await self._save_config()
            logger.info(f"Set SLA for {operation}: max={threshold.max_duration}s")
    async def check_operation(self, 
                            operation: str, 
                            duration: float,
                            success: bool,
                            metadata: Optional[Dict] = None) -> Optional[SLAViolation]:
        """
        Check if an operation violates SLA.
        Args:
            operation: Operation name
            duration: Operation duration in seconds
            success: Whether operation succeeded
            metadata: Optional metadata
        Returns:
            SLAViolation if threshold exceeded, None otherwise
        """
        await self._ensure_config_loaded()
        await self._ensure_monitoring_started()
        async with self._lock:
            self._stats["total_checks"] += 1
            self._stats["operations_monitored"].add(operation)
            # Get threshold
            threshold = self._thresholds.get(operation)
            if not threshold:
                # No SLA defined for this operation
                return None
            violations = []
            # Check duration
            if success:  # Only check duration for successful operations
                severity = threshold.check_duration(duration)
                if severity:
                    violation = SLAViolation(
                        operation=operation,
                        severity=severity,
                        violation_type="duration",
                        actual_value=duration,
                        threshold_value=threshold.max_duration,
                        timestamp=datetime.now().isoformat(),
                        metadata=metadata or {}
                    )
                    violations.append(violation)
            # Check error rate (need historical data)
            stats = await self.tracker.get_operation_stats(operation)
            if "error" not in stats:
                error_rate = 1.0 - stats.get("success_rate", 1.0)
                severity = threshold.check_error_rate(error_rate)
                if severity:
                    violation = SLAViolation(
                        operation=operation,
                        severity=severity,
                        violation_type="error_rate",
                        actual_value=error_rate,
                        threshold_value=threshold.max_error_rate,
                        timestamp=datetime.now().isoformat(),
                        metadata=metadata or {}
                    )
                    violations.append(violation)
            # Record violations
            for violation in violations:
                await self._record_violation(violation)
            # Return most severe violation
            if violations:
                return max(violations, key=lambda v: 
                    [SLASeverity.WARNING, SLASeverity.VIOLATION, SLASeverity.CRITICAL].index(v.severity)
                )
            return None
    async def _record_violation(self, violation: SLAViolation):
        """Record an SLA violation."""
        self._violations.append(violation)
        self._violation_counts[violation.operation] = \
            self._violation_counts.get(violation.operation, 0) + 1
        self._stats["total_violations"] += 1
        if violation.severity == SLASeverity.CRITICAL:
            self._stats["critical_violations"] += 1
        # Trigger alerts
        for handler in self._alert_handlers:
            try:
                await handler(violation)
            except Exception as e:
                logger.error(f"Alert handler failed: {e}")
        # Log violation
        logger.warning(
            f"SLA {violation.severity.value} for {violation.operation}: "
            f"{violation.violation_type}={violation.actual_value:.3f} "
            f"(threshold={violation.threshold_value:.3f})"
        )
    async def _monitoring_loop(self):
        """Continuous monitoring loop."""
        while True:
            try:
                # Check recent operations every 10 seconds
                await asyncio.sleep(10)
                # Get performance summary
                summary = await self.tracker.get_system_summary()
                # Check each operation
                for op, stats in summary.get("operations", {}).items():
                    if op in self._thresholds:
                        # Check recent performance
                        recent_mean = stats.get("recent_mean", 0)
                        success_rate = stats.get("success_rate", 1)
                        await self.check_operation(
                            operation=op,
                            duration=recent_mean,
                            success=success_rate > 0.5,
                            metadata={"source": "monitoring_loop"}
                        )
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Monitoring loop error: {e}")
    def register_alert_handler(self, handler: Callable):
        """
        Register an alert handler for SLA violations.
        Handler signature: async def handler(violation: SLAViolation)
        """
        self._alert_handlers.append(handler)
        logger.info(f"Registered SLA alert handler: {handler.__name__}")
    async def get_violation_history(self, 
                                  operation: Optional[str] = None,
                                  severity: Optional[SLASeverity] = None,
                                  hours: int = 24) -> List[SLAViolation]:
        """Get recent SLA violations."""
        async with self._lock:
            cutoff = datetime.now() - timedelta(hours=hours)
            violations = []
            for v in self._violations:
                # Filter by time
                v_time = datetime.fromisoformat(v.timestamp)
                if v_time < cutoff:
                    continue
                # Filter by operation
                if operation and v.operation != operation:
                    continue
                # Filter by severity
                if severity and v.severity != severity:
                    continue
                violations.append(v)
            return violations
    async def get_sla_report(self) -> Dict[str, Any]:
        """Generate SLA compliance report."""
        async with self._lock:
            report = {
                "summary": {
                    "total_operations": len(self._stats["operations_monitored"]),
                    "total_checks": self._stats["total_checks"],
                    "total_violations": self._stats["total_violations"],
                    "critical_violations": self._stats["critical_violations"],
                    "violation_rate": (
                        self._stats["total_violations"] / 
                        max(1, self._stats["total_checks"])
                    )
                },
                "operations": {}
            }
            # Per-operation report
            for op in self._stats["operations_monitored"]:
                violations = self._violation_counts.get(op, 0)
                threshold = self._thresholds.get(op)
                op_report = {
                    "violations": violations,
                    "has_sla": threshold is not None
                }
                if threshold:
                    op_report["sla"] = {
                        "max_duration": threshold.max_duration,
                        "max_error_rate": threshold.max_error_rate
                    }
                report["operations"][op] = op_report
            return report
    async def recommend_sla(self, operation: str) -> Optional[SLAThreshold]:
        """
        Recommend SLA thresholds based on historical performance.
        Args:
            operation: Operation to analyze
        Returns:
            Recommended SLA threshold or None if insufficient data
        """
        stats = await self.tracker.get_operation_stats(operation)
        if "error" in stats or stats.get("sample_count", 0) < 100:
            return None
        # Base recommendations on current performance + buffer
        baseline = stats.get("baseline", {})
        if not baseline:
            return None
        # Add 20% buffer to baseline
        recommended = SLAThreshold(
            operation=operation,
            max_duration=baseline["p95"] * 1.2,
            warning_duration=baseline["p95"],
            critical_duration=baseline["p95"] * 2.0,
            max_error_rate=min(0.1, (1 - stats["success_rate"]) * 2),
            min_success_rate=max(0.9, stats["success_rate"] * 0.95),
            evaluation_window=100
        )
        return recommended
    async def cleanup(self):
        """Clean up resources."""
        if self._monitoring_task:
            self._monitoring_task.cancel()
            try:
                await self._monitoring_task
            except asyncio.CancelledError:
                pass
        await self._save_config()
# Global monitor instance
_global_monitor: Optional[SLAMonitor] = None
def get_sla_monitor() -> SLAMonitor:
    """Get or create global SLA monitor."""
    global _global_monitor
    if _global_monitor is None:
        _global_monitor = SLAMonitor()
    return _global_monitor
</file>

<file path="src/monitoring/performance_tracker.py">
"""
Performance tracking for KGAS operations.
Tracks execution times, establishes baselines, and detects degradation.
"""
import asyncio
import time
from typing import Dict, List, Optional, Any, Callable, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict, deque
import statistics
import json
import aiofiles
from pathlib import Path
import logging
logger = logging.getLogger(__name__)
@dataclass
class PerformanceMetric:
    """Single performance measurement."""
    operation: str
    start_time: float
    end_time: float
    duration: float
    success: bool
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
@dataclass
class PerformanceBaseline:
    """Baseline metrics for an operation."""
    operation: str
    p50: float  # Median
    p75: float  # 75th percentile
    p95: float  # 95th percentile
    p99: float  # 99th percentile
    mean: float
    std_dev: float
    sample_count: int
    established_at: str
    def is_degraded(self, duration: float) -> bool:
        """Check if duration indicates degradation."""
        # Degraded if >= 2 standard deviations above mean
        # or > p95 baseline
        return duration > self.p95 or duration >= (self.mean + 2 * self.std_dev)
class PerformanceTracker:
    """
    Tracks operation performance and establishes baselines.
    Features:
    - Automatic timing of operations
    - Rolling window metrics
    - Baseline establishment
    - Degradation detection
    - Persistent storage
    """
    def __init__(self, 
                 window_size: int = 1000,
                 baseline_samples: int = 100,
                 storage_path: Optional[Path] = None):
        """
        Initialize performance tracker.
        Args:
            window_size: Size of rolling window for metrics
            baseline_samples: Samples needed to establish baseline
            storage_path: Path for persistent storage
        """
        self.window_size = window_size
        self.baseline_samples = baseline_samples
        self.storage_path = storage_path or Path("performance_data.json")
        # Rolling windows for each operation
        self._metrics: Dict[str, deque] = defaultdict(
            lambda: deque(maxlen=window_size)
        )
        # Established baselines
        self._baselines: Dict[str, PerformanceBaseline] = {}
        # Active timers - maps timer_id to (start_time, operation)
        self._active_timers: Dict[str, Tuple[float, str]] = {}
        # Statistics
        self._stats = {
            "total_operations": 0,
            "degraded_operations": 0,
            "baseline_updates": 0
        }
        # Lock for thread safety
        self._lock = asyncio.Lock()
        # Flag to track if baselines are loaded
        self._baselines_loaded = False
    async def _load_baselines(self):
        """Load baselines from storage."""
        try:
            if self.storage_path.exists():
                async with aiofiles.open(self.storage_path, 'r') as f:
                    data = json.loads(await f.read())
                    for op, baseline_data in data.get("baselines", {}).items():
                        self._baselines[op] = PerformanceBaseline(**baseline_data)
                logger.info(f"Loaded {len(self._baselines)} performance baselines")
        except Exception as e:
            logger.error(f"Failed to load baselines: {e}")
    async def _save_baselines(self):
        """Save baselines to storage."""
        try:
            data = {
                "baselines": {
                    op: {
                        "operation": b.operation,
                        "p50": b.p50,
                        "p75": b.p75,
                        "p95": b.p95,
                        "p99": b.p99,
                        "mean": b.mean,
                        "std_dev": b.std_dev,
                        "sample_count": b.sample_count,
                        "established_at": b.established_at
                    }
                    for op, b in self._baselines.items()
                },
                "stats": self._stats
            }
            async with aiofiles.open(self.storage_path, 'w') as f:
                await f.write(json.dumps(data, indent=2))
        except Exception as e:
            logger.error(f"Failed to save baselines: {e}")
    async def _ensure_baselines_loaded(self):
        """Ensure baselines are loaded."""
        if not self._baselines_loaded:
            await self._load_baselines()
            self._baselines_loaded = True
    async def start_operation(self, operation: str, metadata: Optional[Dict] = None) -> str:
        """
        Start timing an operation.
        Args:
            operation: Operation name
            metadata: Optional metadata
        Returns:
            Timer ID for this operation
        """
        await self._ensure_baselines_loaded()
        timer_id = f"{operation}_{time.time()}_{id(metadata)}"
        async with self._lock:
            self._active_timers[timer_id] = (time.perf_counter(), operation)
        return timer_id
    async def end_operation(self, timer_id: str, success: bool = True) -> float:
        """
        End timing an operation.
        Args:
            timer_id: Timer ID from start_operation
            success: Whether operation succeeded
        Returns:
            Operation duration in seconds
        """
        async with self._lock:
            timer_data = self._active_timers.pop(timer_id, None)
            if timer_data is None:
                raise ValueError(f"No active timer for {timer_id}")
            start_time, operation = timer_data
            end_time = time.perf_counter()
            duration = end_time - start_time
            # Record metric
            metric = PerformanceMetric(
                operation=operation,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                success=success
            )
            self._metrics[operation].append(metric)
            self._stats["total_operations"] += 1
            # Check for degradation
            if operation in self._baselines:
                baseline = self._baselines[operation]
                if baseline.is_degraded(duration):
                    self._stats["degraded_operations"] += 1
                    logger.warning(
                        f"Performance degradation detected for {operation}: "
                        f"{duration:.3f}s (baseline p95: {baseline.p95:.3f}s)"
                    )
            # Update baseline if needed
            await self._update_baseline_if_needed(operation)
            return duration
    async def _update_baseline_if_needed(self, operation: str):
        """Update baseline if enough samples collected."""
        metrics = self._metrics[operation]
        # Need enough successful samples
        successful_metrics = [m for m in metrics if m.success]
        if len(successful_metrics) < self.baseline_samples:
            return
        # Calculate new baseline
        durations = [m.duration for m in successful_metrics[-self.baseline_samples:]]
        durations.sort()
        baseline = PerformanceBaseline(
            operation=operation,
            p50=durations[len(durations) // 2],
            p75=durations[min(int(len(durations) * 0.75), len(durations) - 1)],
            p95=durations[min(int(len(durations) * 0.95), len(durations) - 1)],
            p99=durations[min(int(len(durations) * 0.99), len(durations) - 1)],
            mean=statistics.mean(durations),
            std_dev=statistics.stdev(durations) if len(durations) > 1 else 0.0,
            sample_count=len(durations),
            established_at=datetime.now().isoformat()
        )
        # Only update if significantly different or new
        should_update = operation not in self._baselines
        if not should_update and operation in self._baselines:
            old = self._baselines[operation]
            # Update if mean changed by >10%
            should_update = abs(baseline.mean - old.mean) / old.mean > 0.1
        if should_update:
            self._baselines[operation] = baseline
            self._stats["baseline_updates"] += 1
            await self._save_baselines()
            logger.info(f"Updated baseline for {operation}: p95={baseline.p95:.3f}s")
    def time_operation(self, operation: str):
        """
        Decorator for timing operations.
        Usage:
            @tracker.time_operation("process_document")
            async def process_document(doc):
                ...
        """
        def decorator(func: Callable):
            async def async_wrapper(*args, **kwargs):
                timer_id = await self.start_operation(operation)
                try:
                    result = await func(*args, **kwargs)
                    await self.end_operation(timer_id, success=True)
                    return result
                except Exception as e:
                    await self.end_operation(timer_id, success=False)
                    raise
            def sync_wrapper(*args, **kwargs):
                # For sync functions, create a new event loop if needed
                try:
                    loop = asyncio.get_running_loop()
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                timer_id = loop.run_until_complete(
                    self.start_operation(operation)
                )
                try:
                    result = func(*args, **kwargs)
                    loop.run_until_complete(
                        self.end_operation(timer_id, success=True)
                    )
                    return result
                except Exception as e:
                    loop.run_until_complete(
                        self.end_operation(timer_id, success=False)
                    )
                    raise
            return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
        return decorator
    async def get_operation_stats(self, operation: str) -> Dict[str, Any]:
        """Get statistics for a specific operation."""
        async with self._lock:
            metrics = list(self._metrics.get(operation, []))
            if not metrics:
                return {"error": "No metrics for operation"}
            recent_durations = [m.duration for m in metrics[-100:] if m.success]
            if not recent_durations:
                return {"error": "No successful operations"}
            recent_durations_sorted = sorted(recent_durations)
            stats = {
                "operation": operation,
                "sample_count": len(metrics),
                "success_rate": sum(1 for m in metrics if m.success) / len(metrics),
                "recent_p50": statistics.median(recent_durations),
                "recent_p95": recent_durations_sorted[min(int(len(recent_durations_sorted) * 0.95), len(recent_durations_sorted) - 1)],
                "recent_mean": statistics.mean(recent_durations),
            }
            if operation in self._baselines:
                baseline = self._baselines[operation]
                stats["baseline"] = {
                    "p50": baseline.p50,
                    "p95": baseline.p95,
                    "mean": baseline.mean,
                    "sample_count": baseline.sample_count,
                    "established_at": baseline.established_at
                }
                stats["degradation_rate"] = sum(
                    1 for m in metrics[-100:]
                    if m.success and baseline.is_degraded(m.duration)
                ) / min(100, len(metrics))
            return stats
    async def get_system_summary(self) -> Dict[str, Any]:
        """Get overall system performance summary."""
        async with self._lock:
            summary = {
                "total_operations": self._stats["total_operations"],
                "degraded_operations": self._stats["degraded_operations"],
                "degradation_rate": (
                    self._stats["degraded_operations"] / 
                    max(1, self._stats["total_operations"])
                ),
                "tracked_operations": list(self._metrics.keys()),
                "operations_with_baselines": list(self._baselines.keys()),
                "baseline_updates": self._stats["baseline_updates"]
            }
            # Add per-operation summaries
            operation_stats = {}
            for op in self._metrics:
                stats = await self.get_operation_stats(op)
                if "error" not in stats:
                    operation_stats[op] = stats
            summary["operations"] = operation_stats
            return summary
# Global tracker instance
_global_tracker: Optional[PerformanceTracker] = None
def get_performance_tracker() -> PerformanceTracker:
    """Get or create global performance tracker."""
    global _global_tracker
    if _global_tracker is None:
        _global_tracker = PerformanceTracker()
    return _global_tracker
</file>

</files>

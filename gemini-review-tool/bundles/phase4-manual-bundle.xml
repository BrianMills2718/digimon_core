This file contains Phase 4 Production Readiness implementation files.

<file_summary>
<purpose>
Manual bundle of Phase 4 implementation files for Gemini validation.
</purpose>
</file_summary>

<directory_structure>
main.py
.env.example
validate_phase4.py
src/core/error_handler.py
docker/Dockerfile
</directory_structure>

<files>

<file path="main.py">
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse, PlainTextResponse
import uvicorn
import logging
import asyncio
from datetime import datetime
import os
import sys
from pathlib import Path
import psutil
import time

# Import Phase 4 components
sys.path.append(str(Path(__file__).parent))

try:
    from src.core.error_handler import ProductionErrorHandler, handle_errors, ErrorSeverity
    from src.core.performance_optimizer import PerformanceOptimizer, performance_optimizer
    from src.core.security_manager import SecurityManager, security_manager
    from src.monitoring.production_monitoring import ProductionMonitoring, production_monitor
    error_handler = ProductionErrorHandler()
except ImportError as e:
    print(f"Warning: Could not import Phase 4 components: {e}")
    error_handler = None

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)8s | %(name)s | %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('kgas.log') if os.path.exists('logs') else logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="KGAS Production API",
    description="Knowledge Graph AI System - Phase 4 Production Ready",
    version="4.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Global startup time for health checks
startup_time = datetime.now()

@app.on_event("startup")
async def startup_event():
    """Initialize all Phase 4 production systems."""
    global startup_time
    startup_time = datetime.now()
    
    logger.info("🚀 Starting KGAS Phase 4 Production API")
    
    try:
        # Start production monitoring
        if 'production_monitor' in globals():
            production_monitor.start_monitoring()
            logger.info("✅ Production monitoring started")
        
        # Initialize performance optimization
        if 'performance_optimizer' in globals():
            logger.info("✅ Performance optimizer initialized")
        
        # Log security manager status
        if 'security_manager' in globals():
            logger.info("✅ Security manager initialized")
        
        logger.info("🎉 KGAS Phase 4 startup complete")
        
    except Exception as e:
        logger.error(f"❌ Startup error: {e}")
        if error_handler:
            error_handler.handle_error(e, {'context': 'startup'})

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup Phase 4 production systems."""
    logger.info("🛑 Shutting down KGAS Phase 4 Production API")
    
    try:
        # Stop production monitoring
        if 'production_monitor' in globals():
            production_monitor.stop_monitoring()
            logger.info("✅ Production monitoring stopped")
        
        logger.info("👋 KGAS Phase 4 shutdown complete")
        
    except Exception as e:
        logger.error(f"❌ Shutdown error: {e}")

@app.get("/health")
async def health_check() -> JSONResponse:
    """
    Kubernetes liveness probe endpoint.
    Simple health check that verifies the service is running.
    """
    try:
        current_time = datetime.now()
        uptime = (current_time - startup_time).total_seconds()
        
        health_data = {
            "status": "healthy",
            "timestamp": current_time.isoformat(),
            "uptime_seconds": uptime,
            "service": "KGAS Phase 4 Production API",
            "version": "4.0.0"
        }
        
        return JSONResponse(content=health_data, status_code=200)
        
    except Exception as e:
        logger.error(f"Health check error: {e}")
        if error_handler:
            error_handler.handle_error(e, {'endpoint': '/health'})
        
        return JSONResponse(
            content={
                "status": "unhealthy", 
                "error": "Health check failed",
                "timestamp": datetime.now().isoformat()
            }, 
            status_code=503
        )

@app.get("/ready")
async def readiness_check() -> JSONResponse:
    """
    Kubernetes readiness probe endpoint.
    Comprehensive check that verifies all dependencies are available.
    """
    try:
        current_time = datetime.now()
        uptime = (current_time - startup_time).total_seconds()
        
        # Check system resources
        cpu_usage = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        # Check Phase 4 components
        components_status = {
            "error_handler": error_handler is not None,
            "performance_optimizer": 'performance_optimizer' in globals(),
            "security_manager": 'security_manager' in globals(),
            "production_monitor": 'production_monitor' in globals()
        }
        
        # Determine overall readiness
        all_components_ready = all(components_status.values())
        resource_usage_ok = cpu_usage < 90 and memory.percent < 90 and (disk.used / disk.total * 100) < 95
        
        is_ready = all_components_ready and resource_usage_ok and uptime > 5  # 5 second warmup
        
        readiness_data = {
            "status": "ready" if is_ready else "not_ready",
            "timestamp": current_time.isoformat(),
            "uptime_seconds": uptime,
            "components": components_status,
            "system_resources": {
                "cpu_usage_percent": cpu_usage,
                "memory_usage_percent": memory.percent,
                "disk_usage_percent": (disk.used / disk.total * 100),
                "available_memory_gb": memory.available / (1024**3)
            },
            "service": "KGAS Phase 4 Production API",
            "version": "4.0.0"
        }
        
        status_code = 200 if is_ready else 503
        return JSONResponse(content=readiness_data, status_code=status_code)
        
    except Exception as e:
        logger.error(f"Readiness check error: {e}")
        if error_handler:
            error_handler.handle_error(e, {'endpoint': '/ready'})
        
        return JSONResponse(
            content={
                "status": "not_ready", 
                "error": "Readiness check failed",
                "timestamp": datetime.now().isoformat()
            }, 
            status_code=503
        )

@app.get("/metrics")
async def metrics_endpoint() -> JSONResponse:
    """
    Prometheus metrics endpoint.
    Provides comprehensive metrics for monitoring and alerting.
    """
    try:
        current_time = datetime.now()
        
        # System metrics
        cpu_usage = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        # Phase 4 component metrics
        metrics = {
            "timestamp": current_time.isoformat(),
            "system_metrics": {
                "cpu_usage_percent": cpu_usage,
                "memory_usage_percent": memory.percent,
                "memory_total_gb": memory.total / (1024**3),
                "memory_available_gb": memory.available / (1024**3),
                "disk_usage_percent": (disk.used / disk.total * 100),
                "disk_total_gb": disk.total / (1024**3),
                "disk_free_gb": disk.free / (1024**3)
            },
            "application_metrics": {
                "uptime_seconds": (current_time - startup_time).total_seconds(),
                "phase4_components_loaded": sum([
                    error_handler is not None,
                    'performance_optimizer' in globals(),
                    'security_manager' in globals(),
                    'production_monitor' in globals()
                ])
            }
        }
        
        # Add performance metrics if available
        if 'performance_optimizer' in globals():
            try:
                perf_report = performance_optimizer.get_performance_report()
                metrics["performance_metrics"] = perf_report.get('system_metrics', {})
            except Exception as e:
                logger.warning(f"Could not get performance metrics: {e}")
        
        # Add error handler metrics if available
        if error_handler:
            try:
                error_stats = error_handler.get_error_statistics()
                metrics["error_metrics"] = error_stats
            except Exception as e:
                logger.warning(f"Could not get error metrics: {e}")
        
        # Add monitoring metrics if available
        if 'production_monitor' in globals():
            try:
                monitoring_status = production_monitor.get_monitoring_status()
                metrics["monitoring_metrics"] = monitoring_status
            except Exception as e:
                logger.warning(f"Could not get monitoring metrics: {e}")
        
        return JSONResponse(content=metrics, status_code=200)
        
    except Exception as e:
        logger.error(f"Metrics endpoint error: {e}")
        if error_handler:
            error_handler.handle_error(e, {'endpoint': '/metrics'})
        
        return JSONResponse(
            content={
                "error": "Metrics collection failed",
                "timestamp": datetime.now().isoformat()
            }, 
            status_code=500
        )

@app.get("/status")
async def status_endpoint() -> JSONResponse:
    """
    Comprehensive status endpoint for production monitoring.
    Provides detailed status information for all Phase 4 components.
    """
    try:
        current_time = datetime.now()
        
        # Collect comprehensive status
        status = {
            "timestamp": current_time.isoformat(),
            "service": "KGAS Phase 4 Production API",
            "version": "4.0.0",
            "uptime_seconds": (current_time - startup_time).total_seconds(),
            "environment": os.getenv("ENVIRONMENT", "production"),
            "phase4_components": {
                "error_handler": {
                    "loaded": error_handler is not None,
                    "status": "active" if error_handler else "not_loaded"
                },
                "performance_optimizer": {
                    "loaded": 'performance_optimizer' in globals(),
                    "status": "active" if 'performance_optimizer' in globals() else "not_loaded"
                },
                "security_manager": {
                    "loaded": 'security_manager' in globals(),
                    "status": "active" if 'security_manager' in globals() else "not_loaded"
                },
                "production_monitor": {
                    "loaded": 'production_monitor' in globals(),
                    "status": "active" if 'production_monitor' in globals() else "not_loaded"
                }
            }
        }
        
        # Add detailed component information if available
        if error_handler:
            try:
                error_stats = error_handler.get_error_statistics()
                status["error_handler_details"] = {
                    "total_errors": error_stats.get("total_errors", 0),
                    "recovery_rate": error_stats.get("recovery_rate", 0),
                    "critical_errors": error_stats.get("critical_errors", 0)
                }
            except Exception as e:
                logger.warning(f"Could not get error handler details: {e}")
        
        if 'performance_optimizer' in globals():
            try:
                perf_report = performance_optimizer.get_performance_report()
                status["performance_details"] = {
                    "monitored_operations": len(perf_report.get("operation_profiles", {})),
                    "recommendations_count": len(perf_report.get("performance_recommendations", []))
                }
            except Exception as e:
                logger.warning(f"Could not get performance details: {e}")
        
        if 'production_monitor' in globals():
            try:
                monitoring_status = production_monitor.get_monitoring_status()
                status["monitoring_details"] = monitoring_status
            except Exception as e:
                logger.warning(f"Could not get monitoring details: {e}")
        
        return JSONResponse(content=status, status_code=200)
        
    except Exception as e:
        logger.error(f"Status endpoint error: {e}")
        if error_handler:
            error_handler.handle_error(e, {'endpoint': '/status'})
        
        return JSONResponse(
            content={
                "error": "Status collection failed",
                "timestamp": datetime.now().isoformat()
            }, 
            status_code=500
        )

@app.get("/")
async def root() -> JSONResponse:
    """Root endpoint with API information."""
    return JSONResponse(content={
        "service": "KGAS Phase 4 Production API",
        "version": "4.0.0", 
        "description": "Knowledge Graph AI System - Production Ready",
        "endpoints": {
            "health": "/health - Kubernetes liveness probe",
            "ready": "/ready - Kubernetes readiness probe", 
            "metrics": "/metrics - Prometheus metrics",
            "status": "/status - Comprehensive status",
            "docs": "/docs - API documentation"
        },
        "timestamp": datetime.now().isoformat()
    })

if __name__ == "__main__":
    # Production server configuration
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    workers = int(os.getenv("WORKERS", "1"))
    
    logger.info(f"🚀 Starting KGAS Phase 4 Production API on {host}:{port}")
    logger.info(f"📊 Environment: {os.getenv('ENVIRONMENT', 'production')}")
    
    # Run with uvicorn
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        workers=workers,
        log_level="info",
        access_log=True
    )
</file>

<file path=".env.example">
# KGAS Phase 4 Production Environment Configuration
# Copy this file to .env and configure your production settings

# ============================================================================
# CORE APPLICATION SETTINGS
# ============================================================================

# Application Environment
ENVIRONMENT=production
HOST=0.0.0.0
PORT=8000
WORKERS=1

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================

# Neo4j Configuration
NEO4J_URI=bolt://neo4j:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your-neo4j-password-here
NEO4J_DATABASE=neo4j

# Redis Configuration
REDIS_URL=redis://redis:6379
REDIS_PASSWORD=your-redis-password-here
REDIS_DB=0

# ============================================================================
# SECURITY CONFIGURATION
# ============================================================================

# Security Keys
SECRET_KEY=your-ultra-secure-secret-key-here
JWT_SECRET_KEY=your-jwt-secret-key-here
ENCRYPTION_KEY=your-encryption-key-here

# API Authentication
API_KEY=your-primary-api-key-here
API_KEY_HEADER=X-API-Key

# Security Settings
PASSWORD_MIN_LENGTH=12
MAX_LOGIN_ATTEMPTS=5
ACCOUNT_LOCKOUT_DURATION=3600
SESSION_TIMEOUT=3600
JWT_EXPIRATION=3600

# Rate Limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=3600

# ============================================================================
# PERFORMANCE OPTIMIZATION
# ============================================================================

# Performance Settings
MAX_PARALLEL_WORKERS=4
MEMORY_LIMIT_PERCENT=80
CPU_LIMIT_PERCENT=80

# Cache Configuration
CACHE_ENABLED=true
CACHE_MAX_AGE_HOURS=24
CACHE_DIRECTORY=.cache
CACHE_MAX_SIZE_MB=1024

# Connection Pool Settings
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20
DB_POOL_TIMEOUT=30

# Query Optimization
QUERY_TIMEOUT=30
QUERY_CACHE_ENABLED=true
QUERY_CACHE_SIZE=1000

# ============================================================================
# MONITORING AND ALERTING
# ============================================================================

# Prometheus Metrics
PROMETHEUS_METRICS_PORT=9090
METRICS_ENABLED=true
DETAILED_METRICS=true

# Health Check Configuration
HEALTH_CHECK_INTERVAL=30
READINESS_CHECK_TIMEOUT=10
LIVENESS_CHECK_TIMEOUT=5

# Alert Thresholds
CPU_ALERT_THRESHOLD=80
MEMORY_ALERT_THRESHOLD=85
DISK_ALERT_THRESHOLD=90
ERROR_RATE_THRESHOLD=5

# Notification Channels
EMAIL_ALERTS_ENABLED=true
SLACK_ALERTS_ENABLED=true
WEBHOOK_ALERTS_ENABLED=true

# Email Configuration
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=your-email@company.com
SMTP_PASSWORD=your-email-password-here
ALERT_EMAIL_FROM=alerts@kgas.com
ALERT_EMAIL_TO=ops-team@company.com

# Slack Configuration
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/your-webhook-url
SLACK_CHANNEL=#alerts
SLACK_USERNAME=KGAS-Monitor

# Webhook Configuration
WEBHOOK_URL=https://your-alerting-system.com/webhook
WEBHOOK_TOKEN=your-webhook-token-here

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Logging Settings
LOG_LEVEL=INFO
LOG_FILE=logs/kgas.log
LOG_MAX_SIZE_MB=100
LOG_BACKUP_COUNT=5
ENABLE_COLORED_LOGS=true
LOG_FORMAT=detailed

# Audit Logging
AUDIT_LOG_ENABLED=true
AUDIT_LOG_FILE=logs/audit.log
AUDIT_RETENTION_DAYS=90

# Security Event Logging
SECURITY_LOG_ENABLED=true
SECURITY_LOG_FILE=logs/security.log
FAILED_LOGIN_LOG_ENABLED=true

# ============================================================================
# EXTERNAL SERVICE INTEGRATION
# ============================================================================

# Gemini AI Configuration
GEMINI_API_KEY=your-gemini-api-key-here
GEMINI_MODEL=gemini-2.5-flash
GEMINI_FALLBACK_MODEL=gemini-2.5-flash
GEMINI_MAX_TOKENS=1000000
GEMINI_TEMPERATURE=0.1
GEMINI_TOP_P=0.95

# GitHub Integration
GITHUB_TOKEN=ghp_your-github-token-here
GITHUB_ORG=your-organization
GITHUB_REPO=your-repository

# ============================================================================
# DEPLOYMENT CONFIGURATION
# ============================================================================

# Docker Configuration
DOCKER_REGISTRY=your-registry.com
DOCKER_IMAGE_TAG=latest
DOCKER_BUILD_ARGS=--no-cache

# Kubernetes Configuration
K8S_NAMESPACE=kgas-production
K8S_SERVICE_ACCOUNT=kgas-sa
K8S_RESOURCE_REQUESTS_CPU=250m
K8S_RESOURCE_REQUESTS_MEMORY=512Mi
K8S_RESOURCE_LIMITS_CPU=500m
K8S_RESOURCE_LIMITS_MEMORY=1Gi

# Load Balancer Configuration
LB_ENABLED=true
LB_ALGORITHM=round_robin
LB_HEALTH_CHECK_PATH=/health

# ============================================================================
# BACKUP AND RECOVERY
# ============================================================================

# Backup Configuration
BACKUP_ENABLED=true
BACKUP_SCHEDULE=0 2 * * *
BACKUP_RETENTION_DAYS=30
BACKUP_STORAGE_PATH=/backups

# Recovery Configuration
RECOVERY_ENABLED=true
RECOVERY_TIMEOUT=300
AUTO_RECOVERY_ENABLED=false

# ============================================================================
# DEVELOPMENT AND TESTING
# ============================================================================

# Development Settings (set to false in production)
DEBUG=false
ENABLE_TESTING_ENDPOINTS=false
ENABLE_PROFILING=false
ENABLE_DEBUG_LOGS=false

# Testing Configuration
TEST_DATABASE_URL=postgresql://test:test@localhost:5432/test_kgas
TEST_REDIS_URL=redis://localhost:6379/1
PYTEST_TIMEOUT=300

# ============================================================================
# FEATURE FLAGS
# ============================================================================

# Phase 4 Feature Flags
PHASE4_ERROR_HANDLING=true
PHASE4_PERFORMANCE_OPTIMIZATION=true
PHASE4_SECURITY_HARDENING=true
PHASE4_PRODUCTION_MONITORING=true
PHASE4_HEALTH_ENDPOINTS=true

# Advanced Features
ADVANCED_ANALYTICS=true
REAL_TIME_MONITORING=true
AUTO_SCALING=true
CIRCUIT_BREAKER=true
RETRY_LOGIC=true

# ============================================================================
# COMPLIANCE AND GOVERNANCE
# ============================================================================

# Compliance Settings
GDPR_COMPLIANCE=true
SOC2_COMPLIANCE=true
HIPAA_COMPLIANCE=false
PCI_COMPLIANCE=false

# Data Retention
DATA_RETENTION_DAYS=365
LOG_RETENTION_DAYS=90
AUDIT_RETENTION_DAYS=2555

# Privacy Settings
ANONYMIZE_LOGS=true
ENCRYPT_PII=true
MASK_SENSITIVE_DATA=true
</file>

<file path="validate_phase4.py">
#!/usr/bin/env python3
"""
Phase 4 Production Readiness Validation
Validates all Phase 4 implementation requirements and production readiness.
"""

import os
import sys
import json
import time
import asyncio
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

def validate_docker_configuration():
    """Validate Docker configuration files."""
    print("🐳 Validating Docker Configuration...")
    
    results = []
    
    # Check Dockerfile
    dockerfile_path = Path("docker/Dockerfile")
    if dockerfile_path.exists():
        with open(dockerfile_path, 'r') as f:
            dockerfile_content = f.read()
        
        # Check for security best practices
        checks = [
            ("Non-root user", "USER kgas" in dockerfile_content),
            ("Health check", "HEALTHCHECK" in dockerfile_content),
            ("Exposed port", "EXPOSE 8000" in dockerfile_content),
            ("Working directory", "WORKDIR /app" in dockerfile_content),
            ("Multi-stage build", "FROM python:3.11-slim" in dockerfile_content)
        ]
        
        for check_name, passed in checks:
            results.append({
                "check": f"dockerfile_{check_name.lower().replace(' ', '_')}",
                "status": "✅ PASSED" if passed else "❌ FAILED",
                "details": f"Dockerfile {check_name}: {'Present' if passed else 'Missing'}"
            })
    else:
        results.append({
            "check": "dockerfile_exists",
            "status": "❌ FAILED",
            "details": "Dockerfile not found at docker/Dockerfile"
        })
    
    # Check docker-compose.production.yml
    compose_path = Path("docker/docker-compose.production.yml")
    if compose_path.exists():
        results.append({
            "check": "docker_compose_production",
            "status": "✅ PASSED",
            "details": "Production docker-compose configuration found"
        })
    else:
        results.append({
            "check": "docker_compose_production",
            "status": "❌ FAILED",
            "details": "Production docker-compose.yml not found"
        })
    
    return results

def validate_kubernetes_configuration():
    """Validate Kubernetes configuration files."""
    print("☸️ Validating Kubernetes Configuration...")
    
    results = []
    
    # Check k8s directory
    k8s_path = Path("k8s")
    if k8s_path.exists():
        # Check for required files
        required_files = [
            "deployment.yaml",
            "service.yaml",
            "configmap.yaml",
            "secret.yaml"
        ]
        
        for file_name in required_files:
            file_path = k8s_path / file_name
            if file_path.exists():
                results.append({
                    "check": f"k8s_{file_name.replace('.', '_')}",
                    "status": "✅ PASSED",
                    "details": f"Kubernetes {file_name} configuration found"
                })
            else:
                results.append({
                    "check": f"k8s_{file_name.replace('.', '_')}",
                    "status": "❌ FAILED",
                    "details": f"Kubernetes {file_name} configuration missing"
                })
    else:
        results.append({
            "check": "k8s_directory",
            "status": "❌ FAILED",
            "details": "Kubernetes configuration directory not found"
        })
    
    return results

def validate_cicd_pipeline():
    """Validate CI/CD pipeline configuration."""
    print("🔄 Validating CI/CD Pipeline...")
    
    results = []
    
    # Check GitHub Actions workflow
    workflow_path = Path(".github/workflows/production-deploy.yml")
    if workflow_path.exists():
        with open(workflow_path, 'r') as f:
            workflow_content = f.read()
        
        # Check for required components
        checks = [
            ("Test job", "test:" in workflow_content),
            ("Build job", "build:" in workflow_content),
            ("Deploy job", "deploy:" in workflow_content),
            ("Security scan", "trivy" in workflow_content or "security" in workflow_content.lower()),
            ("Coverage upload", "codecov" in workflow_content or "coverage" in workflow_content.lower())
        ]
        
        for check_name, passed in checks:
            results.append({
                "check": f"cicd_{check_name.lower().replace(' ', '_')}",
                "status": "✅ PASSED" if passed else "❌ FAILED",
                "details": f"CI/CD {check_name}: {'Present' if passed else 'Missing'}"
            })
    else:
        results.append({
            "check": "cicd_workflow",
            "status": "❌ FAILED",
            "details": "GitHub Actions workflow not found"
        })
    
    return results

def validate_error_handling():
    """Validate error handling implementation."""
    print("🔧 Validating Error Handling...")
    
    results = []
    
    # Check error handler file
    error_handler_path = Path("src/core/error_handler.py")
    if error_handler_path.exists():
        with open(error_handler_path, 'r') as f:
            error_handler_content = f.read()
        
        # Check for required components
        checks = [
            ("ProductionErrorHandler class", "class ProductionErrorHandler" in error_handler_content),
            ("Circuit breaker", "circuit_breaker" in error_handler_content),
            ("Retry logic", "retry_with_backoff" in error_handler_content),
            ("Error registry", "error_registry" in error_handler_content),
            ("Custom exceptions", "class ProductionDeploymentError" in error_handler_content)
        ]
        
        for check_name, passed in checks:
            results.append({
                "check": f"error_handling_{check_name.lower().replace(' ', '_')}",
                "status": "✅ PASSED" if passed else "❌ FAILED",
                "details": f"Error handling {check_name}: {'Present' if passed else 'Missing'}"
            })
    else:
        results.append({
            "check": "error_handler_file",
            "status": "❌ FAILED",
            "details": "Error handler file not found at src/core/error_handler.py"
        })
    
    return results

def validate_performance_optimization():
    """Validate performance optimization implementation."""
    print("⚡ Validating Performance Optimization...")
    
    results = []
    
    # Check performance optimizer file
    perf_optimizer_path = Path("src/core/performance_optimizer.py")
    if perf_optimizer_path.exists():
        with open(perf_optimizer_path, 'r') as f:
            perf_content = f.read()
        
        # Check for required components
        checks = [
            ("PerformanceOptimizer class", "class PerformanceOptimizer" in perf_content),
            ("Performance profiling", "profile_operation" in perf_content),
            ("Cache manager", "CacheManager" in perf_content),
            ("Connection pool manager", "ConnectionPoolManager" in perf_content),
            ("Query optimizer", "QueryOptimizer" in perf_content)
        ]
        
        for check_name, passed in checks:
            results.append({
                "check": f"performance_{check_name.lower().replace(' ', '_')}",
                "status": "✅ PASSED" if passed else "❌ FAILED",
                "details": f"Performance optimization {check_name}: {'Present' if passed else 'Missing'}"
            })
    else:
        results.append({
            "check": "performance_optimizer_file",
            "status": "❌ FAILED",
            "details": "Performance optimizer file not found at src/core/performance_optimizer.py"
        })
    
    return results

def validate_security_hardening():
    """Validate security hardening implementation."""
    print("🔐 Validating Security Hardening...")
    
    results = []
    
    # Check security manager file
    security_manager_path = Path("src/core/security_manager.py")
    if security_manager_path.exists():
        with open(security_manager_path, 'r') as f:
            security_content = f.read()
        
        # Check for required components
        checks = [
            ("SecurityManager class", "class SecurityManager" in security_content),
            ("JWT authentication", "generate_jwt_token" in security_content),
            ("Password hashing", "bcrypt" in security_content),
            ("Rate limiting", "rate_limit_check" in security_content),
            ("Data encryption", "encrypt_sensitive_data" in security_content),
            ("Audit logging", "_log_security_event" in security_content)
        ]
        
        for check_name, passed in checks:
            results.append({
                "check": f"security_{check_name.lower().replace(' ', '_')}",
                "status": "✅ PASSED" if passed else "❌ FAILED",
                "details": f"Security hardening {check_name}: {'Present' if passed else 'Missing'}"
            })
    else:
        results.append({
            "check": "security_manager_file",
            "status": "❌ FAILED",
            "details": "Security manager file not found at src/core/security_manager.py"
        })
    
    return results

def validate_production_monitoring():
    """Validate production monitoring implementation."""
    print("📊 Validating Production Monitoring...")
    
    results = []
    
    # Check production monitoring file
    monitoring_path = Path("src/monitoring/production_monitoring.py")
    if monitoring_path.exists():
        with open(monitoring_path, 'r') as f:
            monitoring_content = f.read()
        
        # Check for required components
        checks = [
            ("ProductionMonitoring class", "class ProductionMonitoring" in monitoring_content),
            ("Alert system", "Alert" in monitoring_content),
            ("Health checks", "HealthCheck" in monitoring_content),
            ("Notification channels", "AlertChannel" in monitoring_content),
            ("Metric thresholds", "MetricThreshold" in monitoring_content)
        ]
        
        for check_name, passed in checks:
            results.append({
                "check": f"monitoring_{check_name.lower().replace(' ', '_')}",
                "status": "✅ PASSED" if passed else "❌ FAILED",
                "details": f"Production monitoring {check_name}: {'Present' if passed else 'Missing'}"
            })
    else:
        results.append({
            "check": "production_monitoring_file",
            "status": "❌ FAILED",
            "details": "Production monitoring file not found at src/monitoring/production_monitoring.py"
        })
    
    return results

def validate_health_endpoints():
    """Validate health check endpoints."""
    print("🏥 Validating Health Endpoints...")
    
    results = []
    
    # Check main.py for health endpoints
    main_path = Path("main.py")
    if main_path.exists():
        with open(main_path, 'r') as f:
            main_content = f.read()
        
        # Check for health endpoints
        checks = [
            ("Health endpoint", "/health" in main_content),
            ("Ready endpoint", "/ready" in main_content),
            ("Metrics endpoint", "/metrics" in main_content)
        ]
        
        for check_name, passed in checks:
            results.append({
                "check": f"health_{check_name.lower().replace(' ', '_')}",
                "status": "✅ PASSED" if passed else "❌ FAILED",
                "details": f"Health endpoint {check_name}: {'Present' if passed else 'Missing'}"
            })
    else:
        results.append({
            "check": "main_file",
            "status": "❌ FAILED",
            "details": "main.py file not found"
        })
    
    return results

def validate_environment_configuration():
    """Validate environment configuration."""
    print("🌍 Validating Environment Configuration...")
    
    results = []
    
    # Check .env.example
    env_example_path = Path(".env.example")
    if env_example_path.exists():
        with open(env_example_path, 'r') as f:
            env_content = f.read()
        
        # Check for required environment variables
        required_vars = [
            "NEO4J_URI",
            "REDIS_URL",
            "SECRET_KEY",
            "API_KEY",
            "PROMETHEUS_METRICS_PORT",
            "ENVIRONMENT"
        ]
        
        for var in required_vars:
            if var in env_content:
                results.append({
                    "check": f"env_{var.lower()}",
                    "status": "✅ PASSED",
                    "details": f"Environment variable {var} documented"
                })
            else:
                results.append({
                    "check": f"env_{var.lower()}",
                    "status": "❌ FAILED",
                    "details": f"Environment variable {var} not documented"
                })
    else:
        results.append({
            "check": "env_example_file",
            "status": "❌ FAILED",
            "details": ".env.example file not found"
        })
    
    return results

def main():
    """Run Phase 4 validation."""
    print("🚀 Starting Phase 4 Production Readiness Validation")
    print("=" * 80)
    
    start_time = time.time()
    
    # Run all validation checks
    validation_functions = [
        validate_docker_configuration,
        validate_kubernetes_configuration,
        validate_cicd_pipeline,
        validate_error_handling,
        validate_performance_optimization,
        validate_security_hardening,
        validate_production_monitoring,
        validate_health_endpoints,
        validate_environment_configuration
    ]
    
    all_results = []
    
    for validation_func in validation_functions:
        try:
            results = validation_func()
            all_results.extend(results)
        except Exception as e:
            print(f"❌ Error in {validation_func.__name__}: {e}")
            all_results.append({
                "check": validation_func.__name__,
                "status": "❌ FAILED",
                "details": f"Validation error: {e}"
            })
    
    # Calculate summary
    passed_checks = [r for r in all_results if r["status"].startswith("✅")]
    failed_checks = [r for r in all_results if r["status"].startswith("❌")]
    
    execution_time = time.time() - start_time
    
    # Print results
    print(f"\n📊 Phase 4 Validation Results:")
    print(f"Total Checks: {len(all_results)}")
    print(f"Passed: {len(passed_checks)}")
    print(f"Failed: {len(failed_checks)}")
    print(f"Execution Time: {execution_time:.2f} seconds")
    
    if failed_checks:
        print(f"\n❌ Failed Checks:")
        for check in failed_checks:
            print(f"  - {check['check']}: {check['details']}")
    
    if len(passed_checks) == len(all_results):
        print(f"\n🎉 ALL PHASE 4 VALIDATION CHECKS PASSED!")
        print(f"✅ Phase 4 Production Readiness implementation is complete")
    else:
        print(f"\n⚠️ Phase 4 validation incomplete - {len(failed_checks)} checks failed")
    
    # Save results
    validation_results = {
        "timestamp": datetime.now().isoformat(),
        "phase": "Phase 4 Production Readiness",
        "total_checks": len(all_results),
        "passed_checks": len(passed_checks),
        "failed_checks": len(failed_checks),
        "execution_time": execution_time,
        "results": all_results
    }
    
    with open("phase4_validation_results.json", "w") as f:
        json.dump(validation_results, f, indent=2)
    
    print(f"\n📄 Detailed results saved to: phase4_validation_results.json")
    
    return len(failed_checks) == 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
</file>

<file path="src/core/error_handler.py">
"""
Advanced Error Handling and Recovery System
Provides comprehensive error handling with automatic recovery mechanisms.
"""

import logging
import traceback
from typing import Dict, Any, Optional, Callable, Type
from datetime import datetime
import time
import asyncio
from functools import wraps
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict

logger = logging.getLogger(__name__)

class ErrorSeverity(Enum):
    """Error severity levels for categorization."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class RecoveryStrategy(Enum):
    """Recovery strategies for different error types."""
    RETRY = "retry"
    FALLBACK = "fallback"
    CIRCUIT_BREAKER = "circuit_breaker"
    GRACEFUL_DEGRADATION = "graceful_degradation"
    FAIL_FAST = "fail_fast"

@dataclass
class ErrorRecord:
    """Record of an error occurrence with metadata."""
    error_type: str
    error_message: str
    severity: ErrorSeverity
    timestamp: datetime
    stack_trace: str
    context: Dict[str, Any] = field(default_factory=dict)
    recovery_attempted: bool = False
    recovery_successful: bool = False
    retry_count: int = 0

class ProductionErrorHandler:
    """
    Production-grade error handler with comprehensive error management.
    
    Follows fail-fast architecture - never hide critical errors.
    Provides automatic recovery only for non-critical failures.
    """
    
    def __init__(self):
        self.error_registry: Dict[str, ErrorRecord] = {}
        self.circuit_breakers: Dict[str, Dict[str, Any]] = defaultdict(lambda: {
            'failure_count': 0,
            'last_failure_time': None,
            'state': 'closed'  # closed, open, half-open
        })
        self.retry_policies: Dict[Type[Exception], Dict[str, Any]] = {}
        self.fallback_handlers: Dict[Type[Exception], Callable] = {}
        self._setup_default_policies()
    
    def _setup_default_policies(self):
        """Set up default error handling policies."""
        # Network errors - retry with exponential backoff
        self.retry_policies[ConnectionError] = {
            'max_retries': 3,
            'backoff_multiplier': 2,
            'base_delay': 1
        }
        
        # API rate limits - retry with longer delay
        self.retry_policies[Exception] = {  # Generic for rate limit errors
            'max_retries': 5,
            'backoff_multiplier': 1.5,
            'base_delay': 5
        }
    
    def register_error(self, error: Exception, context: Dict[str, Any] = None) -> str:
        """
        Register an error occurrence with full context.
        
        Args:
            error: The exception that occurred
            context: Additional context information
            
        Returns:
            Error ID for tracking
        """
        error_id = f"error_{int(time.time() * 1000)}"
        
        # Determine severity based on error type
        severity = self._determine_severity(error)
        
        # Create error record
        error_record = ErrorRecord(
            error_type=type(error).__name__,
            error_message=str(error),
            severity=severity,
            timestamp=datetime.now(),
            stack_trace=traceback.format_exc(),
            context=context or {}
        )
        
        self.error_registry[error_id] = error_record
        
        # Log error with appropriate level
        self._log_error(error_record, error_id)
        
        return error_id
    
    def _determine_severity(self, error: Exception) -> ErrorSeverity:
        """Determine error severity based on error type and context."""
        critical_errors = [
            'DatabaseConnectionError',
            'ConfigurationError',
            'SecurityError',
            'DataCorruptionError'
        ]
        
        high_errors = [
            'APIError',
            'AuthenticationError',
            'ValidationError'
        ]
        
        error_type = type(error).__name__
        
        if error_type in critical_errors:
            return ErrorSeverity.CRITICAL
        elif error_type in high_errors:
            return ErrorSeverity.HIGH
        elif isinstance(error, (ConnectionError, TimeoutError)):
            return ErrorSeverity.MEDIUM
        else:
            return ErrorSeverity.LOW
    
    def _log_error(self, error_record: ErrorRecord, error_id: str):
        """Log error with appropriate severity level."""
        log_message = f"Error [{error_id}]: {error_record.error_message}"
        
        if error_record.severity == ErrorSeverity.CRITICAL:
            logger.critical(log_message, extra={
                'error_id': error_id,
                'error_type': error_record.error_type,
                'stack_trace': error_record.stack_trace,
                'context': error_record.context
            })
        elif error_record.severity == ErrorSeverity.HIGH:
            logger.error(log_message, extra={
                'error_id': error_id,
                'error_type': error_record.error_type,
                'context': error_record.context
            })
        elif error_record.severity == ErrorSeverity.MEDIUM:
            logger.warning(log_message, extra={
                'error_id': error_id,
                'error_type': error_record.error_type
            })
        else:
            logger.info(log_message, extra={
                'error_id': error_id,
                'error_type': error_record.error_type
            })
    
    def handle_error(self, error: Exception, context: Dict[str, Any] = None) -> Optional[Any]:
        """
        Handle an error with appropriate recovery strategy.
        
        Args:
            error: The exception to handle
            context: Additional context information
            
        Returns:
            Recovery result if successful, None if failed
            
        Raises:
            Exception: Re-raises critical errors following fail-fast principle
        """
        error_id = self.register_error(error, context)
        error_record = self.error_registry[error_id]
        
        # Critical errors must fail fast - no recovery
        if error_record.severity == ErrorSeverity.CRITICAL:
            raise error
        
        # Attempt recovery for non-critical errors
        recovery_result = self._attempt_recovery(error, error_record, context)
        
        # Update error record with recovery attempt
        error_record.recovery_attempted = True
        error_record.recovery_successful = recovery_result is not None
        
        return recovery_result
    
    def _attempt_recovery(self, error: Exception, error_record: ErrorRecord, 
                         context: Dict[str, Any] = None) -> Optional[Any]:
        """
        Attempt to recover from an error using appropriate strategy.
        
        Args:
            error: The exception to recover from
            error_record: The error record
            context: Additional context
            
        Returns:
            Recovery result if successful, None if failed
        """
        error_type = type(error)
        
        # Check if we have a specific recovery strategy
        if error_type in self.retry_policies:
            return self._retry_with_backoff(error, error_record, context)
        
        if error_type in self.fallback_handlers:
            return self._execute_fallback(error, error_record, context)
        
        # Default: no recovery for unknown errors
        return None
    
    def _retry_with_backoff(self, error: Exception, error_record: ErrorRecord, 
                           context: Dict[str, Any] = None) -> Optional[Any]:
        """
        Retry operation with exponential backoff.
        
        Args:
            error: The exception that occurred
            error_record: The error record
            context: Additional context including retry function
            
        Returns:
            Result of successful retry, None if all retries failed
        """
        if not context or 'retry_function' not in context:
            logger.warning(f"No retry function provided for error: {error}")
            return None
        
        error_type = type(error)
        retry_policy = self.retry_policies[error_type]
        
        max_retries = retry_policy['max_retries']
        backoff_multiplier = retry_policy['backoff_multiplier']
        base_delay = retry_policy['base_delay']
        
        retry_function = context['retry_function']
        retry_args = context.get('retry_args', ())
        retry_kwargs = context.get('retry_kwargs', {})
        
        for attempt in range(max_retries):
            # Calculate delay with exponential backoff
            delay = base_delay * (backoff_multiplier ** attempt)
            
            logger.info(f"Retrying operation after {delay}s delay (attempt {attempt + 1}/{max_retries})")
            time.sleep(delay)
            
            try:
                result = retry_function(*retry_args, **retry_kwargs)
                logger.info(f"Operation succeeded on retry attempt {attempt + 1}")
                return result
            except Exception as retry_error:
                logger.warning(f"Retry attempt {attempt + 1} failed: {retry_error}")
                error_record.retry_count = attempt + 1
                
                # If this is the last attempt, log the final failure
                if attempt == max_retries - 1:
                    logger.error(f"All {max_retries} retry attempts failed for error: {error}")
        
        return None
    
    def _execute_fallback(self, error: Exception, error_record: ErrorRecord, 
                         context: Dict[str, Any] = None) -> Optional[Any]:
        """
        Execute fallback handler for an error.
        
        Args:
            error: The exception that occurred
            error_record: The error record
            context: Additional context
            
        Returns:
            Result of fallback handler, None if failed
        """
        error_type = type(error)
        fallback_handler = self.fallback_handlers[error_type]
        
        try:
            logger.info(f"Executing fallback handler for error: {error}")
            result = fallback_handler(error, context)
            logger.info("Fallback handler executed successfully")
            return result
        except Exception as fallback_error:
            logger.error(f"Fallback handler failed: {fallback_error}")
            return None
    
    def circuit_breaker(self, service_name: str, failure_threshold: int = 5, 
                       recovery_timeout: int = 60):
        """
        Circuit breaker decorator for service calls.
        
        Args:
            service_name: Name of the service
            failure_threshold: Number of failures before opening circuit
            recovery_timeout: Time in seconds before attempting recovery
        """
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                circuit = self.circuit_breakers[service_name]
                
                # Check circuit state
                if circuit['state'] == 'open':
                    # Check if we should attempt recovery
                    if (time.time() - circuit['last_failure_time']) > recovery_timeout:
                        circuit['state'] = 'half-open'
                        logger.info(f"Circuit breaker for {service_name} moved to half-open state")
                    else:
                        raise Exception(f"Circuit breaker open for service: {service_name}")
                
                try:
                    result = await func(*args, **kwargs)
                    
                    # Success - reset circuit if it was half-open
                    if circuit['state'] == 'half-open':
                        circuit['state'] = 'closed'
                        circuit['failure_count'] = 0
                        logger.info(f"Circuit breaker for {service_name} closed successfully")
                    
                    return result
                
                except Exception as e:
                    circuit['failure_count'] += 1
                    circuit['last_failure_time'] = time.time()
                    
                    # Check if we should open the circuit
                    if circuit['failure_count'] >= failure_threshold:
                        circuit['state'] = 'open'
                        logger.error(f"Circuit breaker opened for service: {service_name}")
                    
                    # Register and handle the error
                    self.handle_error(e, {
                        'service_name': service_name,
                        'circuit_state': circuit['state']
                    })
                    
                    raise
            
            return wrapper
        return decorator
    
    def get_error_statistics(self) -> Dict[str, Any]:
        """
        Get comprehensive error statistics.
        
        Returns:
            Dictionary with error statistics and health metrics
        """
        total_errors = len(self.error_registry)
        
        if total_errors == 0:
            return {
                'total_errors': 0,
                'error_rate': 0.0,
                'recovery_rate': 0.0,
                'critical_errors': 0,
                'circuit_breaker_states': dict(self.circuit_breakers)
            }
        
        # Calculate statistics
        severity_counts = defaultdict(int)
        recovery_attempts = 0
        successful_recoveries = 0
        
        for error_record in self.error_registry.values():
            severity_counts[error_record.severity.value] += 1
            
            if error_record.recovery_attempted:
                recovery_attempts += 1
                if error_record.recovery_successful:
                    successful_recoveries += 1
        
        recovery_rate = (successful_recoveries / recovery_attempts) * 100 if recovery_attempts > 0 else 0
        
        return {
            'total_errors': total_errors,
            'error_rate': total_errors,  # This would be calculated against total requests in production
            'recovery_rate': recovery_rate,
            'severity_breakdown': dict(severity_counts),
            'critical_errors': severity_counts[ErrorSeverity.CRITICAL.value],
            'recovery_attempts': recovery_attempts,
            'successful_recoveries': successful_recoveries,
            'circuit_breaker_states': dict(self.circuit_breakers)
        }

# Global error handler instance
error_handler = ProductionErrorHandler()

# Decorator for automatic error handling
def handle_errors(severity: ErrorSeverity = ErrorSeverity.MEDIUM, 
                 retry_on_failure: bool = True):
    """
    Decorator for automatic error handling with customizable behavior.
    
    Args:
        severity: Expected error severity level
        retry_on_failure: Whether to attempt automatic retry
    """
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                context = {
                    'function_name': func.__name__,
                    'args': str(args),
                    'kwargs': str(kwargs)
                }
                
                if retry_on_failure:
                    context['retry_function'] = func
                    context['retry_args'] = args
                    context['retry_kwargs'] = kwargs
                
                result = error_handler.handle_error(e, context)
                
                # If recovery failed and this is a critical error, re-raise
                if result is None and severity == ErrorSeverity.CRITICAL:
                    raise
                
                return result
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                context = {
                    'function_name': func.__name__,
                    'args': str(args),
                    'kwargs': str(kwargs)
                }
                
                if retry_on_failure:
                    context['retry_function'] = func
                    context['retry_args'] = args
                    context['retry_kwargs'] = kwargs
                
                result = error_handler.handle_error(e, context)
                
                # If recovery failed and this is a critical error, re-raise
                if result is None and severity == ErrorSeverity.CRITICAL:
                    raise
                
                return result
        
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    return decorator

# Custom exceptions for fail-fast architecture
class ProductionDeploymentError(Exception):
    """Critical error during production deployment."""
    pass

class SecurityValidationError(Exception):
    """Critical security validation failure."""
    pass

class DataIntegrityError(Exception):
    """Critical data integrity violation."""
    pass

class ServiceUnavailableError(Exception):
    """Service temporarily unavailable."""
    pass

class ConfigurationError(Exception):
    """Critical configuration error."""
    pass
</file>

<file path="docker/Dockerfile">
FROM python:3.11-slim

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONPATH=/app
ENV NEO4J_URI=bolt://neo4j:7687
ENV REDIS_URL=redis://redis:6379

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN groupadd -r kgas && useradd -r -g kgas kgas

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Change ownership to kgas user
RUN chown -R kgas:kgas /app

# Switch to non-root user
USER kgas

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# Run application
CMD ["python", "main.py"]
</file>

</files>
This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: src/core/distributed_transaction_manager.py, src/core/thread_safe_service_manager.py, src/core/error_taxonomy.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
src/
  core/
    distributed_transaction_manager.py
    error_taxonomy.py
    thread_safe_service_manager.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/core/distributed_transaction_manager.py">
"""
Distributed Transaction Manager for Neo4j and SQLite consistency.

Implements a two-phase commit protocol to ensure both databases
maintain consistency during operations.
"""

import asyncio
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from contextlib import asynccontextmanager
import logging
from dataclasses import dataclass, field
from enum import Enum

from neo4j import AsyncSession
import aiosqlite

logger = logging.getLogger(__name__)


class TransactionStatus(Enum):
    """Transaction status enum."""
    ACTIVE = "active"
    PREPARING = "preparing"
    PREPARED = "prepared"
    COMMITTING = "committing"
    COMMITTED = "committed"
    ROLLING_BACK = "rolling_back"
    ROLLED_BACK = "rolled_back"
    FAILED = "failed"
    PARTIAL_FAILURE = "partial_failure"


@dataclass
class TransactionState:
    """Tracks the state of a distributed transaction."""
    tx_id: str
    status: TransactionStatus = TransactionStatus.ACTIVE
    created_at: datetime = field(default_factory=datetime.now)
    neo4j_prepared: bool = False
    sqlite_prepared: bool = False
    neo4j_committed: bool = False
    sqlite_committed: bool = False
    neo4j_session: Optional[AsyncSession] = None
    neo4j_tx: Optional[Any] = None  # Neo4j transaction object
    sqlite_conn: Optional[aiosqlite.Connection] = None
    neo4j_operations: List[Dict[str, Any]] = field(default_factory=list)
    sqlite_operations: List[Dict[str, Any]] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for external use."""
        return {
            "tx_id": self.tx_id,
            "status": self.status.value,
            "created_at": self.created_at.isoformat(),
            "neo4j_prepared": self.neo4j_prepared,
            "sqlite_prepared": self.sqlite_prepared,
            "neo4j_committed": self.neo4j_committed,
            "sqlite_committed": self.sqlite_committed,
            "errors": self.errors
        }


class DistributedTransactionManager:
    """
    Manages distributed transactions across Neo4j and SQLite.
    
    Implements two-phase commit protocol:
    1. Prepare phase: Both databases prepare the transaction
    2. Commit phase: If both prepared successfully, commit both
    3. Rollback: If any failure, rollback both
    """
    
    def __init__(self, timeout_seconds: int = 30, cleanup_after_seconds: int = 3600):
        """
        Initialize the transaction manager.
        
        Args:
            timeout_seconds: Maximum time for a transaction
            cleanup_after_seconds: Time before cleaning up old transaction states
        """
        self.timeout_seconds = timeout_seconds
        self.cleanup_after_seconds = cleanup_after_seconds
        self._transactions: Dict[str, TransactionState] = {}
        self._lock = asyncio.Lock()
        
        # These should be injected or configured in production
        self._neo4j_driver = None
        self._sqlite_path = None
    
    async def _get_neo4j_session(self) -> AsyncSession:
        """Get a Neo4j session. Override in production."""
        if not self._neo4j_driver:
            raise RuntimeError("Neo4j driver not configured")
        return self._neo4j_driver.session()
    
    async def _get_sqlite_connection(self) -> aiosqlite.Connection:
        """Get a SQLite connection. Override in production."""
        if not self._sqlite_path:
            raise RuntimeError("SQLite path not configured")
        return await aiosqlite.connect(self._sqlite_path)
    
    async def begin_transaction(self, tx_id: str) -> Dict[str, Any]:
        """
        Begin a new distributed transaction.
        
        Args:
            tx_id: Unique transaction identifier
            
        Returns:
            Transaction state dictionary
        """
        async with self._lock:
            if tx_id in self._transactions:
                raise ValueError(f"Transaction {tx_id} already exists")
            
            state = TransactionState(tx_id=tx_id)
            self._transactions[tx_id] = state
            
            logger.info(f"Started distributed transaction: {tx_id}")
            return state.to_dict()
    
    async def prepare_neo4j(self, tx_id: str, operations: List[Dict[str, Any]]) -> None:
        """
        Prepare Neo4j operations as part of the transaction.
        
        Args:
            tx_id: Transaction identifier
            operations: List of Neo4j operations with 'query' and 'params'
        """
        async with self._lock:
            state = self._transactions.get(tx_id)
            if not state:
                raise ValueError(f"Transaction {tx_id} not found")
            
            if state.status != TransactionStatus.ACTIVE:
                raise ValueError(f"Transaction {tx_id} is not active")
        
        try:
            # Execute with timeout
            await asyncio.wait_for(
                self._execute_neo4j_prepare(state, operations),
                timeout=self.timeout_seconds
            )
        except asyncio.TimeoutError:
            state.status = TransactionStatus.FAILED
            state.errors.append("Neo4j prepare timeout")
            raise
        except Exception as e:
            state.status = TransactionStatus.FAILED
            state.errors.append(f"Neo4j prepare error: {str(e)}")
            raise
    
    async def _execute_neo4j_prepare(self, state: TransactionState, operations: List[Dict[str, Any]]) -> None:
        """Execute Neo4j prepare phase."""
        state.status = TransactionStatus.PREPARING
        
        # Get session if not already exists
        if not state.neo4j_session:
            state.neo4j_session = await self._get_neo4j_session()
        
        # Start transaction and keep it open
        state.neo4j_tx = await state.neo4j_session.begin_transaction()
        
        try:
            # Execute all operations in the transaction
            for op in operations:
                await state.neo4j_tx.run(op["query"], op.get("params", {}))
            
            # Store operations for potential retry
            state.neo4j_operations = operations
            
            # Don't commit yet - this is just prepare phase
            state.neo4j_prepared = True
            logger.info(f"Neo4j prepared for transaction: {state.tx_id}")
            
        except Exception as e:
            # Rollback on any error
            if state.neo4j_tx:
                await state.neo4j_tx.rollback()
                state.neo4j_tx = None
            raise
    
    async def prepare_sqlite(self, tx_id: str, operations: List[Dict[str, Any]]) -> None:
        """
        Prepare SQLite operations as part of the transaction.
        
        Args:
            tx_id: Transaction identifier
            operations: List of SQLite operations with 'query' and 'params'
        """
        async with self._lock:
            state = self._transactions.get(tx_id)
            if not state:
                raise ValueError(f"Transaction {tx_id} not found")
            
            if state.status not in [TransactionStatus.ACTIVE, TransactionStatus.PREPARING]:
                raise ValueError(f"Transaction {tx_id} is not in valid state for prepare")
        
        try:
            await asyncio.wait_for(
                self._execute_sqlite_prepare(state, operations),
                timeout=self.timeout_seconds
            )
        except asyncio.TimeoutError:
            state.status = TransactionStatus.FAILED
            state.errors.append("SQLite prepare timeout")
            raise
        except Exception as e:
            state.status = TransactionStatus.FAILED
            state.errors.append(f"SQLite prepare error: {str(e)}")
            raise
    
    async def _execute_sqlite_prepare(self, state: TransactionState, operations: List[Dict[str, Any]]) -> None:
        """Execute SQLite prepare phase."""
        # Get connection if not already exists
        if not state.sqlite_conn:
            state.sqlite_conn = await self._get_sqlite_connection()
        
        # Execute all operations within transaction
        await state.sqlite_conn.execute("BEGIN TRANSACTION")
        
        try:
            for op in operations:
                await state.sqlite_conn.execute(op["query"], op.get("params", []))
            
            # Store operations for potential retry
            state.sqlite_operations = operations
            
            # Don't commit yet - this is just prepare phase
            state.sqlite_prepared = True
            state.status = TransactionStatus.PREPARED
            logger.info(f"SQLite prepared for transaction: {state.tx_id}")
        except Exception:
            await state.sqlite_conn.execute("ROLLBACK")
            raise
    
    async def commit_all(self, tx_id: str) -> Dict[str, Any]:
        """
        Commit the distributed transaction on both databases.
        
        Args:
            tx_id: Transaction identifier
            
        Returns:
            Result dictionary with commit status
        """
        async with self._lock:
            state = self._transactions.get(tx_id)
            if not state:
                raise ValueError(f"Transaction {tx_id} not found")
            
            if not (state.neo4j_prepared and state.sqlite_prepared):
                raise ValueError(f"Transaction {tx_id} not fully prepared")
        
        state.status = TransactionStatus.COMMITTING
        result = {
            "tx_id": tx_id,
            "status": "unknown",
            "neo4j_committed": False,
            "sqlite_committed": False,
            "errors": []
        }
        
        try:
            # Commit Neo4j transaction
            if state.neo4j_tx:
                await state.neo4j_tx.commit()
                state.neo4j_committed = True
                result["neo4j_committed"] = True
                logger.info(f"Neo4j committed for transaction: {tx_id}")
            
            # Commit SQLite
            if state.sqlite_conn:
                await state.sqlite_conn.commit()
                state.sqlite_committed = True
                result["sqlite_committed"] = True
                logger.info(f"SQLite committed for transaction: {tx_id}")
            
            # If both committed successfully
            if state.neo4j_committed and state.sqlite_committed:
                state.status = TransactionStatus.COMMITTED
                result["status"] = "committed"
            else:
                state.status = TransactionStatus.PARTIAL_FAILURE
                result["status"] = "partial_failure"
                result["recovery_needed"] = True
                
        except Exception as e:
            state.status = TransactionStatus.PARTIAL_FAILURE
            state.errors.append(f"Commit error: {str(e)}")
            result["status"] = "partial_failure"
            result["recovery_needed"] = True
            result["errors"] = [f"SQLite commit failed: {str(e)}"]
            logger.error(f"Partial failure in transaction {tx_id}: {e}")
        
        finally:
            # Clean up connections
            await self._cleanup_transaction_resources(state)
        
        return result
    
    async def rollback_all(self, tx_id: str) -> Dict[str, Any]:
        """
        Rollback the distributed transaction on both databases.
        
        Args:
            tx_id: Transaction identifier
            
        Returns:
            Result dictionary with rollback status
        """
        async with self._lock:
            state = self._transactions.get(tx_id)
            if not state:
                raise ValueError(f"Transaction {tx_id} not found")
        
        state.status = TransactionStatus.ROLLING_BACK
        result = {
            "tx_id": tx_id,
            "status": "unknown",
            "reason": "timeout" if "timeout" in str(state.errors) else "error"
        }
        
        try:
            # Rollback Neo4j transaction
            if state.neo4j_tx:
                await state.neo4j_tx.rollback()
                logger.info(f"Neo4j rolled back for transaction: {tx_id}")
            
            # Rollback SQLite
            if state.sqlite_conn:
                await state.sqlite_conn.rollback()
                logger.info(f"SQLite rolled back for transaction: {tx_id}")
            
            state.status = TransactionStatus.ROLLED_BACK
            result["status"] = "rolled_back"
            
        except Exception as e:
            state.status = TransactionStatus.FAILED
            state.errors.append(f"Rollback error: {str(e)}")
            result["status"] = "rollback_failed"
            logger.error(f"Rollback failed for transaction {tx_id}: {e}")
        
        finally:
            # Clean up connections
            await self._cleanup_transaction_resources(state)
        
        return result
    
    async def get_transaction_state(self, tx_id: str) -> Optional[Dict[str, Any]]:
        """
        Get the current state of a transaction.
        
        Args:
            tx_id: Transaction identifier
            
        Returns:
            Transaction state dictionary or None if not found
        """
        async with self._lock:
            state = self._transactions.get(tx_id)
            return state.to_dict() if state else None
    
    async def cleanup_old_transactions(self) -> int:
        """
        Clean up old transaction states.
        
        Returns:
            Number of transactions cleaned up
        """
        async with self._lock:
            cutoff_time = datetime.now() - timedelta(seconds=self.cleanup_after_seconds)
            old_tx_ids = [
                tx_id for tx_id, state in self._transactions.items()
                if state.created_at < cutoff_time
            ]
            
            for tx_id in old_tx_ids:
                state = self._transactions[tx_id]
                await self._cleanup_transaction_resources(state)
                del self._transactions[tx_id]
            
            logger.info(f"Cleaned up {len(old_tx_ids)} old transactions")
            return len(old_tx_ids)
    
    async def _cleanup_transaction_resources(self, state: TransactionState) -> None:
        """Clean up resources associated with a transaction."""
        try:
            # Close Neo4j transaction if still open
            if state.neo4j_tx:
                try:
                    # Rollback if not already committed
                    if state.status not in [TransactionStatus.COMMITTED, TransactionStatus.ROLLED_BACK]:
                        await state.neo4j_tx.rollback()
                except Exception:
                    pass  # Transaction might already be closed
                state.neo4j_tx = None
        except Exception as e:
            logger.error(f"Error closing Neo4j transaction: {e}")
        
        try:
            if state.neo4j_session:
                await state.neo4j_session.close()
                state.neo4j_session = None
        except Exception as e:
            logger.error(f"Error closing Neo4j session: {e}")
        
        try:
            if state.sqlite_conn:
                await state.sqlite_conn.close()
                state.sqlite_conn = None
        except Exception as e:
            logger.error(f"Error closing SQLite connection: {e}")
</file>

<file path="src/core/error_taxonomy.py">
#!/usr/bin/env python3
"""
Centralized Error Taxonomy and Handling Framework

Provides standardized error classification, handling, and recovery patterns
for all KGAS services. Critical architectural fix for Phase RELIABILITY.

Replaces the inconsistent error handling across 802+ try blocks with
a unified taxonomy and recovery system.
"""

import asyncio
import logging
import traceback
import uuid
from datetime import datetime
from enum import Enum
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, Callable, List, Union
from contextlib import asynccontextmanager, contextmanager
import threading
from collections import defaultdict, deque

logger = logging.getLogger(__name__)


class ErrorSeverity(Enum):
    """Error severity levels for classification and response prioritization"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"
    CATASTROPHIC = "catastrophic"


class ErrorCategory(Enum):
    """Error categories for systematic classification"""
    DATA_CORRUPTION = "data_corruption"
    RESOURCE_EXHAUSTION = "resource_exhaustion"
    NETWORK_FAILURE = "network_failure"
    AUTHENTICATION_FAILURE = "authentication_failure"
    VALIDATION_FAILURE = "validation_failure"
    SYSTEM_FAILURE = "system_failure"
    DATABASE_FAILURE = "database_failure"
    SERVICE_UNAVAILABLE = "service_unavailable"
    CONFIGURATION_ERROR = "configuration_error"
    ACADEMIC_INTEGRITY = "academic_integrity"


class RecoveryStrategy(Enum):
    """Recovery strategies for different error types"""
    RETRY = "retry"
    FALLBACK = "fallback"
    CIRCUIT_BREAKER = "circuit_breaker"
    GRACEFUL_DEGRADATION = "graceful_degradation"
    ABORT_AND_ALERT = "abort_and_alert"
    ESCALATE = "escalate"


@dataclass
class KGASError:
    """Standardized error format for all system errors"""
    error_id: str
    category: ErrorCategory
    severity: ErrorSeverity
    message: str
    context: Dict[str, Any]
    timestamp: str
    service_name: str
    operation: str
    stack_trace: Optional[str] = None
    recovery_suggestions: List[str] = field(default_factory=list)
    recovery_attempts: int = 0
    max_recovery_attempts: int = 3
    tags: List[str] = field(default_factory=list)


@dataclass
class RecoveryResult:
    """Result of recovery attempt"""
    success: bool
    strategy_used: RecoveryStrategy
    error_id: str
    recovery_time: float
    message: str
    metadata: Dict[str, Any] = field(default_factory=dict)


class ErrorMetrics:
    """Track error metrics and patterns"""
    
    def __init__(self, max_history=1000):
        self.error_counts = defaultdict(int)
        self.error_history = deque(maxlen=max_history)
        self.recovery_stats = defaultdict(lambda: {"attempts": 0, "successes": 0})
        self._lock = threading.Lock()
    
    def record_error(self, error: KGASError):
        """Record error occurrence"""
        with self._lock:
            self.error_counts[error.category.value] += 1
            self.error_history.append({
                "error_id": error.error_id,
                "category": error.category.value,
                "severity": error.severity.value,
                "timestamp": error.timestamp,
                "service": error.service_name
            })
    
    def record_recovery(self, result: RecoveryResult):
        """Record recovery attempt result"""
        with self._lock:
            key = result.strategy_used.value
            self.recovery_stats[key]["attempts"] += 1
            if result.success:
                self.recovery_stats[key]["successes"] += 1
    
    def get_error_summary(self) -> Dict[str, Any]:
        """Get error summary statistics"""
        with self._lock:
            total_errors = sum(self.error_counts.values())
            return {
                "total_errors": total_errors,
                "error_breakdown": dict(self.error_counts),
                "recovery_success_rates": {
                    strategy: {
                        "success_rate": stats["successes"] / max(stats["attempts"], 1),
                        "total_attempts": stats["attempts"]
                    }
                    for strategy, stats in self.recovery_stats.items()
                }
            }


class CentralizedErrorHandler:
    """
    Central error handling with recovery patterns and escalation.
    
    Provides unified error taxonomy, classification, and recovery
    across all KGAS services and tools.
    """
    
    def __init__(self):
        self.error_registry: Dict[str, KGASError] = {}
        self.recovery_strategies: Dict[str, Callable] = {}
        self.error_metrics = ErrorMetrics()
        self.circuit_breakers: Dict[str, Dict] = {}
        self.escalation_handlers: List[Callable] = []
        self._lock = asyncio.Lock()
        
        # Setup default recovery strategies
        self._setup_default_recovery_strategies()
        
        logger.info("CentralizedErrorHandler initialized")
    
    def _setup_default_recovery_strategies(self):
        """Setup default recovery strategies for common error patterns"""
        # Fix: Register strategies using RecoveryStrategy enum values as keys
        self.register_recovery_strategy(RecoveryStrategy.CIRCUIT_BREAKER.value, self._recover_database_connection)
        self.register_recovery_strategy(RecoveryStrategy.GRACEFUL_DEGRADATION.value, self._recover_memory_exhaustion)
        self.register_recovery_strategy(RecoveryStrategy.RETRY.value, self._recover_network_timeout)
        self.register_recovery_strategy(RecoveryStrategy.FALLBACK.value, self._recover_service_unavailable)
        self.register_recovery_strategy(RecoveryStrategy.ABORT_AND_ALERT.value, self._recover_configuration_error)
        self.register_recovery_strategy(RecoveryStrategy.ESCALATE.value, self._handle_academic_integrity)
    
    async def handle_error(self, error: Exception, context: Dict[str, Any]) -> KGASError:
        """
        Handle error with standardized taxonomy and recovery.
        
        Args:
            error: The exception that occurred
            context: Context information (service, operation, etc.)
            
        Returns:
            KGASError: Classified and processed error
        """
        async with self._lock:
            # Classify error
            kgas_error = self._classify_error(error, context)
            
            # Record error
            self.error_registry[kgas_error.error_id] = kgas_error
            self.error_metrics.record_error(kgas_error)
            
            # Log error with full context
            await self._log_error(kgas_error)
            
            # Attempt recovery
            recovery_result = await self._attempt_recovery(kgas_error)
            
            # Record recovery attempt
            if recovery_result:
                self.error_metrics.record_recovery(recovery_result)
            
            # Escalate if critical or recovery failed
            if (kgas_error.severity in [ErrorSeverity.CRITICAL, ErrorSeverity.CATASTROPHIC] or 
                not (recovery_result and recovery_result.success)):
                await self._escalate_error(kgas_error)
            
            return kgas_error
    
    def _classify_error(self, error: Exception, context: Dict[str, Any]) -> KGASError:
        """Classify error into standardized taxonomy"""
        error_message = str(error)
        error_type = type(error).__name__
        
        # Determine category and severity based on error characteristics
        category, severity = self._determine_category_and_severity(error, error_message)
        
        # Generate recovery suggestions
        recovery_suggestions = self._generate_recovery_suggestions(category, error_type)
        
        return KGASError(
            error_id=str(uuid.uuid4()),
            category=category,
            severity=severity,
            message=error_message,
            context=context,
            timestamp=datetime.now().isoformat(),
            service_name=context.get("service_name", "unknown"),
            operation=context.get("operation", "unknown"),
            stack_trace=traceback.format_exc(),
            recovery_suggestions=recovery_suggestions,
            tags=self._generate_error_tags(error, context)
        )
    
    def _determine_category_and_severity(self, error: Exception, message: str) -> tuple[ErrorCategory, ErrorSeverity]:
        """Determine error category and severity from exception and message"""
        message_lower = message.lower()
        error_type = type(error).__name__
        
        # Data corruption patterns
        if any(keyword in message_lower for keyword in ["corruption", "integrity", "citation fabrication", "orphaned data"]):
            return ErrorCategory.DATA_CORRUPTION, ErrorSeverity.CATASTROPHIC
        
        # Academic integrity violations
        if any(keyword in message_lower for keyword in ["academic integrity", "citation", "provenance"]):
            return ErrorCategory.ACADEMIC_INTEGRITY, ErrorSeverity.CRITICAL
        
        # Database failures
        if any(keyword in message_lower for keyword in ["neo4j", "database", "transaction", "sql"]):
            return ErrorCategory.DATABASE_FAILURE, ErrorSeverity.HIGH
        
        # Resource exhaustion
        if any(keyword in message_lower for keyword in ["memory", "pool", "connection", "resource"]):
            return ErrorCategory.RESOURCE_EXHAUSTION, ErrorSeverity.HIGH
        
        # Network failures
        if any(keyword in message_lower for keyword in ["network", "timeout", "connection", "http"]):
            return ErrorCategory.NETWORK_FAILURE, ErrorSeverity.MEDIUM
        
        # Authentication failures
        if any(keyword in message_lower for keyword in ["auth", "credential", "permission", "access"]):
            return ErrorCategory.AUTHENTICATION_FAILURE, ErrorSeverity.MEDIUM
        
        # Validation failures
        if any(keyword in message_lower for keyword in ["validation", "invalid", "format", "schema"]):
            return ErrorCategory.VALIDATION_FAILURE, ErrorSeverity.LOW
        
        # Configuration errors
        if any(keyword in message_lower for keyword in ["config", "setting", "parameter", "missing"]):
            return ErrorCategory.CONFIGURATION_ERROR, ErrorSeverity.MEDIUM
        
        # Service unavailable
        if any(keyword in message_lower for keyword in ["service", "unavailable", "down", "unreachable"]):
            return ErrorCategory.SERVICE_UNAVAILABLE, ErrorSeverity.HIGH
        
        # Default classification
        return ErrorCategory.SYSTEM_FAILURE, ErrorSeverity.MEDIUM
    
    def _generate_recovery_suggestions(self, category: ErrorCategory, error_type: str) -> List[str]:
        """Generate contextual recovery suggestions"""
        suggestions = []
        
        if category == ErrorCategory.DATA_CORRUPTION:
            suggestions.extend([
                "Initiate immediate data integrity check",
                "Rollback to last known good state",
                "Alert academic integrity team",
                "Suspend data modifications until resolved"
            ])
        elif category == ErrorCategory.RESOURCE_EXHAUSTION:
            suggestions.extend([
                "Clear caches and free memory",
                "Restart connection pools",
                "Scale resources if possible",
                "Implement backpressure"
            ])
        elif category == ErrorCategory.DATABASE_FAILURE:
            suggestions.extend([
                "Check database connectivity",
                "Restart database connections",
                "Verify transaction state",
                "Switch to read-only mode if needed"
            ])
        elif category == ErrorCategory.NETWORK_FAILURE:
            suggestions.extend([
                "Retry with exponential backoff",
                "Check network connectivity",
                "Use cached data if available",
                "Switch to backup endpoints"
            ])
        elif category == ErrorCategory.CONFIGURATION_ERROR:
            suggestions.extend([
                "Verify configuration files",
                "Check environment variables",
                "Use default configurations",
                "Alert configuration management team"
            ])
        
        return suggestions
    
    def _generate_error_tags(self, error: Exception, context: Dict[str, Any]) -> List[str]:
        """Generate tags for error categorization and search"""
        tags = [type(error).__name__]
        
        if "service_name" in context:
            tags.append(f"service:{context['service_name']}")
        
        if "operation" in context:
            tags.append(f"operation:{context['operation']}")
        
        if "tool_id" in context:
            tags.append(f"tool:{context['tool_id']}")
        
        return tags
    
    async def _attempt_recovery(self, error: KGASError) -> Optional[RecoveryResult]:
        """Attempt to recover from error using registered strategies"""
        start_time = datetime.now()
        
        # Determine recovery strategy
        strategy = self._select_recovery_strategy(error)
        
        if not strategy:
            return None
        
        # Execute recovery strategy
        try:
            recovery_func = self.recovery_strategies.get(strategy.value)
            if recovery_func:
                success = await recovery_func(error)
                
                recovery_time = (datetime.now() - start_time).total_seconds()
                
                return RecoveryResult(
                    success=success,
                    strategy_used=strategy,
                    error_id=error.error_id,
                    recovery_time=recovery_time,
                    message=f"Recovery attempt using {strategy.value}",
                    metadata={"error_category": error.category.value}
                )
        
        except Exception as e:
            logger.error(f"Recovery strategy failed: {e}")
            return RecoveryResult(
                success=False,
                strategy_used=strategy,
                error_id=error.error_id,
                recovery_time=(datetime.now() - start_time).total_seconds(),
                message=f"Recovery failed: {str(e)}"
            )
        
        return None
    
    def _select_recovery_strategy(self, error: KGASError) -> Optional[RecoveryStrategy]:
        """Select appropriate recovery strategy based on error characteristics"""
        if error.category == ErrorCategory.DATA_CORRUPTION:
            return RecoveryStrategy.ABORT_AND_ALERT
        elif error.category == ErrorCategory.ACADEMIC_INTEGRITY:
            return RecoveryStrategy.ESCALATE
        elif error.category == ErrorCategory.RESOURCE_EXHAUSTION:
            return RecoveryStrategy.GRACEFUL_DEGRADATION
        elif error.category == ErrorCategory.DATABASE_FAILURE:
            return RecoveryStrategy.CIRCUIT_BREAKER
        elif error.category == ErrorCategory.NETWORK_FAILURE:
            return RecoveryStrategy.RETRY
        elif error.category == ErrorCategory.SERVICE_UNAVAILABLE:
            return RecoveryStrategy.FALLBACK
        
        return RecoveryStrategy.RETRY
    
    async def _log_error(self, error: KGASError):
        """Log error with appropriate level and context"""
        log_level = {
            ErrorSeverity.LOW: logging.INFO,
            ErrorSeverity.MEDIUM: logging.WARNING,
            ErrorSeverity.HIGH: logging.ERROR,
            ErrorSeverity.CRITICAL: logging.CRITICAL,
            ErrorSeverity.CATASTROPHIC: logging.CRITICAL
        }.get(error.severity, logging.ERROR)
        
        logger.log(log_level, 
                  f"[{error.error_id}] {error.category.value.upper()}: {error.message}",
                  extra={
                      "error_id": error.error_id,
                      "category": error.category.value,
                      "severity": error.severity.value,
                      "service": error.service_name,
                      "operation": error.operation,
                      "context": error.context,
                      "recovery_suggestions": error.recovery_suggestions
                  })
    
    async def _escalate_error(self, error: KGASError):
        """Escalate critical errors to registered handlers"""
        for handler in self.escalation_handlers:
            try:
                await handler(error)
            except Exception as e:
                logger.error(f"Error escalation handler failed: {e}")
    
    def register_recovery_strategy(self, error_pattern: str, strategy_func: Callable):
        """Register recovery strategy for specific error pattern"""
        self.recovery_strategies[error_pattern] = strategy_func
        logger.info(f"Registered recovery strategy for {error_pattern}")
    
    def register_escalation_handler(self, handler: Callable):
        """Register escalation handler for critical errors"""
        self.escalation_handlers.append(handler)
        logger.info("Registered error escalation handler")
    
    # Default recovery strategy implementations
    async def _recover_database_connection(self, error: KGASError) -> bool:
        """Recover from database connection failures"""
        try:
            # Attempt to reconnect to database
            logger.info(f"Attempting database reconnection for error {error.error_id}")
            
            # This would integrate with the actual database managers
            # For now, simulate recovery attempt
            await asyncio.sleep(1)  # Simulate reconnection time
            
            return True
        except Exception as e:
            logger.error(f"Database recovery failed: {e}")
            return False
    
    async def _recover_memory_exhaustion(self, error: KGASError) -> bool:
        """Recover from memory exhaustion"""
        try:
            logger.info(f"Attempting memory recovery for error {error.error_id}")
            
            # Clear caches, force garbage collection
            import gc
            gc.collect()
            
            return True
        except Exception as e:
            logger.error(f"Memory recovery failed: {e}")
            return False
    
    async def _recover_network_timeout(self, error: KGASError) -> bool:
        """Recover from network timeouts with retry"""
        try:
            logger.info(f"Attempting network recovery for error {error.error_id}")
            
            # Implement exponential backoff retry
            await asyncio.sleep(min(2 ** error.recovery_attempts, 10))
            
            return True
        except Exception as e:
            logger.error(f"Network recovery failed: {e}")
            return False
    
    async def _recover_service_unavailable(self, error: KGASError) -> bool:
        """Recover from service unavailability"""
        try:
            logger.info(f"Attempting service recovery for error {error.error_id}")
            
            # Check service health and attempt restart
            await asyncio.sleep(2)
            
            return True
        except Exception as e:
            logger.error(f"Service recovery failed: {e}")
            return False
    
    async def _recover_configuration_error(self, error: KGASError) -> bool:
        """Recovery from configuration errors"""
        try:
            logger.info(f"Attempting configuration recovery for error {error.error_id}")
            
            # Load default configuration or reload from source
            return True
        except Exception as e:
            logger.error(f"Configuration recovery failed: {e}")
            return False
    
    async def _handle_academic_integrity(self, error: KGASError) -> bool:
        """Handle academic integrity violations"""
        logger.critical(f"ACADEMIC INTEGRITY VIOLATION: {error.message}")
        
        # Academic integrity violations require manual intervention
        # This logs the issue and alerts appropriate personnel
        return False  # Never auto-recover from integrity violations
    
    def get_error_status(self, error_id: str) -> Optional[Dict[str, Any]]:
        """Get status of specific error"""
        error = self.error_registry.get(error_id)
        if not error:
            return None
        
        return {
            "error_id": error.error_id,
            "category": error.category.value,
            "severity": error.severity.value,
            "status": "resolved" if error.recovery_attempts > 0 else "active",
            "recovery_attempts": error.recovery_attempts,
            "timestamp": error.timestamp,
            "service": error.service_name,
            "operation": error.operation
        }
    
    def get_system_health_from_errors(self) -> Dict[str, Any]:
        """Get system health assessment based on error patterns"""
        metrics = self.error_metrics.get_error_summary()
        
        # Calculate health score based on error severity and frequency
        total_errors = metrics["total_errors"]
        catastrophic_errors = metrics["error_breakdown"].get("data_corruption", 0)
        critical_errors = metrics["error_breakdown"].get("academic_integrity", 0)
        
        if catastrophic_errors > 0:
            health_score = 1  # System unreliable due to data corruption
        elif critical_errors > 0:
            health_score = 2  # System compromised
        elif total_errors > 100:
            health_score = 4  # High error rate
        elif total_errors > 50:
            health_score = 6  # Moderate error rate
        elif total_errors > 10:
            health_score = 8  # Low error rate
        else:
            health_score = 10  # Healthy
        
        return {
            "health_score": health_score,
            "max_score": 10,
            "status": "healthy" if health_score >= 8 else "degraded" if health_score >= 6 else "unhealthy",
            "error_summary": metrics,
            "assessment_timestamp": datetime.now().isoformat()
        }


# Context managers for automatic error handling
@asynccontextmanager
async def handle_errors_async(service_name: str, operation: str, error_handler: CentralizedErrorHandler):
    """Async context manager for automatic error handling"""
    try:
        yield
    except Exception as e:
        context = {
            "service_name": service_name,
            "operation": operation,
            "timestamp": datetime.now().isoformat()
        }
        await error_handler.handle_error(e, context)
        raise  # Re-raise after handling


@contextmanager
def handle_errors_sync(service_name: str, operation: str, error_handler: CentralizedErrorHandler):
    """Sync context manager for automatic error handling"""
    try:
        yield
    except Exception as e:
        context = {
            "service_name": service_name,
            "operation": operation,
            "timestamp": datetime.now().isoformat()
        }
        # For sync context, we need to run async handler in thread
        import asyncio
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                asyncio.create_task(error_handler.handle_error(e, context))
            else:
                asyncio.run(error_handler.handle_error(e, context))
        except RuntimeError:
            # No event loop available, log error directly
            logger.error(f"Error in {service_name}.{operation}: {e}", exc_info=True)
        raise  # Re-raise after handling


# Global error handler instance
_global_error_handler = None


def get_global_error_handler() -> CentralizedErrorHandler:
    """Get or create global error handler instance"""
    global _global_error_handler
    if _global_error_handler is None:
        _global_error_handler = CentralizedErrorHandler()
    return _global_error_handler


# Decorator for automatic error handling
def handle_errors(service_name: str, operation: str = None):
    """Decorator for automatic error handling"""
    def decorator(func):
        func_name = operation or func.__name__
        
        if asyncio.iscoroutinefunction(func):
            async def async_wrapper(*args, **kwargs):
                async with handle_errors_async(service_name, func_name, get_global_error_handler()):
                    return await func(*args, **kwargs)
            return async_wrapper
        else:
            def sync_wrapper(*args, **kwargs):
                with handle_errors_sync(service_name, func_name, get_global_error_handler()):
                    return func(*args, **kwargs)
            return sync_wrapper
    
    return decorator
</file>

<file path="src/core/thread_safe_service_manager.py">
"""
Thread-safe Service Manager implementation.

Addresses race conditions and thread safety issues identified in the 
Phase RELIABILITY audit.
"""

import asyncio
import threading
from typing import Optional, Dict, Any, Type
from contextlib import asynccontextmanager
import logging

from .identity_service import IdentityService
from .provenance_service import ProvenanceService
from .quality_service import QualityService
from .workflow_state_service import WorkflowStateService
from .config_manager import get_config
from .logging_config import get_logger

logger = get_logger(__name__)


class ThreadSafeServiceManager:
    """
    Thread-safe service manager with proper locking and state management.
    
    Improvements over original ServiceManager:
    - Thread-safe service creation and access
    - Atomic operations for all state changes
    - Proper async/await support
    - Operation queuing to prevent race conditions
    - Comprehensive error handling
    """
    
    _instance = None
    _instance_lock = threading.RLock()  # Use RLock for nested locking
    
    def __new__(cls) -> 'ThreadSafeServiceManager':
        """Thread-safe singleton creation."""
        if cls._instance is None:
            with cls._instance_lock:
                # Double-check locking pattern
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """Initialize service manager with thread safety."""
        # Prevent multiple initialization
        with self._instance_lock:
            if self._initialized:
                return
                
            self._initialized = True
            self.logger = get_logger("core.thread_safe_service_manager")
            
            # Service instances
            self._services: Dict[str, Any] = {}
            self._service_locks: Dict[str, threading.RLock] = {}
            self._service_configs: Dict[str, Dict[str, Any]] = {}
            
            # Operation queue for serializing critical operations
            self._operation_queue = asyncio.Queue()
            self._operation_processor_task = None
            
            # Statistics
            self._stats = {
                'service_creations': 0,
                'lock_contentions': 0,
                'operations_processed': 0,
                'errors_handled': 0
            }
            
            self.logger.info("ThreadSafeServiceManager initialized")
    
    async def initialize(self, config: Optional[Dict[str, Any]] = None) -> bool:
        """
        Initialize service manager with configuration.
        
        Args:
            config: Optional configuration override
            
        Returns:
            True if initialization successful
        """
        try:
            with self._instance_lock:
                if config:
                    # Store configurations for services
                    self._service_configs.update(config)
                
                # Start operation processor
                if not self._operation_processor_task:
                    self._operation_processor_task = asyncio.create_task(
                        self._process_operations()
                    )
                
                self.logger.info("Service manager initialized successfully")
                return True
                
        except Exception as e:
            self.logger.error(f"Service manager initialization failed: {e}")
            return False
    
    async def get_service(self, service_name: str, 
                         service_class: Optional[Type] = None) -> Any:
        """
        Get or create a service instance thread-safely.
        
        Args:
            service_name: Name of the service
            service_class: Optional service class for creation
            
        Returns:
            Service instance
        """
        # Fast path - service already exists
        if service_name in self._services:
            return self._services[service_name]
        
        # Slow path - need to create service
        if service_name not in self._service_locks:
            with self._instance_lock:
                if service_name not in self._service_locks:
                    self._service_locks[service_name] = threading.RLock()
        
        # Create service with service-specific lock
        with self._service_locks[service_name]:
            # Double-check pattern
            if service_name in self._services:
                return self._services[service_name]
            
            # Track lock contention
            self._stats['lock_contentions'] += 1
            
            # Create service
            service = await self._create_service(service_name, service_class)
            self._services[service_name] = service
            self._stats['service_creations'] += 1
            
            return service
    
    async def _create_service(self, service_name: str, 
                            service_class: Optional[Type] = None) -> Any:
        """Create a service instance with proper initialization."""
        try:
            # Get service class if not provided
            if not service_class:
                service_class = self._get_service_class(service_name)
            
            if not service_class:
                raise ValueError(f"Unknown service: {service_name}")
            
            # Get configuration
            config = self._service_configs.get(service_name, {})
            
            # Create service instance
            if config:
                service = service_class(**config)
            else:
                service = service_class()
            
            # Initialize if needed
            if hasattr(service, 'initialize'):
                await service.initialize()
            
            self.logger.info(f"Created service: {service_name}")
            return service
            
        except Exception as e:
            self.logger.error(f"Failed to create service {service_name}: {e}")
            self._stats['errors_handled'] += 1
            raise
    
    def _get_service_class(self, service_name: str) -> Optional[Type]:
        """Get service class by name."""
        service_map = {
            'identity': IdentityService,
            'provenance': ProvenanceService,
            'quality': QualityService,
            'workflow': WorkflowStateService
        }
        return service_map.get(service_name)
    
    @property
    def identity_service(self) -> IdentityService:
        """Get identity service (async-safe property)."""
        # Use sync method for property access
        return asyncio.run(self.get_service('identity', IdentityService))
    
    @property
    def provenance_service(self) -> ProvenanceService:
        """Get provenance service (async-safe property)."""
        return asyncio.run(self.get_service('provenance', ProvenanceService))
    
    @property
    def quality_service(self) -> QualityService:
        """Get quality service (async-safe property)."""
        return asyncio.run(self.get_service('quality', QualityService))
    
    @property
    def workflow_service(self) -> WorkflowStateService:
        """Get workflow state service (async-safe property)."""
        return asyncio.run(self.get_service('workflow', WorkflowStateService))
    
    async def queue_operation(self, operation: Dict[str, Any]) -> Any:
        """
        Queue an operation for serialized execution.
        
        Used for operations that must be executed atomically.
        
        Args:
            operation: Operation dictionary with 'type' and 'params'
            
        Returns:
            Operation result
        """
        # Create future for result
        future = asyncio.Future()
        
        # Queue operation with future
        await self._operation_queue.put({
            'operation': operation,
            'future': future
        })
        
        # Wait for result
        return await future
    
    async def _process_operations(self):
        """Process queued operations serially."""
        while True:
            try:
                # Get next operation
                item = await self._operation_queue.get()
                operation = item['operation']
                future = item['future']
                
                try:
                    # Execute operation
                    result = await self._execute_operation(operation)
                    future.set_result(result)
                    self._stats['operations_processed'] += 1
                    
                except Exception as e:
                    future.set_exception(e)
                    self._stats['errors_handled'] += 1
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Operation processor error: {e}")
    
    async def _execute_operation(self, operation: Dict[str, Any]) -> Any:
        """Execute a queued operation."""
        op_type = operation.get('type')
        params = operation.get('params', {})
        
        if op_type == 'configure_service':
            service_name = params['service_name']
            config = params['config']
            self._service_configs[service_name] = config
            return True
            
        elif op_type == 'reset_service':
            service_name = params['service_name']
            if service_name in self._services:
                service = self._services[service_name]
                if hasattr(service, 'cleanup'):
                    await service.cleanup()
                del self._services[service_name]
            return True
            
        else:
            raise ValueError(f"Unknown operation type: {op_type}")
    
    async def health_check(self) -> Dict[str, bool]:
        """Check health of all services."""
        health_status = {}
        
        for service_name, service in self._services.items():
            try:
                if hasattr(service, 'health_check'):
                    health_status[service_name] = await service.health_check()
                else:
                    health_status[service_name] = True
            except Exception as e:
                self.logger.error(f"Health check failed for {service_name}: {e}")
                health_status[service_name] = False
        
        return health_status
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get service manager statistics."""
        stats = self._stats.copy()
        stats['active_services'] = list(self._services.keys())
        stats['service_count'] = len(self._services)
        return stats
    
    async def cleanup(self):
        """Clean up all services and resources."""
        self.logger.info("Starting service manager cleanup")
        
        # Cancel operation processor
        if self._operation_processor_task:
            self._operation_processor_task.cancel()
            try:
                await self._operation_processor_task
            except asyncio.CancelledError:
                pass
        
        # Clean up services
        for service_name, service in self._services.items():
            try:
                if hasattr(service, 'cleanup'):
                    await service.cleanup()
                self.logger.info(f"Cleaned up service: {service_name}")
            except Exception as e:
                self.logger.error(f"Cleanup failed for {service_name}: {e}")
        
        # Clear state
        self._services.clear()
        self._service_locks.clear()
        self._service_configs.clear()
        
        self.logger.info("Service manager cleanup complete")
    
    @asynccontextmanager
    async def atomic_operation(self, service_name: str):
        """
        Context manager for atomic operations on a service.
        
        Args:
            service_name: Name of the service
            
        Yields:
            Service instance for atomic operations
        """
        # Fix race condition - use instance lock to protect service lock creation
        if service_name not in self._service_locks:
            with self._instance_lock:
                # Double-check locking pattern
                if service_name not in self._service_locks:
                    self._service_locks[service_name] = threading.RLock()
        
        with self._service_locks[service_name]:
            service = await self.get_service(service_name)
            try:
                yield service
            finally:
                pass  # Cleanup if needed


# Global instance getter
_manager_instance: Optional[ThreadSafeServiceManager] = None
_manager_lock = threading.Lock()


def get_thread_safe_service_manager() -> ThreadSafeServiceManager:
    """Get the global thread-safe service manager instance."""
    global _manager_instance
    
    if _manager_instance is None:
        with _manager_lock:
            if _manager_instance is None:
                _manager_instance = ThreadSafeServiceManager()
    
    return _manager_instance
</file>

</files>

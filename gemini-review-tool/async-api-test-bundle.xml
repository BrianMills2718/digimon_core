This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, line numbers have been added.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: tests/unit/test_async_api_client.py, tests/unit/test_async_api_client_step3.py, tests/unit/test_async_api_client_step4.py, src/core/async_api_client.py
- Files matching these patterns are excluded: **/*.pyc, **/__pycache__/**, **/.git/**, **/*.log, **/.pytest_cache/**, **/*.cache, **/Evidence.md, **/CLAUDE.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Line numbers have been added to the beginning of each line
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
src/
  core/
    async_api_client.py
tests/
  unit/
    test_async_api_client_step3.py
    test_async_api_client_step4.py
    test_async_api_client.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="tests/unit/test_async_api_client_step3.py">
  1: #!/usr/bin/env python3
  2: """
  3: Unit tests for AsyncEnhancedAPIClient - Step 3: Caching and Performance Metrics Tests
  4: This file tests caching functionality, performance tracking, and optimization features.
  5: Part of comprehensive 80%+ coverage unit testing suite.
  6: """
  7: import pytest
  8: import asyncio
  9: import time
 10: from unittest.mock import Mock, patch, AsyncMock
 11: from typing import Dict, Any
 12: import sys
 13: from pathlib import Path
 14: sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
 15: from src.core.async_api_client import (
 16:     AsyncEnhancedAPIClient,
 17:     AsyncAPIRequest,
 18:     AsyncAPIResponse,
 19:     AsyncAPIRequestType
 20: )
 21: class TestCachingFunctionality:
 22:     """Test response caching functionality."""
 23:     @pytest.fixture
 24:     def mock_config_manager(self):
 25:         """Mock configuration manager."""
 26:         mock_config = Mock()
 27:         mock_config.get_api_config.return_value = {}
 28:         return mock_config
 29:     @pytest.fixture
 30:     def client(self, mock_config_manager):
 31:         """Create AsyncEnhancedAPIClient for testing."""
 32:         return AsyncEnhancedAPIClient(config_manager=mock_config_manager)
 33:     def test_get_cache_key_generation(self, client):
 34:         """Test cache key generation for requests."""
 35:         request = AsyncAPIRequest(
 36:             service_type="openai",
 37:             request_type=AsyncAPIRequestType.EMBEDDING,
 38:             prompt="test prompt",
 39:             max_tokens=100,
 40:             temperature=0.7,
 41:             model="text-embedding-3-small"
 42:         )
 43:         cache_key = client._get_cache_key(request)
 44:         assert isinstance(cache_key, int)
 45:         # Same request should generate same key
 46:         cache_key2 = client._get_cache_key(request)
 47:         assert cache_key == cache_key2
 48:     def test_get_cache_key_different_requests(self, client):
 49:         """Test cache key generation for different requests."""
 50:         request1 = AsyncAPIRequest(
 51:             service_type="openai",
 52:             request_type=AsyncAPIRequestType.EMBEDDING,
 53:             prompt="test prompt 1"
 54:         )
 55:         request2 = AsyncAPIRequest(
 56:             service_type="openai",
 57:             request_type=AsyncAPIRequestType.EMBEDDING,
 58:             prompt="test prompt 2"
 59:         )
 60:         cache_key1 = client._get_cache_key(request1)
 61:         cache_key2 = client._get_cache_key(request2)
 62:         assert cache_key1 != cache_key2
 63:     @pytest.mark.asyncio
 64:     async def test_cache_response(self, client):
 65:         """Test caching successful responses."""
 66:         cache_key = "test_cache_key"
 67:         response = AsyncAPIResponse(
 68:             success=True,
 69:             service_used="openai",
 70:             request_type=AsyncAPIRequestType.EMBEDDING,
 71:             response_data={"embedding": [0.1, 0.2, 0.3]},
 72:             response_time=1.0
 73:         )
 74:         await client._cache_response(cache_key, response)
 75:         assert cache_key in client.response_cache
 76:         cached_response, timestamp = client.response_cache[cache_key]
 77:         assert cached_response == response
 78:         assert isinstance(timestamp, float)
 79:     @pytest.mark.asyncio
 80:     async def test_check_cache_valid(self, client):
 81:         """Test checking valid cached responses."""
 82:         cache_key = "test_cache_key"
 83:         response = AsyncAPIResponse(
 84:             success=True,
 85:             service_used="openai",
 86:             request_type=AsyncAPIRequestType.EMBEDDING,
 87:             response_data={"embedding": [0.1, 0.2, 0.3]},
 88:             response_time=1.0
 89:         )
 90:         # Cache the response
 91:         await client._cache_response(cache_key, response)
 92:         # Check cache
 93:         cached_response = await client._check_cache(cache_key)
 94:         assert cached_response == response
 95:         assert client.performance_metrics["cache_hits"] == 1
 96:     @pytest.mark.asyncio
 97:     async def test_check_cache_expired(self, client):
 98:         """Test checking expired cached responses."""
 99:         cache_key = "test_cache_key"
100:         response = AsyncAPIResponse(
101:             success=True,
102:             service_used="openai",
103:             request_type=AsyncAPIRequestType.EMBEDDING,
104:             response_data={"embedding": [0.1, 0.2, 0.3]},
105:             response_time=1.0
106:         )
107:         # Cache the response with old timestamp
108:         old_timestamp = time.time() - client.cache_ttl - 10
109:         client.response_cache[cache_key] = (response, old_timestamp)
110:         # Check cache - should return None and remove expired entry
111:         cached_response = await client._check_cache(cache_key)
112:         assert cached_response is None
113:         assert cache_key not in client.response_cache
114:     @pytest.mark.asyncio
115:     async def test_check_cache_missing(self, client):
116:         """Test checking cache for non-existent key."""
117:         cached_response = await client._check_cache("nonexistent_key")
118:         assert cached_response is None
119:         assert client.performance_metrics["cache_hits"] == 0
120:     @pytest.mark.asyncio
121:     async def test_cache_cleanup_when_full(self, client):
122:         """Test cache cleanup when cache size exceeds limit."""
123:         # Fill cache with expired entries
124:         current_time = time.time()
125:         expired_time = current_time - client.cache_ttl - 10
126:         for i in range(1005):  # Exceed the 1000 limit
127:             cache_key = f"key_{i}"
128:             response = AsyncAPIResponse(
129:                 success=True,
130:                 service_used="openai",
131:                 request_type=AsyncAPIRequestType.EMBEDDING,
132:                 response_data={"embedding": [0.1, 0.2, 0.3]},
133:                 response_time=1.0
134:             )
135:             if i < 500:  # First 500 are expired
136:                 client.response_cache[cache_key] = (response, expired_time)
137:             else:  # Rest are fresh
138:                 client.response_cache[cache_key] = (response, current_time)
139:         # Cache a new response - should trigger cleanup
140:         new_response = AsyncAPIResponse(
141:             success=True,
142:             service_used="gemini",
143:             request_type=AsyncAPIRequestType.TEXT_GENERATION,
144:             response_data={"text": "test"},
145:             response_time=1.0
146:         )
147:         await client._cache_response("new_key", new_response)
148:         # Expired entries should be removed
149:         for i in range(500):
150:             assert f"key_{i}" not in client.response_cache
151: class TestPerformanceMetrics:
152:     """Test performance metrics tracking."""
153:     @pytest.fixture
154:     def mock_config_manager(self):
155:         """Mock configuration manager."""
156:         mock_config = Mock()
157:         mock_config.get_api_config.return_value = {}
158:         return mock_config
159:     @pytest.fixture
160:     def client(self, mock_config_manager):
161:         """Create AsyncEnhancedAPIClient for testing."""
162:         return AsyncEnhancedAPIClient(config_manager=mock_config_manager)
163:     def test_initial_performance_metrics(self, client):
164:         """Test initial state of performance metrics."""
165:         metrics = client.get_performance_metrics()
166:         assert metrics["total_requests"] == 0
167:         assert metrics["concurrent_requests"] == 0
168:         assert metrics["batch_requests"] == 0
169:         assert metrics["cache_hits"] == 0
170:         assert metrics["average_response_time"] == 0.0
171:         assert metrics["total_response_time"] == 0.0
172:         assert metrics["cache_hit_rate_percent"] == 0.0
173:         assert metrics["cache_size"] == 0
174:         assert metrics["processing_active"] is False
175:         assert metrics["session_initialized"] is False
176:     def test_performance_metrics_after_request(self, client):
177:         """Test performance metrics after simulated request."""
178:         # Simulate request processing
179:         client.performance_metrics["total_requests"] = 5
180:         client.performance_metrics["total_response_time"] = 10.0
181:         client.performance_metrics["cache_hits"] = 2
182:         # Need to trigger the calculation that happens in get_performance_metrics
183:         metrics = client.get_performance_metrics()
184:         assert metrics["total_requests"] == 5
185:         # The average gets calculated in get_performance_metrics, check the raw value
186:         assert client.performance_metrics["total_response_time"] == 10.0
187:         assert metrics["cache_hit_rate_percent"] == 40.0
188:     @pytest.mark.asyncio
189:     async def test_performance_metrics_with_session(self, client):
190:         """Test performance metrics with initialized session."""
191:         await client.initialize_clients()
192:         metrics = client.get_performance_metrics()
193:         assert metrics["session_initialized"] is True
194:         assert metrics["processing_active"] is True
195:         assert "connection_pool_stats" in metrics
196:         # Clean up
197:         await client.close()
198:     def test_connection_pool_stats_without_session(self, client):
199:         """Test connection pool stats when session not initialized."""
200:         metrics = client.get_performance_metrics()
201:         pool_stats = metrics["connection_pool_stats"]
202:         assert pool_stats["active_connections"] == 0
203:         assert pool_stats["idle_connections"] == 0
204:         assert pool_stats["pool_utilization"] == 0.0
205:         assert pool_stats["connection_reuse_rate"] == 0.0
206: class TestOptimizationFeatures:
207:     """Test optimization features and methods."""
208:     @pytest.fixture
209:     def mock_config_manager(self):
210:         """Mock configuration manager."""
211:         mock_config = Mock()
212:         mock_config.get_api_config.return_value = {}
213:         return mock_config
214:     @pytest.fixture
215:     def client(self, mock_config_manager):
216:         """Create AsyncEnhancedAPIClient for testing."""
217:         return AsyncEnhancedAPIClient(config_manager=mock_config_manager)
218:     @pytest.mark.asyncio
219:     async def test_optimize_connection_pool_without_session(self, client):
220:         """Test connection pool optimization when session not initialized."""
221:         result = await client.optimize_connection_pool()
222:         assert "optimizations_applied" in result
223:         assert "performance_improvements" in result
224:         assert "recommendations" in result
225:         assert "Initialize HTTP session for connection pooling" in result["recommendations"]
226:     @pytest.mark.asyncio
227:     async def test_optimize_connection_pool_with_session(self, client):
228:         """Test connection pool optimization with initialized session."""
229:         await client.initialize_clients()
230:         result = await client.optimize_connection_pool()
231:         assert "optimizations_applied" in result
232:         assert "current_stats" in result
233:         assert "recommendations" in result
234:         # Check that we get meaningful recommendations
235:         recommendations = result["recommendations"]
236:         assert isinstance(recommendations, list)
237:         # Clean up
238:         await client.close()
239:     @pytest.mark.asyncio
240:     async def test_optimize_connection_pool_high_utilization(self, client):
241:         """Test optimization recommendations for high utilization."""
242:         await client.initialize_clients()
243:         # Mock high utilization
244:         with patch.object(client, 'get_performance_metrics') as mock_metrics:
245:             mock_metrics.return_value = {
246:                 "connection_pool_stats": {
247:                     "pool_utilization": 85.0,
248:                     "connection_reuse_rate": 75.0
249:                 }
250:             }
251:             result = await client.optimize_connection_pool()
252:             recommendations = result["recommendations"]
253:             assert any("increasing connection pool size" in rec for rec in recommendations)
254:         # Clean up
255:         await client.close()
256:     @pytest.mark.asyncio
257:     async def test_optimize_connection_pool_low_utilization(self, client):
258:         """Test optimization recommendations for low utilization."""
259:         await client.initialize_clients()
260:         # Mock low utilization
261:         with patch.object(client, 'get_performance_metrics') as mock_metrics:
262:             mock_metrics.return_value = {
263:                 "connection_pool_stats": {
264:                     "pool_utilization": 15.0,
265:                     "connection_reuse_rate": 30.0
266:                 }
267:             }
268:             result = await client.optimize_connection_pool()
269:             recommendations = result["recommendations"]
270:             assert any("decreasing connection pool size" in rec for rec in recommendations)
271:             assert any("keepalive optimization" in rec for rec in recommendations)
272:         # Clean up
273:         await client.close()
274: class TestBenchmarkingFunctionality:
275:     """Test benchmarking and performance validation."""
276:     @pytest.fixture
277:     def mock_config_manager(self):
278:         """Mock configuration manager."""
279:         mock_config = Mock()
280:         mock_config.get_api_config.return_value = {}
281:         return mock_config
282:     @pytest.fixture
283:     def client(self, mock_config_manager):
284:         """Create AsyncEnhancedAPIClient for testing."""
285:         return AsyncEnhancedAPIClient(config_manager=mock_config_manager)
286:     @pytest.mark.asyncio
287:     async def test_benchmark_performance_without_clients(self, client):
288:         """Test benchmarking when clients are not available."""
289:         with patch.object(client, '_make_actual_request', new_callable=AsyncMock) as mock_request:
290:             # Mock successful responses
291:             mock_request.return_value = AsyncAPIResponse(
292:                 success=True,
293:                 service_used="openai",
294:                 request_type=AsyncAPIRequestType.COMPLETION,
295:                 response_data={"text": "test response"},
296:                 response_time=0.5
297:             )
298:             result = await client.benchmark_performance(num_requests=4)
299:             assert "sequential_time" in result
300:             assert "concurrent_time" in result
301:             assert "performance_improvement_percent" in result
302:             assert "sequential_successful" in result
303:             assert "concurrent_successful" in result
304:             assert "target_improvement" in result
305:             assert "achieved_target" in result
306:             assert "metrics" in result
307:             # Should have made requests
308:             assert mock_request.call_count >= 4
309:     @pytest.mark.asyncio
310:     async def test_benchmark_performance_structure(self, client):
311:         """Test benchmark performance result structure."""
312:         with patch.object(client, '_make_actual_request', new_callable=AsyncMock) as mock_request:
313:             with patch.object(client, 'process_concurrent_requests', new_callable=AsyncMock) as mock_concurrent:
314:                 # Mock responses
315:                 mock_response = AsyncAPIResponse(
316:                     success=True,
317:                     service_used="openai", 
318:                     request_type=AsyncAPIRequestType.COMPLETION,
319:                     response_data={"text": "test"},
320:                     response_time=0.1
321:                 )
322:                 mock_request.return_value = mock_response
323:                 mock_concurrent.return_value = [mock_response] * 5
324:                 result = await client.benchmark_performance(num_requests=10)
325:                 # Verify result structure
326:                 required_keys = [
327:                     "sequential_time",
328:                     "concurrent_time", 
329:                     "performance_improvement_percent",
330:                     "sequential_successful",
331:                     "concurrent_successful",
332:                     "target_improvement",
333:                     "achieved_target",
334:                     "metrics"
335:                 ]
336:                 for key in required_keys:
337:                     assert key in result
338:                 assert result["target_improvement"] == "50-60%"
339:                 assert isinstance(result["achieved_target"], bool)
340: if __name__ == "__main__":
341:     pytest.main([__file__, "-v"])
</file>

<file path="tests/unit/test_async_api_client_step4.py">
  1: #!/usr/bin/env python3
  2: """
  3: Unit tests for AsyncEnhancedAPIClient - Step 4: Request Processing and Error Handling Tests
  4: This file tests request processing, error handling, batch operations, and edge cases.
  5: Part of comprehensive 80%+ coverage unit testing suite.
  6: """
  7: import pytest
  8: import asyncio
  9: import time
 10: from unittest.mock import Mock, patch, AsyncMock
 11: from typing import Dict, Any
 12: import sys
 13: from pathlib import Path
 14: sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
 15: from src.core.async_api_client import (
 16:     AsyncEnhancedAPIClient,
 17:     AsyncOpenAIClient,
 18:     AsyncGeminiClient,
 19:     AsyncAPIRequest,
 20:     AsyncAPIResponse,
 21:     AsyncAPIRequestType
 22: )
 23: class TestRequestProcessing:
 24:     """Test request processing functionality."""
 25:     @pytest.fixture
 26:     def mock_config_manager(self):
 27:         """Mock configuration manager."""
 28:         mock_config = Mock()
 29:         mock_config.get_api_config.return_value = {}
 30:         return mock_config
 31:     @pytest.fixture
 32:     def client(self, mock_config_manager):
 33:         """Create AsyncEnhancedAPIClient for testing."""
 34:         return AsyncEnhancedAPIClient(config_manager=mock_config_manager)
 35:     @pytest.mark.asyncio
 36:     async def test_make_actual_request_openai_embedding(self, client):
 37:         """Test making actual OpenAI embedding request."""
 38:         # Mock OpenAI client
 39:         mock_openai_client = Mock()
 40:         mock_openai_client.create_single_embedding = AsyncMock(return_value=[0.1, 0.2, 0.3])
 41:         client.openai_client = mock_openai_client
 42:         request = AsyncAPIRequest(
 43:             service_type="openai",
 44:             request_type=AsyncAPIRequestType.EMBEDDING,
 45:             prompt="test prompt"
 46:         )
 47:         response = await client._make_actual_request(request)
 48:         assert response.success is True
 49:         assert response.service_used == "openai"
 50:         assert response.request_type == AsyncAPIRequestType.EMBEDDING
 51:         assert response.response_data == {"embedding": [0.1, 0.2, 0.3]}
 52:         assert response.response_time > 0
 53:         mock_openai_client.create_single_embedding.assert_called_once_with("test prompt")
 54:     @pytest.mark.asyncio
 55:     async def test_make_actual_request_openai_completion(self, client):
 56:         """Test making actual OpenAI completion request."""
 57:         # Mock OpenAI client
 58:         mock_openai_client = Mock()
 59:         mock_openai_client.create_completion = AsyncMock(return_value="Generated text")
 60:         client.openai_client = mock_openai_client
 61:         request = AsyncAPIRequest(
 62:             service_type="openai",
 63:             request_type=AsyncAPIRequestType.COMPLETION,
 64:             prompt="test prompt",
 65:             max_tokens=100,
 66:             temperature=0.7
 67:         )
 68:         response = await client._make_actual_request(request)
 69:         assert response.success is True
 70:         assert response.service_used == "openai"
 71:         assert response.request_type == AsyncAPIRequestType.COMPLETION
 72:         assert response.response_data == {"text": "Generated text"}
 73:         mock_openai_client.create_completion.assert_called_once_with(
 74:             "test prompt", max_tokens=100, temperature=0.7
 75:         )
 76:     @pytest.mark.asyncio
 77:     async def test_make_actual_request_gemini(self, client):
 78:         """Test making actual Gemini request."""
 79:         # Mock Gemini client
 80:         mock_gemini_client = Mock()
 81:         mock_gemini_client.generate_content = AsyncMock(return_value="Gemini response")
 82:         client.gemini_client = mock_gemini_client
 83:         request = AsyncAPIRequest(
 84:             service_type="gemini",
 85:             request_type=AsyncAPIRequestType.TEXT_GENERATION,
 86:             prompt="test prompt"
 87:         )
 88:         response = await client._make_actual_request(request)
 89:         assert response.success is True
 90:         assert response.service_used == "gemini"
 91:         assert response.request_type == AsyncAPIRequestType.TEXT_GENERATION
 92:         assert response.response_data == {"text": "Gemini response"}
 93:         mock_gemini_client.generate_content.assert_called_once_with("test prompt")
 94:     @pytest.mark.asyncio
 95:     async def test_make_actual_request_unsupported_service(self, client):
 96:         """Test making request with unsupported service."""
 97:         request = AsyncAPIRequest(
 98:             service_type="unsupported_service",
 99:             request_type=AsyncAPIRequestType.TEXT_GENERATION,
100:             prompt="test prompt"
101:         )
102:         response = await client._make_actual_request(request)
103:         assert response.success is False
104:         assert response.service_used == "unsupported_service"
105:         assert response.error == "Service unsupported_service not available"
106:     @pytest.mark.asyncio
107:     async def test_make_actual_request_unsupported_request_type(self, client):
108:         """Test making request with unsupported request type for OpenAI."""
109:         # Mock OpenAI client
110:         mock_openai_client = Mock()
111:         client.openai_client = mock_openai_client
112:         request = AsyncAPIRequest(
113:             service_type="openai",
114:             request_type=AsyncAPIRequestType.CLASSIFICATION,  # Unsupported
115:             prompt="test prompt"
116:         )
117:         response = await client._make_actual_request(request)
118:         assert response.success is False
119:         assert response.service_used == "openai"
120:         assert "Unsupported request type" in response.error
121: class TestErrorHandling:
122:     """Test error handling in various scenarios."""
123:     @pytest.fixture
124:     def mock_config_manager(self):
125:         """Mock configuration manager."""
126:         mock_config = Mock()
127:         mock_config.get_api_config.return_value = {}
128:         return mock_config
129:     @pytest.fixture
130:     def client(self, mock_config_manager):
131:         """Create AsyncEnhancedAPIClient for testing."""
132:         return AsyncEnhancedAPIClient(config_manager=mock_config_manager)
133:     @pytest.mark.asyncio
134:     async def test_request_with_client_exception(self, client):
135:         """Test request processing when client raises exception."""
136:         # Mock OpenAI client that raises exception
137:         mock_openai_client = Mock()
138:         mock_openai_client.create_single_embedding = AsyncMock(side_effect=Exception("API Error"))
139:         client.openai_client = mock_openai_client
140:         request = AsyncAPIRequest(
141:             service_type="openai",
142:             request_type=AsyncAPIRequestType.EMBEDDING,
143:             prompt="test prompt"
144:         )
145:         response = await client._make_actual_request(request)
146:         assert response.success is False
147:         assert response.service_used == "openai"
148:         assert response.error == "API Error"
149:         assert response.response_data is None
150:     @pytest.mark.asyncio
151:     async def test_process_concurrent_requests_with_exceptions(self, client):
152:         """Test concurrent request processing with some requests failing."""
153:         # Mock make_actual_request to return mix of success and failure
154:         async def mock_make_request(request):
155:             if "fail" in request.prompt:
156:                 raise Exception("Simulated failure")
157:             return AsyncAPIResponse(
158:                 success=True,
159:                 service_used=request.service_type,
160:                 request_type=request.request_type,
161:                 response_data={"text": "success"},
162:                 response_time=0.1
163:             )
164:         client._make_actual_request = AsyncMock(side_effect=mock_make_request)
165:         requests = [
166:             AsyncAPIRequest("openai", AsyncAPIRequestType.COMPLETION, "success 1"),
167:             AsyncAPIRequest("openai", AsyncAPIRequestType.COMPLETION, "fail prompt"),
168:             AsyncAPIRequest("openai", AsyncAPIRequestType.COMPLETION, "success 2"),
169:         ]
170:         responses = await client.process_concurrent_requests(requests)
171:         assert len(responses) == 3
172:         assert responses[0].success is True
173:         assert responses[1].success is False
174:         assert responses[2].success is True
175:         assert "Simulated failure" in responses[1].error
176:     @pytest.mark.asyncio
177:     async def test_create_embeddings_service_unavailable(self, client):
178:         """Test create_embeddings when service is unavailable."""
179:         # No OpenAI client set
180:         client.openai_client = None
181:         with pytest.raises(ValueError, match="Service openai not available"):
182:             await client.create_embeddings(["test text"], service="openai")
183:     @pytest.mark.asyncio
184:     async def test_generate_content_service_unavailable(self, client):
185:         """Test generate_content when service is unavailable."""
186:         # Mock failed request
187:         mock_response = AsyncAPIResponse(
188:             success=False,
189:             service_used="gemini",
190:             request_type=AsyncAPIRequestType.TEXT_GENERATION,
191:             response_data=None,
192:             response_time=0.0,
193:             error="Service unavailable"
194:         )
195:         client._process_request_with_cache = AsyncMock(return_value=mock_response)
196:         with pytest.raises(ValueError, match="Content generation failed"):
197:             await client.generate_content("test prompt", service="gemini")
198: class TestBatchProcessing:
199:     """Test batch processing functionality."""
200:     @pytest.fixture
201:     def mock_config_manager(self):
202:         """Mock configuration manager."""
203:         mock_config = Mock()
204:         mock_config.get_api_config.return_value = {}
205:         return mock_config
206:     @pytest.fixture
207:     def client(self, mock_config_manager):
208:         """Create AsyncEnhancedAPIClient for testing."""
209:         return AsyncEnhancedAPIClient(config_manager=mock_config_manager)
210:     @pytest.mark.asyncio
211:     async def test_create_embeddings_batch_single_text(self, client):
212:         """Test create_embeddings with single text (no batching)."""
213:         # Mock OpenAI client
214:         mock_openai_client = Mock()
215:         mock_openai_client.create_embeddings = AsyncMock(return_value=[[0.1, 0.2, 0.3]])
216:         client.openai_client = mock_openai_client
217:         result = await client.create_embeddings(["single text"], service="openai")
218:         assert result == [[0.1, 0.2, 0.3]]
219:         mock_openai_client.create_embeddings.assert_called_once_with(["single text"])
220:     @pytest.mark.asyncio
221:     async def test_create_embeddings_batch_multiple_texts(self, client):
222:         """Test create_embeddings with multiple texts (batching)."""
223:         # Mock OpenAI client
224:         mock_openai_client = Mock()
225:         mock_openai_client.create_embeddings = AsyncMock(return_value=[[0.1, 0.2], [0.3, 0.4]])
226:         client.openai_client = mock_openai_client
227:         texts = ["text 1", "text 2"]
228:         result = await client.create_embeddings(texts, service="openai")
229:         assert len(result) == 2
230:         assert result == [[0.1, 0.2], [0.3, 0.4]]
231:     @pytest.mark.asyncio
232:     async def test_process_batch_mixed_services(self, client):
233:         """Test process_batch with mixed OpenAI and Gemini requests."""
234:         # Mock clients
235:         mock_openai_client = Mock()
236:         mock_openai_client.create_single_embedding = AsyncMock(return_value=[0.1, 0.2])
237:         mock_gemini_client = Mock()
238:         mock_gemini_client.generate_content = AsyncMock(return_value="Generated")
239:         client.openai_client = mock_openai_client
240:         client.gemini_client = mock_gemini_client
241:         requests = [
242:             AsyncAPIRequest("openai", AsyncAPIRequestType.EMBEDDING, "text 1"),
243:             AsyncAPIRequest("gemini", AsyncAPIRequestType.TEXT_GENERATION, "prompt 1"),
244:             AsyncAPIRequest("openai", AsyncAPIRequestType.EMBEDDING, "text 2"),
245:         ]
246:         responses = await client.process_batch(requests)
247:         assert len(responses) == 3
248:         # Check OpenAI responses
249:         openai_responses = [r for r in responses if r.service_used == "openai"]
250:         assert len(openai_responses) == 2
251:         for response in openai_responses:
252:             assert response.success is True
253:             assert response.response_data == {"embedding": [0.1, 0.2]}
254:         # Check Gemini responses
255:         gemini_responses = [r for r in responses if r.service_used == "gemini"]
256:         assert len(gemini_responses) == 1
257:         assert gemini_responses[0].success is True
258:         assert gemini_responses[0].response_data == {"text": "Generated"}
259:     @pytest.mark.asyncio
260:     async def test_process_openai_batch_no_client(self, client):
261:         """Test _process_openai_batch when OpenAI client is None."""
262:         client.openai_client = None
263:         requests = [
264:             AsyncAPIRequest("openai", AsyncAPIRequestType.EMBEDDING, "text")
265:         ]
266:         responses = await client._process_openai_batch(requests)
267:         assert responses == []
268:     @pytest.mark.asyncio
269:     async def test_process_gemini_batch_no_client(self, client):
270:         """Test _process_gemini_batch when Gemini client is None."""
271:         client.gemini_client = None
272:         requests = [
273:             AsyncAPIRequest("gemini", AsyncAPIRequestType.TEXT_GENERATION, "prompt")
274:         ]
275:         responses = await client._process_gemini_batch(requests)
276:         assert responses == []
277: class TestEdgeCases:
278:     """Test edge cases and unusual scenarios."""
279:     @pytest.fixture
280:     def mock_config_manager(self):
281:         """Mock configuration manager."""
282:         mock_config = Mock()
283:         mock_config.get_api_config.return_value = {}
284:         return mock_config
285:     @pytest.fixture
286:     def client(self, mock_config_manager):
287:         """Create AsyncEnhancedAPIClient for testing."""
288:         return AsyncEnhancedAPIClient(config_manager=mock_config_manager)
289:     @pytest.mark.asyncio
290:     async def test_empty_request_list(self, client):
291:         """Test processing empty request list."""
292:         responses = await client.process_concurrent_requests([])
293:         assert responses == []
294:     @pytest.mark.asyncio
295:     async def test_process_batch_empty_requests(self, client):
296:         """Test process_batch with empty request list."""
297:         responses = await client.process_batch([])
298:         assert responses == []
299:     @pytest.mark.asyncio
300:     async def test_close_without_initialization(self, client):
301:         """Test closing client that was never initialized."""
302:         # Should not raise exception
303:         await client.close()
304:         assert client.processing_active is False
305:         assert len(client.response_cache) == 0
306:     @pytest.mark.asyncio
307:     async def test_multiple_close_calls(self, client):
308:         """Test calling close multiple times."""
309:         await client.initialize_clients()
310:         # First close
311:         await client.close()
312:         assert client.processing_active is False
313:         # Second close should not raise exception
314:         await client.close()
315:         assert client.processing_active is False
316:     def test_get_cache_key_with_none_values(self, client):
317:         """Test cache key generation with None values."""
318:         request = AsyncAPIRequest(
319:             service_type="openai",
320:             request_type=AsyncAPIRequestType.EMBEDDING,
321:             prompt="test",
322:             max_tokens=None,
323:             temperature=None,
324:             model=None
325:         )
326:         cache_key = client._get_cache_key(request)
327:         assert isinstance(cache_key, int)
328:     def test_get_cache_key_with_very_long_prompt(self, client):
329:         """Test cache key generation with very long prompt."""
330:         long_prompt = "a" * 1000  # Very long prompt
331:         request = AsyncAPIRequest(
332:             service_type="openai",
333:             request_type=AsyncAPIRequestType.EMBEDDING,
334:             prompt=long_prompt
335:         )
336:         cache_key = client._get_cache_key(request)
337:         assert isinstance(cache_key, int)
338:         # Cache key should be generated from first 100 chars
339:     @pytest.mark.asyncio
340:     async def test_benchmark_with_zero_requests(self, client):
341:         """Test benchmark_performance with zero requests."""
342:         result = await client.benchmark_performance(num_requests=0)
343:         # Should handle gracefully
344:         assert "sequential_time" in result
345:         assert "concurrent_time" in result
346:         assert isinstance(result["performance_improvement_percent"], (int, float))
347: if __name__ == "__main__":
348:     pytest.main([__file__, "-v"])
</file>

<file path="tests/unit/test_async_api_client.py">
  1: #!/usr/bin/env python3
  2: """
  3: Unit tests for AsyncEnhancedAPIClient - Step 1: Basic Setup and Initialization Tests
  4: This file tests the core initialization and basic functionality of the async API client.
  5: Part of comprehensive 80%+ coverage unit testing suite.
  6: """
  7: import pytest
  8: import asyncio
  9: import time
 10: import os
 11: from unittest.mock import Mock, patch, AsyncMock
 12: from typing import Dict, Any
 13: import sys
 14: from pathlib import Path
 15: sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
 16: from src.core.async_api_client import (
 17:     AsyncEnhancedAPIClient,
 18:     AsyncOpenAIClient, 
 19:     AsyncGeminiClient,
 20:     AsyncAPIRequest,
 21:     AsyncAPIResponse,
 22:     AsyncAPIRequestType
 23: )
 24: class TestAsyncEnhancedAPIClientBasics:
 25:     """Test basic initialization and configuration of AsyncEnhancedAPIClient."""
 26:     @pytest.fixture
 27:     def mock_config_manager(self):
 28:         """Mock configuration manager for testing."""
 29:         mock_config = Mock()
 30:         mock_config.get_api_config.return_value = {
 31:             "openai_model": "text-embedding-3-small",
 32:             "gemini_model": "gemini-2.0-flash-exp"
 33:         }
 34:         return mock_config
 35:     def test_init_basic(self, mock_config_manager):
 36:         """Test basic initialization of AsyncEnhancedAPIClient."""
 37:         client = AsyncEnhancedAPIClient(config_manager=mock_config_manager)
 38:         assert client.config_manager == mock_config_manager
 39:         assert client.openai_client is None
 40:         assert client.gemini_client is None
 41:         assert client.session_initialized is False
 42:         assert client.processing_active is False
 43:         assert len(client.response_cache) == 0
 44:         assert client.performance_metrics["total_requests"] == 0
 45:     def test_init_without_config_manager(self):
 46:         """Test initialization without config manager uses default."""
 47:         with patch('src.core.async_api_client.get_config') as mock_get_config:
 48:             mock_config = Mock()
 49:             mock_get_config.return_value = mock_config
 50:             mock_config.get_api_config.return_value = {}
 51:             client = AsyncEnhancedAPIClient()
 52:             assert client.config_manager == mock_config
 53:             mock_get_config.assert_called_once()
 54:     def test_rate_limits_configuration(self, mock_config_manager):
 55:         """Test rate limiting semaphores are configured correctly."""
 56:         client = AsyncEnhancedAPIClient(config_manager=mock_config_manager)
 57:         # Check rate limit semaphores exist and have correct values
 58:         assert "openai" in client.rate_limits
 59:         assert "gemini" in client.rate_limits
 60:         assert client.rate_limits["openai"]._value == 25
 61:         assert client.rate_limits["gemini"]._value == 15
 62:     def test_performance_metrics_initialization(self, mock_config_manager):
 63:         """Test performance metrics are properly initialized."""
 64:         client = AsyncEnhancedAPIClient(config_manager=mock_config_manager)
 65:         expected_metrics = {
 66:             "total_requests": 0,
 67:             "concurrent_requests": 0,
 68:             "batch_requests": 0,
 69:             "cache_hits": 0,
 70:             "average_response_time": 0.0,
 71:             "total_response_time": 0.0
 72:         }
 73:         for key, expected_value in expected_metrics.items():
 74:             assert client.performance_metrics[key] == expected_value
 75:         # Check connection pool stats exist
 76:         assert "connection_pool_stats" in client.performance_metrics
 77:         pool_stats = client.performance_metrics["connection_pool_stats"]
 78:         assert "active_connections" in pool_stats
 79:         assert "idle_connections" in pool_stats
 80:         assert "pool_utilization" in pool_stats
 81:         assert "connection_reuse_rate" in pool_stats
 82: class TestAsyncAPIRequestResponse:
 83:     """Test AsyncAPIRequest and AsyncAPIResponse data classes."""
 84:     def test_async_api_request_creation(self):
 85:         """Test AsyncAPIRequest creation with required fields."""
 86:         request = AsyncAPIRequest(
 87:             service_type="openai",
 88:             request_type=AsyncAPIRequestType.EMBEDDING,
 89:             prompt="test prompt"
 90:         )
 91:         assert request.service_type == "openai"
 92:         assert request.request_type == AsyncAPIRequestType.EMBEDDING
 93:         assert request.prompt == "test prompt"
 94:         assert request.max_tokens is None
 95:         assert request.temperature is None
 96:         assert request.model is None
 97:         assert request.additional_params is None
 98:     def test_async_api_request_with_optional_params(self):
 99:         """Test AsyncAPIRequest creation with optional parameters."""
100:         request = AsyncAPIRequest(
101:             service_type="gemini",
102:             request_type=AsyncAPIRequestType.TEXT_GENERATION,
103:             prompt="test prompt",
104:             max_tokens=100,
105:             temperature=0.7,
106:             model="gemini-pro",
107:             additional_params={"top_p": 0.9}
108:         )
109:         assert request.service_type == "gemini"
110:         assert request.request_type == AsyncAPIRequestType.TEXT_GENERATION
111:         assert request.prompt == "test prompt"
112:         assert request.max_tokens == 100
113:         assert request.temperature == 0.7
114:         assert request.model == "gemini-pro"
115:         assert request.additional_params == {"top_p": 0.9}
116:     def test_async_api_response_success(self):
117:         """Test AsyncAPIResponse creation for successful response."""
118:         response = AsyncAPIResponse(
119:             success=True,
120:             service_used="openai",
121:             request_type=AsyncAPIRequestType.EMBEDDING,
122:             response_data={"embedding": [0.1, 0.2, 0.3]},
123:             response_time=1.5,
124:             tokens_used=10
125:         )
126:         assert response.success is True
127:         assert response.service_used == "openai"
128:         assert response.request_type == AsyncAPIRequestType.EMBEDDING
129:         assert response.response_data == {"embedding": [0.1, 0.2, 0.3]}
130:         assert response.response_time == 1.5
131:         assert response.tokens_used == 10
132:         assert response.error is None
133:         assert response.fallback_used is False
134:     def test_async_api_response_failure(self):
135:         """Test AsyncAPIResponse creation for failed response."""
136:         response = AsyncAPIResponse(
137:             success=False,
138:             service_used="gemini",
139:             request_type=AsyncAPIRequestType.TEXT_GENERATION,
140:             response_data=None,
141:             response_time=2.0,
142:             error="API rate limit exceeded",
143:             fallback_used=True
144:         )
145:         assert response.success is False
146:         assert response.service_used == "gemini"
147:         assert response.request_type == AsyncAPIRequestType.TEXT_GENERATION
148:         assert response.response_data is None
149:         assert response.response_time == 2.0
150:         assert response.error == "API rate limit exceeded"
151:         assert response.fallback_used is True
152:         assert response.tokens_used is None
153: class TestAsyncAPIRequestType:
154:     """Test AsyncAPIRequestType enumeration."""
155:     def test_enum_values(self):
156:         """Test all enum values exist and have correct string values."""
157:         assert AsyncAPIRequestType.TEXT_GENERATION.value == "text_generation"
158:         assert AsyncAPIRequestType.EMBEDDING.value == "embedding"
159:         assert AsyncAPIRequestType.CLASSIFICATION.value == "classification"
160:         assert AsyncAPIRequestType.COMPLETION.value == "completion"
161:         assert AsyncAPIRequestType.CHAT.value == "chat"
162:     def test_enum_comparison(self):
163:         """Test enum comparison and equality."""
164:         assert AsyncAPIRequestType.EMBEDDING == AsyncAPIRequestType.EMBEDDING
165:         assert AsyncAPIRequestType.EMBEDDING != AsyncAPIRequestType.TEXT_GENERATION
166:         assert AsyncAPIRequestType.COMPLETION != AsyncAPIRequestType.CHAT
167: class TestAsyncOpenAIClient:
168:     """Test AsyncOpenAIClient initialization and basic functionality."""
169:     @pytest.fixture
170:     def mock_config_manager(self):
171:         """Mock configuration manager for OpenAI client testing."""
172:         mock_config = Mock()
173:         mock_config.get_api_config.return_value = {
174:             "openai_model": "text-embedding-3-small"
175:         }
176:         return mock_config
177:     def test_init_with_api_key(self, mock_config_manager):
178:         """Test OpenAI client initialization with API key."""
179:         with patch.dict(os.environ, {"OPENAI_API_KEY": "test_key"}):
180:             client = AsyncOpenAIClient(config_manager=mock_config_manager)
181:             assert client.api_key == "test_key"
182:             assert client.model == "text-embedding-3-small"
183:             assert client.config_manager == mock_config_manager
184:     def test_init_without_api_key(self, mock_config_manager):
185:         """Test OpenAI client initialization without API key raises error."""
186:         with patch.dict(os.environ, {}, clear=True):
187:             with pytest.raises(ValueError, match="OpenAI API key is required"):
188:                 AsyncOpenAIClient(config_manager=mock_config_manager)
189:     def test_init_with_provided_api_key(self, mock_config_manager):
190:         """Test OpenAI client initialization with provided API key."""
191:         client = AsyncOpenAIClient(api_key="provided_key", config_manager=mock_config_manager)
192:         assert client.api_key == "provided_key"
193:         assert client.model == "text-embedding-3-small"
194:     @patch('src.core.async_api_client.OPENAI_AVAILABLE', False)
195:     def test_init_without_openai_library(self, mock_config_manager):
196:         """Test OpenAI client initialization when OpenAI library not available."""
197:         with patch.dict(os.environ, {"OPENAI_API_KEY": "test_key"}):
198:             client = AsyncOpenAIClient(config_manager=mock_config_manager)
199:             assert client.client is None
200:             assert client.api_key == "test_key"
201: class TestAsyncGeminiClient:
202:     """Test AsyncGeminiClient initialization and basic functionality."""
203:     @pytest.fixture
204:     def mock_config_manager(self):
205:         """Mock configuration manager for Gemini client testing."""
206:         mock_config = Mock()
207:         mock_config.get_api_config.return_value = {
208:             "gemini_model": "gemini-2.0-flash-exp"
209:         }
210:         return mock_config
211:     def test_init_with_google_api_key(self, mock_config_manager):
212:         """Test Gemini client initialization with Google API key."""
213:         with patch.dict(os.environ, {"GOOGLE_API_KEY": "google_test_key"}):
214:             client = AsyncGeminiClient(config_manager=mock_config_manager)
215:             assert client.api_key == "google_test_key"
216:             assert client.model_name == "gemini-2.0-flash-exp"
217:             assert client.config_manager == mock_config_manager
218:     def test_init_with_gemini_api_key(self, mock_config_manager):
219:         """Test Gemini client initialization with Gemini API key."""
220:         with patch.dict(os.environ, {"GEMINI_API_KEY": "gemini_test_key"}):
221:             client = AsyncGeminiClient(config_manager=mock_config_manager)
222:             assert client.api_key == "gemini_test_key"
223:             assert client.model_name == "gemini-2.0-flash-exp"
224:     def test_init_without_api_key(self, mock_config_manager):
225:         """Test Gemini client initialization without API key raises error."""
226:         with patch.dict(os.environ, {}, clear=True):
227:             with pytest.raises(ValueError, match="Google/Gemini API key is required"):
228:                 AsyncGeminiClient(config_manager=mock_config_manager)
229:     def test_init_with_provided_api_key(self, mock_config_manager):
230:         """Test Gemini client initialization with provided API key."""
231:         client = AsyncGeminiClient(api_key="provided_gemini_key", config_manager=mock_config_manager)
232:         assert client.api_key == "provided_gemini_key"
233:         assert client.model_name == "gemini-2.0-flash-exp"
234:     @patch('src.core.async_api_client.GOOGLE_AVAILABLE', False)
235:     def test_init_without_google_library(self, mock_config_manager):
236:         """Test Gemini client initialization when Google library not available."""
237:         with patch.dict(os.environ, {"GOOGLE_API_KEY": "test_key"}):
238:             client = AsyncGeminiClient(config_manager=mock_config_manager)
239:             assert client.model is None
240:             assert client.api_key == "test_key"
241: class TestAsyncEnhancedAPIClientInitialization:
242:     """Test AsyncEnhancedAPIClient client initialization methods."""
243:     @pytest.fixture
244:     def mock_config_manager(self):
245:         """Mock configuration manager."""
246:         mock_config = Mock()
247:         mock_config.get_api_config.return_value = {}
248:         return mock_config
249:     @pytest.fixture
250:     def client(self, mock_config_manager):
251:         """Create AsyncEnhancedAPIClient for testing."""
252:         return AsyncEnhancedAPIClient(config_manager=mock_config_manager)
253:     @pytest.mark.asyncio
254:     async def test_initialize_clients_basic(self, client):
255:         """Test basic client initialization."""
256:         with patch.dict(os.environ, {}, clear=True):
257:             await client.initialize_clients()
258:             assert client.session_initialized is True
259:             assert client.http_session is not None
260:             assert client.openai_client is None
261:             assert client.gemini_client is None
262:     @pytest.mark.asyncio
263:     async def test_initialize_clients_with_openai(self, client):
264:         """Test client initialization with OpenAI API key."""
265:         with patch.dict(os.environ, {"OPENAI_API_KEY": "test_openai_key"}):
266:             with patch('src.core.async_api_client.AsyncOpenAIClient') as mock_openai:
267:                 mock_openai.return_value = Mock()
268:                 await client.initialize_clients()
269:                 assert client.session_initialized is True
270:                 assert client.openai_client is not None
271:                 mock_openai.assert_called_once_with(config_manager=client.config_manager)
272:     @pytest.mark.asyncio
273:     async def test_initialize_clients_with_gemini(self, client):
274:         """Test client initialization with Gemini API key."""
275:         with patch.dict(os.environ, {"GOOGLE_API_KEY": "test_google_key"}):
276:             with patch('src.core.async_api_client.AsyncGeminiClient') as mock_gemini:
277:                 mock_gemini.return_value = Mock()
278:                 await client.initialize_clients()
279:                 assert client.session_initialized is True
280:                 assert client.gemini_client is not None
281:                 mock_gemini.assert_called_once_with(config_manager=client.config_manager)
282:     @pytest.mark.asyncio
283:     async def test_initialize_clients_with_both_apis(self, client):
284:         """Test client initialization with both OpenAI and Gemini API keys."""
285:         with patch.dict(os.environ, {"OPENAI_API_KEY": "test_openai", "GOOGLE_API_KEY": "test_google"}):
286:             with patch('src.core.async_api_client.AsyncOpenAIClient') as mock_openai:
287:                 with patch('src.core.async_api_client.AsyncGeminiClient') as mock_gemini:
288:                     mock_openai.return_value = Mock()
289:                     mock_gemini.return_value = Mock()
290:                     await client.initialize_clients()
291:                     assert client.session_initialized is True
292:                     assert client.openai_client is not None
293:                     assert client.gemini_client is not None
294:                     assert client.processing_active is True
295:     @pytest.mark.asyncio
296:     async def test_close_clients(self, client):
297:         """Test proper cleanup of client resources."""
298:         # Initialize clients first
299:         with patch.dict(os.environ, {"OPENAI_API_KEY": "test_key"}):
300:             with patch('src.core.async_api_client.AsyncOpenAIClient') as mock_openai:
301:                 mock_openai_instance = Mock()
302:                 mock_openai_instance.close = AsyncMock()
303:                 mock_openai.return_value = mock_openai_instance
304:                 await client.initialize_clients()
305:                 # Test close
306:                 await client.close()
307:                 assert client.processing_active is False
308:                 assert len(client.response_cache) == 0
309:                 mock_openai_instance.close.assert_called_once()
310: if __name__ == "__main__":
311:     pytest.main([__file__, "-v"])
</file>

<file path="src/core/async_api_client.py">
  1: """Async API Client for Enhanced Performance
  2: This module provides async versions of API clients for improved performance
  3: with concurrent requests. Implements Phase 5.1 Task 4 async optimization to achieve
  4: 50-60% performance gains through full async processing, connection pooling,
  5: and optimized batch operations.
  6: """
  7: import asyncio
  8: import aiohttp
  9: import time
 10: from typing import Dict, Any, Optional, List, Union, Callable
 11: from dataclasses import dataclass
 12: from enum import Enum
 13: from datetime import datetime
 14: import json
 15: import os
 16: import ssl
 17: from concurrent.futures import ThreadPoolExecutor
 18: from .api_auth_manager import APIAuthManager, APIServiceType, APIAuthError
 19: from .logging_config import get_logger
 20: from src.core.config_manager import ConfigurationManager
 21: # Optional import for OpenAI async client
 22: try:
 23:     import openai
 24:     OPENAI_AVAILABLE = True
 25: except ImportError:
 26:     OPENAI_AVAILABLE = False
 27: # Optional import for Google Generative AI
 28: try:
 29:     import google.generativeai as genai
 30:     GOOGLE_AVAILABLE = True
 31: except ImportError:
 32:     GOOGLE_AVAILABLE = False
 33: from src.core.config_manager import get_config
 34: class AsyncAPIRequestType(Enum):
 35:     """Types of async API requests"""
 36:     TEXT_GENERATION = "text_generation"
 37:     EMBEDDING = "embedding"
 38:     CLASSIFICATION = "classification"
 39:     COMPLETION = "completion"
 40:     CHAT = "chat"
 41: @dataclass
 42: class AsyncAPIRequest:
 43:     """Async API request configuration"""
 44:     service_type: str
 45:     request_type: AsyncAPIRequestType
 46:     prompt: str
 47:     max_tokens: Optional[int] = None
 48:     temperature: Optional[float] = None
 49:     model: Optional[str] = None
 50:     additional_params: Optional[Dict[str, Any]] = None
 51: @dataclass
 52: class AsyncAPIResponse:
 53:     """Async API response wrapper"""
 54:     success: bool
 55:     service_used: str
 56:     request_type: AsyncAPIRequestType
 57:     response_data: Any
 58:     response_time: float
 59:     tokens_used: Optional[int] = None
 60:     error: Optional[str] = None
 61:     fallback_used: bool = False
 62: class AsyncOpenAIClient:
 63:     """Async OpenAI client for embeddings and completions"""
 64:     def __init__(self, api_key: str = None, config_manager: ConfigurationManager = None):
 65:         self.config_manager = config_manager or get_config()
 66:         self.logger = get_logger("core.async_openai_client")
 67:         # Get API key from config or environment
 68:         self.api_key = api_key or os.getenv("OPENAI_API_KEY")
 69:         if not self.api_key:
 70:             raise ValueError("OpenAI API key is required")
 71:         # Get API configuration
 72:         self.api_config = self.config_manager.get_api_config()
 73:         self.model = self.api_config.get("openai_model", "text-embedding-3-small")
 74:         # Initialize async client if available
 75:         if OPENAI_AVAILABLE:
 76:             self.client = openai.AsyncOpenAI(api_key=self.api_key)
 77:         else:
 78:             self.client = None
 79:             self.logger.warning("OpenAI async client not available")
 80:         self.logger.info("Async OpenAI client initialized")
 81:     async def create_embeddings(self, texts: List[str], model: str = None) -> List[List[float]]:
 82:         """Create embeddings for multiple texts asynchronously"""
 83:         if not self.client:
 84:             raise RuntimeError("OpenAI async client not available")
 85:         model = model or self.model
 86:         try:
 87:             # Create embeddings in parallel for better performance
 88:             start_time = time.time()
 89:             # Split into batches to avoid rate limits
 90:             batch_size = 100
 91:             all_embeddings = []
 92:             for i in range(0, len(texts), batch_size):
 93:                 batch = texts[i:i + batch_size]
 94:                 response = await self.client.embeddings.create(
 95:                     model=model,
 96:                     input=batch
 97:                 )
 98:                 # Extract embeddings from response
 99:                 batch_embeddings = [item.embedding for item in response.data]
100:                 all_embeddings.extend(batch_embeddings)
101:                 # Small delay between batches to respect rate limits
102:                 if i + batch_size < len(texts):
103:                     await asyncio.sleep(0.1)
104:             response_time = time.time() - start_time
105:             self.logger.info(f"Created {len(all_embeddings)} embeddings in {response_time:.2f}s")
106:             return all_embeddings
107:         except Exception as e:
108:             self.logger.error(f"Error creating embeddings: {e}")
109:             raise
110:     async def create_single_embedding(self, text: str, model: str = None) -> List[float]:
111:         """Create embedding for a single text"""
112:         embeddings = await self.create_embeddings([text], model)
113:         return embeddings[0]
114:     async def create_completion(self, prompt: str, model: str = "gpt-3.5-turbo", 
115:                                max_tokens: int = 150, temperature: float = 0.7) -> str:
116:         """Create a completion using OpenAI API"""
117:         if not self.client:
118:             raise RuntimeError("OpenAI async client not available")
119:         try:
120:             response = await self.client.chat.completions.create(
121:                 model=model,
122:                 messages=[{"role": "user", "content": prompt}],
123:                 max_tokens=max_tokens,
124:                 temperature=temperature
125:             )
126:             return response.choices[0].message.content
127:         except Exception as e:
128:             self.logger.error(f"Error creating completion: {e}")
129:             raise
130:     async def close(self):
131:         """Close the async client"""
132:         if self.client:
133:             await self.client.close()
134: class AsyncGeminiClient:
135:     """Async Gemini client for text generation"""
136:     def __init__(self, api_key: str = None, config_manager: ConfigurationManager = None):
137:         self.config_manager = config_manager or get_config()
138:         self.logger = get_logger("core.async_gemini_client")
139:         # Get API key from config or environment
140:         self.api_key = api_key or os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
141:         if not self.api_key:
142:             raise ValueError("Google/Gemini API key is required")
143:         # Get API configuration
144:         self.api_config = self.config_manager.get_api_config()
145:         self.model_name = self.api_config.get("gemini_model", "gemini-2.0-flash-exp")
146:         # Initialize Gemini client if available
147:         if GOOGLE_AVAILABLE:
148:             genai.configure(api_key=self.api_key)
149:             self.model = genai.GenerativeModel(self.model_name)
150:         else:
151:             self.model = None
152:             self.logger.warning("Google Generative AI not available")
153:         self.logger.info("Async Gemini client initialized")
154:     async def generate_content(self, prompt: str, max_tokens: int = None, 
155:                               temperature: float = None) -> str:
156:         """Generate content using Gemini API"""
157:         if not self.model:
158:             raise RuntimeError("Gemini model not available")
159:         try:
160:             # Note: The Google Generative AI library doesn't have native async support
161:             # We'll use asyncio.to_thread to run the synchronous call in a thread
162:             start_time = time.time()
163:             response = await asyncio.to_thread(
164:                 self.model.generate_content,
165:                 prompt
166:             )
167:             response_time = time.time() - start_time
168:             self.logger.info(f"Generated content in {response_time:.2f}s")
169:             return response.text
170:         except Exception as e:
171:             self.logger.error(f"Error generating content: {e}")
172:             raise
173:     async def generate_multiple_content(self, prompts: List[str]) -> List[str]:
174:         """Generate content for multiple prompts concurrently"""
175:         if not self.model:
176:             raise RuntimeError("Gemini model not available")
177:         try:
178:             # Use asyncio.gather to run multiple requests concurrently
179:             tasks = [self.generate_content(prompt) for prompt in prompts]
180:             results = await asyncio.gather(*tasks, return_exceptions=True)
181:             # Handle any exceptions that occurred
182:             processed_results = []
183:             for result in results:
184:                 if isinstance(result, Exception):
185:                     self.logger.error(f"Error in concurrent generation: {result}")
186:                     processed_results.append("")
187:                 else:
188:                     processed_results.append(result)
189:             return processed_results
190:         except Exception as e:
191:             self.logger.error(f"Error in concurrent generation: {e}")
192:             raise
193: class AsyncEnhancedAPIClient:
194:     """Enhanced async API client with multiple service support and 50-60% performance optimization"""
195:     def __init__(self, config_manager: ConfigurationManager = None):
196:         self.config_manager = config_manager or get_config()
197:         self.logger = get_logger("core.async_enhanced_api_client")
198:         # Initialize clients
199:         self.openai_client = None
200:         self.gemini_client = None
201:         # Enhanced rate limiting with higher concurrency
202:         self.rate_limits = {
203:             "openai": asyncio.Semaphore(25),   # Increased from 10 to 25
204:             "gemini": asyncio.Semaphore(15)    # Increased from 5 to 15
205:         }
206:         # Connection pooling for HTTP requests
207:         self.http_session = None
208:         self.session_initialized = False
209:         # Batch processing optimization
210:         self.batch_processor = None
211:         self.request_queue = asyncio.Queue()
212:         self.processing_active = False
213:         # Performance tracking
214:         self.performance_metrics = {
215:             "total_requests": 0,
216:             "concurrent_requests": 0,
217:             "batch_requests": 0,
218:             "cache_hits": 0,
219:             "average_response_time": 0.0,
220:             "connection_pool_stats": {
221:                 "active_connections": 0,
222:                 "idle_connections": 0,
223:                 "pool_utilization": 0.0,
224:                 "connection_reuse_rate": 0.0
225:             },
226:             "total_response_time": 0.0
227:         }
228:         # Response caching for identical requests
229:         self.response_cache = {}
230:         self.cache_ttl = 300  # 5 minutes
231:         self.logger.info("Async Enhanced API client initialized with performance optimizations")
232:     async def initialize_clients(self):
233:         """Initialize API clients asynchronously with optimized connection pooling"""
234:         try:
235:             # Initialize optimized HTTP session with connection pooling
236:             if not self.session_initialized:
237:                 connector = aiohttp.TCPConnector(
238:                     limit=100,        # Total connection pool size
239:                     limit_per_host=30,  # Connections per host
240:                     ttl_dns_cache=300,  # DNS cache TTL
241:                     use_dns_cache=True,
242:                     keepalive_timeout=30,
243:                     enable_cleanup_closed=True
244:                 )
245:                 timeout = aiohttp.ClientTimeout(total=60, connect=10)
246:                 self.http_session = aiohttp.ClientSession(
247:                     connector=connector,
248:                     timeout=timeout
249:                 )
250:                 self.session_initialized = True
251:                 self.logger.info("Optimized HTTP session initialized with connection pooling")
252:             # Initialize OpenAI client
253:             if os.getenv("OPENAI_API_KEY"):
254:                 self.openai_client = AsyncOpenAIClient(config_manager=self.config_manager)
255:                 self.logger.info("OpenAI async client initialized")
256:             # Initialize Gemini client
257:             if os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY"):
258:                 self.gemini_client = AsyncGeminiClient(config_manager=self.config_manager)
259:                 self.logger.info("Gemini async client initialized")
260:             # Start batch processor
261:             await self._start_batch_processor()
262:         except Exception as e:
263:             self.logger.error(f"Error initializing clients: {e}")
264:             raise
265:     async def _start_batch_processor(self):
266:         """Start the batch processing background task"""
267:         if not self.processing_active:
268:             self.processing_active = True
269:             self.batch_processor = asyncio.create_task(self._process_batch_queue())
270:             self.logger.info("Batch processor started")
271:     async def _process_batch_queue(self):
272:         """Background task to process batched requests"""
273:         while self.processing_active:
274:             try:
275:                 # Wait for requests to batch
276:                 await asyncio.sleep(0.1)  # Small delay to allow batching
277:                 if not self.request_queue.empty():
278:                     # Collect pending requests
279:                     batch_requests = []
280:                     while not self.request_queue.empty() and len(batch_requests) < 10:
281:                         try:
282:                             batch_requests.append(self.request_queue.get_nowait())
283:                         except asyncio.QueueEmpty:
284:                             break
285:                     if batch_requests:
286:                         # Process batch
287:                         await self._process_request_batch(batch_requests)
288:             except Exception as e:
289:                 self.logger.error(f"Error in batch processor: {e}")
290:                 await asyncio.sleep(1)  # Wait before retrying
291:     async def _process_request_batch(self, batch_requests: List):
292:         """Process a batch of requests concurrently"""
293:         self.performance_metrics["batch_requests"] += len(batch_requests)
294:         # Process requests concurrently
295:         tasks = []
296:         for request_data in batch_requests:
297:             task = asyncio.create_task(self._execute_single_request(request_data))
298:             tasks.append(task)
299:         await asyncio.gather(*tasks, return_exceptions=True)
300:     async def _execute_single_request(self, request_data):
301:         """Execute a single request from the batch"""
302:         try:
303:             request, future = request_data
304:             result = await self._process_request_with_cache(request)
305:             if not future.cancelled():
306:                 future.set_result(result)
307:         except Exception as e:
308:             if not future.cancelled():
309:                 future.set_exception(e)
310:     def _get_cache_key(self, request: AsyncAPIRequest) -> str:
311:         """Generate cache key for request"""
312:         key_data = {
313:             "service": request.service_type,
314:             "type": request.request_type.value,
315:             "prompt": request.prompt[:100],  # First 100 chars
316:             "model": request.model,
317:             "max_tokens": request.max_tokens,
318:             "temperature": request.temperature
319:         }
320:         return hash(str(sorted(key_data.items())))
321:     async def _check_cache(self, cache_key: str) -> Optional[AsyncAPIResponse]:
322:         """Check if response is cached and valid"""
323:         if cache_key in self.response_cache:
324:             cached_data, timestamp = self.response_cache[cache_key]
325:             if time.time() - timestamp < self.cache_ttl:
326:                 self.performance_metrics["cache_hits"] += 1
327:                 return cached_data
328:             else:
329:                 # Remove expired cache entry
330:                 del self.response_cache[cache_key]
331:         return None
332:     async def _cache_response(self, cache_key: str, response: AsyncAPIResponse):
333:         """Cache the response"""
334:         self.response_cache[cache_key] = (response, time.time())
335:         # Clean up old cache entries if cache gets too large
336:         if len(self.response_cache) > 1000:
337:             current_time = time.time()
338:             expired_keys = [
339:                 key for key, (_, timestamp) in self.response_cache.items()
340:                 if current_time - timestamp > self.cache_ttl
341:             ]
342:             for key in expired_keys:
343:                 del self.response_cache[key]
344:     async def _process_request_with_cache(self, request: AsyncAPIRequest) -> AsyncAPIResponse:
345:         """Process request with caching optimization"""
346:         # Check cache first
347:         cache_key = self._get_cache_key(request)
348:         cached_response = await self._check_cache(cache_key)
349:         if cached_response:
350:             return cached_response
351:         # Process request
352:         response = await self._make_actual_request(request)
353:         # Cache successful responses
354:         if response.success:
355:             await self._cache_response(cache_key, response)
356:         return response
357:     async def _make_actual_request(self, request: AsyncAPIRequest) -> AsyncAPIResponse:
358:         """Make the actual API request with performance tracking"""
359:         start_time = time.time()
360:         self.performance_metrics["total_requests"] += 1
361:         self.performance_metrics["concurrent_requests"] += 1
362:         try:
363:             if request.service_type == "openai" and self.openai_client:
364:                 async with self.rate_limits["openai"]:
365:                     if request.request_type == AsyncAPIRequestType.EMBEDDING:
366:                         result = await self.openai_client.create_single_embedding(request.prompt)
367:                         response_data = {"embedding": result}
368:                     elif request.request_type == AsyncAPIRequestType.COMPLETION:
369:                         result = await self.openai_client.create_completion(
370:                             request.prompt,
371:                             max_tokens=request.max_tokens,
372:                             temperature=request.temperature
373:                         )
374:                         response_data = {"text": result}
375:                     else:
376:                         raise ValueError(f"Unsupported request type: {request.request_type}")
377:                     response_time = time.time() - start_time
378:                     return AsyncAPIResponse(
379:                         success=True,
380:                         service_used="openai",
381:                         request_type=request.request_type,
382:                         response_data=response_data,
383:                         response_time=response_time
384:                     )
385:             elif request.service_type == "gemini" and self.gemini_client:
386:                 async with self.rate_limits["gemini"]:
387:                     result = await self.gemini_client.generate_content(request.prompt)
388:                     response_data = {"text": result}
389:                     response_time = time.time() - start_time
390:                     return AsyncAPIResponse(
391:                         success=True,
392:                         service_used="gemini",
393:                         request_type=request.request_type,
394:                         response_data=response_data,
395:                         response_time=response_time
396:                     )
397:             else:
398:                 raise ValueError(f"Service {request.service_type} not available")
399:         except Exception as e:
400:             response_time = time.time() - start_time
401:             return AsyncAPIResponse(
402:                 success=False,
403:                 service_used=request.service_type,
404:                 request_type=request.request_type,
405:                 response_data=None,
406:                 response_time=response_time,
407:                 error=str(e)
408:             )
409:         finally:
410:             self.performance_metrics["concurrent_requests"] -= 1
411:             response_time = time.time() - start_time
412:             self.performance_metrics["total_response_time"] += response_time
413:             if self.performance_metrics["total_requests"] > 0:
414:                 self.performance_metrics["average_response_time"] = (
415:                     self.performance_metrics["total_response_time"] / 
416:                     self.performance_metrics["total_requests"]
417:                 )
418:     async def create_embeddings(self, texts: List[str], service: str = "openai") -> List[List[float]]:
419:         """Create embeddings using specified service with optimization"""
420:         if service == "openai" and self.openai_client:
421:             # Use optimized batch processing for multiple texts
422:             if len(texts) > 1:
423:                 return await self._create_embeddings_batch(texts, service)
424:             else:
425:                 async with self.rate_limits["openai"]:
426:                     return await self.openai_client.create_embeddings(texts)
427:         else:
428:             raise ValueError(f"Service {service} not available for embeddings")
429:     async def _create_embeddings_batch(self, texts: List[str], service: str) -> List[List[float]]:
430:         """Create embeddings for multiple texts using optimized batch processing"""
431:         start_time = time.time()
432:         # Split into optimal batch sizes for the service
433:         batch_size = 50 if service == "openai" else 20
434:         batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]
435:         # Process batches concurrently
436:         tasks = []
437:         for batch in batches:
438:             task = asyncio.create_task(self.openai_client.create_embeddings(batch))
439:             tasks.append(task)
440:         batch_results = await asyncio.gather(*tasks)
441:         # Flatten results
442:         all_embeddings = []
443:         for batch_result in batch_results:
444:             all_embeddings.extend(batch_result)
445:         duration = time.time() - start_time
446:         self.logger.info(f"Created {len(all_embeddings)} embeddings in {duration:.2f}s using optimized batching")
447:         return all_embeddings
448:     async def generate_content(self, prompt: str, service: str = "gemini") -> str:
449:         """Generate content using specified service with optimization"""
450:         request = AsyncAPIRequest(
451:             service_type=service,
452:             request_type=AsyncAPIRequestType.TEXT_GENERATION,
453:             prompt=prompt
454:         )
455:         response = await self._process_request_with_cache(request)
456:         if response.success:
457:             if service == "gemini":
458:                 return response.response_data.get("text", "")
459:             elif service == "openai":
460:                 return response.response_data.get("text", "")
461:         else:
462:             raise ValueError(f"Content generation failed: {response.error}")
463:     async def process_concurrent_requests(self, requests: List[AsyncAPIRequest]) -> List[AsyncAPIResponse]:
464:         """Process multiple requests concurrently with optimized performance"""
465:         start_time = time.time()
466:         # Process requests using optimized batching and caching
467:         tasks = []
468:         for request in requests:
469:             task = asyncio.create_task(self._process_request_with_cache(request))
470:             tasks.append(task)
471:         responses = await asyncio.gather(*tasks, return_exceptions=True)
472:         # Convert exceptions to error responses
473:         processed_responses = []
474:         for i, response in enumerate(responses):
475:             if isinstance(response, Exception):
476:                 error_response = AsyncAPIResponse(
477:                     success=False,
478:                     service_used=requests[i].service_type,
479:                     request_type=requests[i].request_type,
480:                     response_data=None,
481:                     response_time=0.0,
482:                     error=str(response)
483:                 )
484:                 processed_responses.append(error_response)
485:             else:
486:                 processed_responses.append(response)
487:         duration = time.time() - start_time
488:         successful_requests = sum(1 for r in processed_responses if r.success)
489:         self.logger.info(f"Processed {len(requests)} concurrent requests in {duration:.2f}s "
490:                         f"({successful_requests}/{len(requests)} successful)")
491:         return processed_responses
492:     def get_performance_metrics(self) -> Dict[str, Any]:
493:         """Get detailed performance metrics including connection pool stats"""
494:         cache_hit_rate = (
495:             self.performance_metrics["cache_hits"] / max(self.performance_metrics["total_requests"], 1)
496:         ) * 100
497:         # Update connection pool stats if session is active
498:         if self.session_initialized and self.http_session:
499:             connector = self.http_session.connector
500:             if hasattr(connector, '_connections'):
501:                 # Get actual connection pool statistics
502:                 total_connections = len(connector._connections)
503:                 active_connections = sum(1 for conns in connector._connections.values() for conn in conns if not conn.is_closing())
504:                 idle_connections = total_connections - active_connections
505:                 self.performance_metrics["connection_pool_stats"].update({
506:                     "active_connections": active_connections,
507:                     "idle_connections": idle_connections,
508:                     "total_connections": total_connections,
509:                     "pool_utilization": (active_connections / max(100, 1)) * 100,  # Based on limit=100
510:                     "connection_reuse_rate": (total_connections / max(self.performance_metrics["total_requests"], 1)) * 100
511:                 })
512:         return {
513:             **self.performance_metrics,
514:             "cache_hit_rate_percent": cache_hit_rate,
515:             "cache_size": len(self.response_cache),
516:             "processing_active": self.processing_active,
517:             "session_initialized": self.session_initialized
518:         }
519:     async def optimize_connection_pool(self) -> Dict[str, Any]:
520:         """Optimize connection pool based on usage patterns"""
521:         optimization_results = {
522:             'optimizations_applied': [],
523:             'performance_improvements': {},
524:             'recommendations': []
525:         }
526:         if not self.session_initialized:
527:             optimization_results['recommendations'].append('Initialize HTTP session for connection pooling')
528:             return optimization_results
529:         metrics = self.get_performance_metrics()
530:         pool_stats = metrics['connection_pool_stats']
531:         # Analyze pool utilization
532:         utilization = pool_stats.get('pool_utilization', 0)
533:         if utilization > 80:
534:             optimization_results['recommendations'].append('Consider increasing connection pool size (high utilization)')
535:         elif utilization < 20:
536:             optimization_results['recommendations'].append('Consider decreasing connection pool size (low utilization)')
537:         # Analyze connection reuse
538:         reuse_rate = pool_stats.get('connection_reuse_rate', 0)
539:         if reuse_rate < 50:
540:             optimization_results['recommendations'].append('Low connection reuse - consider keepalive optimization')
541:         optimization_results['current_stats'] = pool_stats
542:         return optimization_results
543:     async def benchmark_performance(self, num_requests: int = 20) -> Dict[str, Any]:
544:         """Benchmark async client performance for validation"""
545:         self.logger.info(f"Starting performance benchmark with {num_requests} requests")
546:         # Reset metrics
547:         self.performance_metrics = {
548:             "total_requests": 0,
549:             "concurrent_requests": 0,
550:             "batch_requests": 0,
551:             "cache_hits": 0,
552:             "average_response_time": 0.0,
553:             "total_response_time": 0.0
554:         }
555:         # Create test requests
556:         test_requests = []
557:         for i in range(num_requests):
558:             if i % 2 == 0:  # Mix of OpenAI and Gemini requests
559:                 request = AsyncAPIRequest(
560:                     service_type="openai",
561:                     request_type=AsyncAPIRequestType.COMPLETION,
562:                     prompt=f"Test prompt {i}",
563:                     max_tokens=10
564:                 )
565:             else:
566:                 request = AsyncAPIRequest(
567:                     service_type="gemini",
568:                     request_type=AsyncAPIRequestType.TEXT_GENERATION,
569:                     prompt=f"Test prompt {i}",
570:                     max_tokens=10
571:                 )
572:             test_requests.append(request)
573:         # Benchmark sequential processing
574:         sequential_start = time.time()
575:         sequential_responses = []
576:         for request in test_requests[:5]:  # Limit to 5 for sequential test
577:             response = await self._make_actual_request(request)
578:             sequential_responses.append(response)
579:         sequential_time = time.time() - sequential_start
580:         # Reset metrics for concurrent test
581:         self.performance_metrics["total_requests"] = 0
582:         self.performance_metrics["total_response_time"] = 0.0
583:         # Benchmark concurrent processing
584:         concurrent_start = time.time()
585:         concurrent_responses = await self.process_concurrent_requests(test_requests[:5])
586:         concurrent_time = time.time() - concurrent_start
587:         # Calculate performance improvement
588:         performance_improvement = ((sequential_time - concurrent_time) / sequential_time) * 100
589:         sequential_successful = sum(1 for r in sequential_responses if r.success)
590:         concurrent_successful = sum(1 for r in concurrent_responses if r.success)
591:         return {
592:             "sequential_time": sequential_time,
593:             "concurrent_time": concurrent_time,
594:             "performance_improvement_percent": performance_improvement,
595:             "sequential_successful": sequential_successful,
596:             "concurrent_successful": concurrent_successful,
597:             "target_improvement": "50-60%",
598:             "achieved_target": performance_improvement >= 50.0,
599:             "metrics": self.get_performance_metrics()
600:         }
601:     async def process_batch(self, requests: List[AsyncAPIRequest]) -> List[AsyncAPIResponse]:
602:         """Process multiple API requests concurrently"""
603:         start_time = time.time()
604:         # Group requests by service type
605:         openai_requests = [r for r in requests if r.service_type == "openai"]
606:         gemini_requests = [r for r in requests if r.service_type == "gemini"]
607:         # Create tasks for each service
608:         tasks = []
609:         # Process OpenAI requests
610:         if openai_requests:
611:             tasks.append(self._process_openai_batch(openai_requests))
612:         # Process Gemini requests
613:         if gemini_requests:
614:             tasks.append(self._process_gemini_batch(gemini_requests))
615:         # Wait for all tasks to complete
616:         results = await asyncio.gather(*tasks, return_exceptions=True)
617:         # Flatten results
618:         all_responses = []
619:         for result in results:
620:             if isinstance(result, Exception):
621:                 self.logger.error(f"Batch processing error: {result}")
622:             else:
623:                 all_responses.extend(result)
624:         total_time = time.time() - start_time
625:         self.logger.info(f"Processed {len(requests)} requests in {total_time:.2f}s")
626:         return all_responses
627:     async def _process_openai_batch(self, requests: List[AsyncAPIRequest]) -> List[AsyncAPIResponse]:
628:         """Process OpenAI requests in batch"""
629:         if not self.openai_client:
630:             return []
631:         responses = []
632:         for request in requests:
633:             try:
634:                 start_time = time.time()
635:                 if request.request_type == AsyncAPIRequestType.EMBEDDING:
636:                     result = await self.openai_client.create_single_embedding(request.prompt)
637:                     response_data = {"embedding": result}
638:                 elif request.request_type == AsyncAPIRequestType.COMPLETION:
639:                     result = await self.openai_client.create_completion(
640:                         request.prompt,
641:                         max_tokens=request.max_tokens,
642:                         temperature=request.temperature
643:                     )
644:                     response_data = {"text": result}
645:                 else:
646:                     raise ValueError(f"Unsupported request type: {request.request_type}")
647:                 response_time = time.time() - start_time
648:                 responses.append(AsyncAPIResponse(
649:                     success=True,
650:                     service_used="openai",
651:                     request_type=request.request_type,
652:                     response_data=response_data,
653:                     response_time=response_time
654:                 ))
655:             except Exception as e:
656:                 responses.append(AsyncAPIResponse(
657:                     success=False,
658:                     service_used="openai",
659:                     request_type=request.request_type,
660:                     response_data=None,
661:                     response_time=0.0,
662:                     error=str(e)
663:                 ))
664:         return responses
665:     async def _process_gemini_batch(self, requests: List[AsyncAPIRequest]) -> List[AsyncAPIResponse]:
666:         """Process Gemini requests in batch"""
667:         if not self.gemini_client:
668:             return []
669:         responses = []
670:         for request in requests:
671:             try:
672:                 start_time = time.time()
673:                 if request.request_type == AsyncAPIRequestType.TEXT_GENERATION:
674:                     result = await self.gemini_client.generate_content(request.prompt)
675:                     response_data = {"text": result}
676:                 else:
677:                     raise ValueError(f"Unsupported request type: {request.request_type}")
678:                 response_time = time.time() - start_time
679:                 responses.append(AsyncAPIResponse(
680:                     success=True,
681:                     service_used="gemini",
682:                     request_type=request.request_type,
683:                     response_data=response_data,
684:                     response_time=response_time
685:                 ))
686:             except Exception as e:
687:                 responses.append(AsyncAPIResponse(
688:                     success=False,
689:                     service_used="gemini",
690:                     request_type=request.request_type,
691:                     response_data=None,
692:                     response_time=0.0,
693:                     error=str(e)
694:                 ))
695:         return responses
696:     async def close(self):
697:         """Close all async clients and cleanup resources"""
698:         self.logger.info("Shutting down async API clients...")
699:         # Stop batch processor
700:         if self.processing_active:
701:             self.processing_active = False
702:             if self.batch_processor and not self.batch_processor.done():
703:                 self.batch_processor.cancel()
704:                 try:
705:                     await self.batch_processor
706:                 except asyncio.CancelledError:
707:                     pass
708:         # Close HTTP session
709:         if self.http_session and not self.http_session.closed:
710:             await self.http_session.close()
711:         # Close individual clients
712:         if self.openai_client:
713:             await self.openai_client.close()
714:         # Clear cache
715:         self.response_cache.clear()
716:         self.logger.info("Async API clients closed and resources cleaned up")
717: # Global async client instance
718: _async_client = None
719: async def get_async_api_client() -> AsyncEnhancedAPIClient:
720:     """Get the global async API client instance"""
721:     global _async_client
722:     if _async_client is None:
723:         _async_client = AsyncEnhancedAPIClient()
724:         await _async_client.initialize_clients()
725:     return _async_client
726: async def close_async_api_client():
727:     """Close the global async API client"""
728:     global _async_client
729:     if _async_client is not None:
730:         await _async_client.close()
731:         _async_client = None
</file>

</files>

This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: src/analytics/real_*.py, src/analytics/advanced_scoring.py, src/analytics/theory_knowledge_base.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
src/
  analytics/
    advanced_scoring.py
    real_embedding_service.py
    real_llm_service.py
    real_percentile_ranker.py
    theory_knowledge_base.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/analytics/advanced_scoring.py">
"""Advanced scoring using NLP models for hypothesis evaluation"""

import logging
from typing import Dict, List, Any, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import asyncio

logger = logging.getLogger(__name__)

# Try to import transformers for zero-shot classification
try:
    from transformers import pipeline
    HAS_TRANSFORMERS = True
except ImportError:
    logger.warning("transformers library not available, some advanced scoring features will be limited")
    HAS_TRANSFORMERS = False


class AdvancedScoring:
    """Advanced scoring using NLP models for hypothesis and synthesis evaluation"""
    
    def __init__(self):
        """Initialize NLP models for scoring"""
        # Initialize sentence similarity model
        self.similarity_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Initialize classification pipelines if available
        if HAS_TRANSFORMERS:
            try:
                self.classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
                self.qa_model = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")
                logger.info("Initialized advanced NLP pipelines for scoring")
            except Exception as e:
                logger.warning(f"Failed to initialize NLP pipelines: {e}")
                self.classifier = None
                self.qa_model = None
        else:
            self.classifier = None
            self.qa_model = None
    
    async def calculate_explanatory_power(self, hypothesis: Dict, anomaly: Dict) -> float:
        """Calculate explanatory power using semantic similarity and classification.
        
        Args:
            hypothesis: Hypothesis dictionary with 'text' field
            anomaly: Anomaly dictionary with 'description' and 'type' fields
            
        Returns:
            Score between 0 and 1 indicating explanatory power
        """
        try:
            # Get embeddings for hypothesis and anomaly
            hypothesis_text = hypothesis.get('text', '')
            anomaly_text = anomaly.get('description', '')
            
            if not hypothesis_text or not anomaly_text:
                return 0.5  # Default score if missing text
            
            # Calculate semantic similarity
            embeddings = self.similarity_model.encode([hypothesis_text, anomaly_text])
            similarity = cosine_similarity(embeddings[0:1], embeddings[1:2])[0][0]
            
            # Use zero-shot classification if available
            if self.classifier:
                try:
                    # Check if hypothesis explains anomaly
                    result = await self._run_classifier(
                        hypothesis_text,
                        candidate_labels=['explains anomaly', 'unrelated to anomaly', 'partially explains anomaly'],
                        hypothesis_template="This hypothesis {} the observed pattern."
                    )
                    
                    # Map classification scores
                    explanation_scores = {
                        'explains anomaly': 1.0,
                        'partially explains anomaly': 0.7,
                        'unrelated to anomaly': 0.3
                    }
                    
                    top_label = result['labels'][0]
                    classification_score = explanation_scores.get(top_label, 0.5) * result['scores'][0]
                    
                    # Combine semantic similarity and classification
                    return (similarity + classification_score) / 2
                    
                except Exception as e:
                    logger.warning(f"Classification failed: {e}")
                    return similarity
            else:
                # Enhanced similarity score based on key concepts
                key_concepts = anomaly.get('type', '').replace('_', ' ').split()
                concept_matches = sum(1 for concept in key_concepts if concept.lower() in hypothesis_text.lower())
                concept_bonus = min(concept_matches / max(len(key_concepts), 1), 0.3)
                
                return min(similarity + concept_bonus, 1.0)
                
        except Exception as e:
            logger.error(f"Failed to calculate explanatory power: {e}")
            return 0.5
    
    async def calculate_testability(self, hypothesis: Dict, evidence_base: Dict) -> float:
        """Calculate testability using NLP analysis.
        
        Args:
            hypothesis: Hypothesis dictionary
            evidence_base: Available evidence
            
        Returns:
            Score between 0 and 1 indicating testability
        """
        try:
            hypothesis_text = hypothesis.get('text', '')
            
            if not hypothesis_text:
                return 0.5
            
            # Base testability score from language patterns
            testability_score = await self._analyze_testability_language(hypothesis_text)
            
            # Check if we have evidence to test it
            evidence_texts = []
            for entity in evidence_base.get('entities', []):
                if 'text' in entity:
                    evidence_texts.append(entity['text'])
                elif 'description' in entity:
                    evidence_texts.append(entity['description'])
            
            if evidence_texts and self.qa_model:
                # Try to answer questions about the hypothesis using evidence
                sample_question = f"What evidence supports this claim: {hypothesis_text[:200]}?"
                
                try:
                    # Combine evidence texts
                    context = ' '.join(evidence_texts[:10])  # Limit context size
                    
                    answer = await self._run_qa(sample_question, context)
                    
                    # Higher confidence in answer = more testable with current evidence
                    evidence_score = answer.get('score', 0) if answer else 0
                    testability_score = (testability_score + evidence_score) / 2
                    
                except Exception as e:
                    logger.warning(f"QA model failed: {e}")
            
            return min(testability_score, 1.0)
            
        except Exception as e:
            logger.error(f"Failed to calculate testability: {e}")
            return 0.5
    
    async def calculate_simplicity(self, hypothesis: Dict) -> float:
        """Calculate hypothesis simplicity using advanced linguistic analysis.
        
        Args:
            hypothesis: Hypothesis dictionary
            
        Returns:
            Score between 0 and 1 (higher = simpler)
        """
        try:
            text = hypothesis.get('text', '')
            
            if not text:
                return 0.5
            
            # Analyze linguistic complexity
            words = text.split()
            sentence_count = text.count('.') + text.count('!') + text.count('?')
            sentence_count = max(sentence_count, 1)
            
            # Average words per sentence (lower is simpler)
            avg_words_per_sentence = len(words) / sentence_count
            sentence_complexity = 1 - min(avg_words_per_sentence / 30, 1.0)
            
            # Complex word ratio (approximate using word length)
            complex_words = sum(1 for word in words if len(word) > 8)
            complex_ratio = complex_words / max(len(words), 1)
            word_simplicity = 1 - min(complex_ratio, 1.0)
            
            # Check for jargon and technical terms
            technical_indicators = [
                'mechanism', 'framework', 'paradigm', 'methodology', 'architecture',
                'infrastructure', 'implementation', 'optimization', 'algorithm'
            ]
            
            technical_count = sum(1 for term in technical_indicators if term in text.lower())
            technical_simplicity = 1 - min(technical_count / 5, 1.0)
            
            # Combine scores
            simplicity_score = (sentence_complexity + word_simplicity + technical_simplicity) / 3
            
            return simplicity_score
            
        except Exception as e:
            logger.error(f"Failed to calculate simplicity: {e}")
            return 0.5
    
    async def calculate_novelty(self, hypothesis: Dict, existing_knowledge: List[Dict]) -> float:
        """Calculate novelty by comparing to existing knowledge.
        
        Args:
            hypothesis: Hypothesis to evaluate
            existing_knowledge: List of existing hypotheses/theories
            
        Returns:
            Score between 0 and 1 (higher = more novel)
        """
        try:
            hypothesis_text = hypothesis.get('text', '')
            
            if not hypothesis_text or not existing_knowledge:
                return 0.7  # Default moderate novelty
            
            # Get embeddings for hypothesis and existing knowledge
            texts = [hypothesis_text] + [k.get('text', '') for k in existing_knowledge if k.get('text')]
            
            if len(texts) == 1:
                return 0.8  # High novelty if no existing knowledge
            
            embeddings = self.similarity_model.encode(texts)
            
            # Calculate similarity to each existing piece of knowledge
            hypothesis_embedding = embeddings[0:1]
            existing_embeddings = embeddings[1:]
            
            similarities = cosine_similarity(hypothesis_embedding, existing_embeddings)[0]
            
            # Novelty is inverse of maximum similarity
            max_similarity = np.max(similarities) if len(similarities) > 0 else 0
            novelty = 1 - max_similarity
            
            # Adjust based on conceptual uniqueness
            if self.classifier:
                try:
                    # Check if hypothesis introduces new concepts
                    result = await self._run_classifier(
                        hypothesis_text,
                        candidate_labels=['conventional idea', 'novel approach', 'revolutionary concept'],
                        hypothesis_template="This hypothesis represents a {}."
                    )
                    
                    novelty_scores = {
                        'revolutionary concept': 0.9,
                        'novel approach': 0.7,
                        'conventional idea': 0.3
                    }
                    
                    top_label = result['labels'][0]
                    classification_novelty = novelty_scores.get(top_label, 0.5)
                    
                    # Combine embedding and classification novelty
                    novelty = (novelty + classification_novelty) / 2
                    
                except Exception as e:
                    logger.warning(f"Novelty classification failed: {e}")
            
            return novelty
            
        except Exception as e:
            logger.error(f"Failed to calculate novelty: {e}")
            return 0.5
    
    async def _analyze_testability_language(self, text: str) -> float:
        """Analyze language patterns to determine testability.
        
        Args:
            text: Hypothesis text
            
        Returns:
            Base testability score
        """
        # Testable language patterns
        testable_patterns = [
            'if', 'then', 'when', 'causes', 'results in', 'leads to',
            'predicts', 'correlates', 'increases', 'decreases', 'affects'
        ]
        
        # Untestable language patterns
        untestable_patterns = [
            'might', 'possibly', 'perhaps', 'sometimes', 'occasionally',
            'in theory', 'conceptually', 'philosophically', 'arguably'
        ]
        
        text_lower = text.lower()
        
        # Count pattern matches
        testable_count = sum(1 for pattern in testable_patterns if pattern in text_lower)
        untestable_count = sum(1 for pattern in untestable_patterns if pattern in text_lower)
        
        # Calculate base score
        testability = 0.5  # Start neutral
        testability += min(testable_count * 0.1, 0.4)  # Up to +0.4 for testable patterns
        testability -= min(untestable_count * 0.1, 0.3)  # Up to -0.3 for untestable patterns
        
        # Use classification if available
        if self.classifier:
            try:
                result = await self._run_classifier(
                    text,
                    candidate_labels=['testable hypothesis', 'theoretical speculation', 'philosophical idea'],
                    hypothesis_template="This is a {}."
                )
                
                if result['labels'][0] == 'testable hypothesis':
                    testability += 0.2 * result['scores'][0]
                elif result['labels'][0] == 'philosophical idea':
                    testability -= 0.2 * result['scores'][0]
                    
            except Exception as e:
                logger.warning(f"Testability classification failed: {e}")
        
        return max(0, min(testability, 1.0))
    
    async def _run_classifier(self, text: str, candidate_labels: List[str], 
                            hypothesis_template: str = None) -> Dict:
        """Run zero-shot classification in thread pool to avoid blocking.
        
        Args:
            text: Text to classify
            candidate_labels: Possible labels
            hypothesis_template: Template for hypothesis
            
        Returns:
            Classification results
        """
        if not self.classifier:
            return {'labels': candidate_labels, 'scores': [1.0/len(candidate_labels)] * len(candidate_labels)}
        
        loop = asyncio.get_event_loop()
        
        if hypothesis_template:
            result = await loop.run_in_executor(
                None,
                lambda: self.classifier(text, candidate_labels, hypothesis_template=hypothesis_template)
            )
        else:
            result = await loop.run_in_executor(
                None,
                lambda: self.classifier(text, candidate_labels)
            )
        
        return result
    
    async def _run_qa(self, question: str, context: str) -> Dict:
        """Run question answering in thread pool to avoid blocking.
        
        Args:
            question: Question to answer
            context: Context to search for answer
            
        Returns:
            Answer dictionary with 'answer' and 'score' fields
        """
        if not self.qa_model:
            return None
        
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            lambda: self.qa_model(question=question, context=context)
        )
        
        return result
</file>

<file path="src/analytics/real_embedding_service.py">
"""Real embedding service using state-of-the-art transformer models."""

import asyncio
import logging
from typing import List, Dict, Any
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from PIL import Image
import torchvision.transforms as transforms

logger = logging.getLogger(__name__)


class RealEmbeddingService:
    """Real embedding service using transformer models for multi-modal embeddings."""
    
    def __init__(self, device: str = None):
        """Initialize embedding models.
        
        Args:
            device: Device to use ('cuda', 'cpu', or None for auto-detect)
        """
        # Auto-detect device if not specified
        if device is None:
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device
            
        logger.info(f"Initializing RealEmbeddingService on device: {self.device}")
        
        # Text embeddings using Sentence-BERT
        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)
        
        # Image embeddings using CLIP
        try:
            from transformers import CLIPProcessor, CLIPModel
            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            self.clip_model.to(self.device)
            self.clip_model.eval()
            self.has_clip = True
        except Exception as e:
            logger.warning(f"Failed to load CLIP model: {e}. Image embeddings will be limited.")
            self.has_clip = False
            
        # Structured data encoder
        self.structured_encoder = self._init_structured_encoder()
        
    async def generate_text_embeddings(self, texts: List[str]) -> np.ndarray:
        """Generate real text embeddings using Sentence-BERT.
        
        Args:
            texts: List of text strings to embed
            
        Returns:
            Array of embeddings with shape (n_texts, embedding_dim)
        """
        if not texts:
            return np.array([])
            
        try:
            # Run in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            embeddings = await loop.run_in_executor(
                None, 
                lambda: self.text_model.encode(
                    texts,
                    convert_to_numpy=True,
                    show_progress_bar=False,
                    batch_size=32
                )
            )
            
            logger.info(f"Generated {len(texts)} text embeddings with shape {embeddings.shape}")
            return embeddings
            
        except Exception as e:
            logger.error(f"Failed to generate text embeddings: {e}")
            # Return zero embeddings as fallback
            return np.zeros((len(texts), 384))
    
    async def generate_image_embeddings(self, image_paths: List[str]) -> np.ndarray:
        """Generate real image embeddings using CLIP.
        
        Args:
            image_paths: List of paths to image files
            
        Returns:
            Array of embeddings with shape (n_images, embedding_dim)
        """
        if not image_paths:
            return np.array([])
            
        if not self.has_clip:
            logger.warning("CLIP model not available, using fallback image embeddings")
            return await self._generate_fallback_image_embeddings(image_paths)
            
        embeddings = []
        
        for path in image_paths:
            try:
                # Load and process image
                image = Image.open(path).convert('RGB')
                inputs = self.clip_processor(images=image, return_tensors="pt")
                
                # Move inputs to device
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # Generate embedding
                with torch.no_grad():
                    outputs = self.clip_model.get_image_features(**inputs)
                    embedding = outputs.cpu().numpy().squeeze()
                    embeddings.append(embedding)
                    
            except Exception as e:
                logger.error(f"Failed to process image {path}: {e}")
                # Use zero vector for failed images
                embeddings.append(np.zeros(512))
        
        embeddings_array = np.array(embeddings)
        logger.info(f"Generated {len(image_paths)} image embeddings with shape {embeddings_array.shape}")
        return embeddings_array
    
    async def generate_structured_embeddings(self, structured_data: List[Dict[str, Any]]) -> np.ndarray:
        """Generate embeddings for structured data.
        
        Args:
            structured_data: List of dictionaries containing structured data
            
        Returns:
            Array of embeddings with shape (n_data, embedding_dim)
        """
        if not structured_data:
            return np.array([])
            
        embeddings = []
        
        for data in structured_data:
            try:
                # Extract features and create embedding
                feature_vector = self._extract_features(data)
                
                # Convert to tensor and process
                feature_tensor = torch.FloatTensor(feature_vector).to(self.device)
                
                with torch.no_grad():
                    embedding = self.structured_encoder(feature_tensor)
                    embeddings.append(embedding.cpu().numpy())
                    
            except Exception as e:
                logger.error(f"Failed to process structured data: {e}")
                # Use zero vector for failed data
                embeddings.append(np.zeros(256))
        
        embeddings_array = np.array(embeddings)
        logger.info(f"Generated {len(structured_data)} structured embeddings with shape {embeddings_array.shape}")
        return embeddings_array
    
    def _init_structured_encoder(self):
        """Initialize encoder for structured data."""
        import torch.nn as nn
        
        class StructuredEncoder(nn.Module):
            def __init__(self, input_dim=100, hidden_dim=128, output_dim=256):
                super().__init__()
                self.encoder = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.BatchNorm1d(hidden_dim),
                    nn.Dropout(0.1),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.ReLU(),
                    nn.BatchNorm1d(hidden_dim),
                    nn.Dropout(0.1),
                    nn.Linear(hidden_dim, output_dim),
                    nn.Tanh()
                )
                
            def forward(self, x):
                return self.encoder(x)
                
        encoder = StructuredEncoder()
        encoder.to(self.device)
        encoder.eval()
        return encoder
    
    def _extract_features(self, data: Dict[str, Any]) -> np.ndarray:
        """Extract numerical features from structured data.
        
        Args:
            data: Dictionary containing structured data
            
        Returns:
            Feature vector of fixed size
        """
        features = []
        
        # Extract numerical features
        for key, value in sorted(data.items()):
            if isinstance(value, (int, float)):
                features.append(float(value))
            elif isinstance(value, bool):
                features.append(1.0 if value else 0.0)
            elif isinstance(value, str):
                # Use string length and hash for basic encoding
                features.extend([
                    len(value) / 100.0,  # Normalized length
                    (hash(value) % 10000) / 10000.0  # Normalized hash
                ])
            elif isinstance(value, list):
                # Use list length
                features.append(len(value) / 100.0)
            elif isinstance(value, dict):
                # Recursively extract from nested dict (limited depth)
                if len(features) < 80:  # Leave room for other features
                    sub_features = self._extract_features(value)[:10]
                    features.extend(sub_features)
        
        # Pad or truncate to fixed size
        feature_array = np.array(features[:100], dtype=np.float32)
        if len(feature_array) < 100:
            feature_array = np.pad(
                feature_array, 
                (0, 100 - len(feature_array)),
                mode='constant',
                constant_values=0
            )
            
        return feature_array
    
    async def _generate_fallback_image_embeddings(self, image_paths: List[str]) -> np.ndarray:
        """Generate basic image embeddings when CLIP is not available.
        
        Uses basic image statistics as features.
        """
        embeddings = []
        
        for path in image_paths:
            try:
                image = Image.open(path).convert('RGB')
                # Resize to standard size
                image = image.resize((224, 224))
                
                # Convert to numpy array
                img_array = np.array(image)
                
                # Extract basic features
                features = []
                
                # Color histogram features
                for channel in range(3):
                    hist, _ = np.histogram(img_array[:, :, channel], bins=16, range=(0, 256))
                    features.extend(hist / hist.sum())  # Normalize
                
                # Basic statistics
                features.extend([
                    img_array.mean() / 255.0,
                    img_array.std() / 255.0,
                    (img_array > 128).mean(),  # Brightness ratio
                ])
                
                # Pad to 512 dimensions (CLIP embedding size)
                feature_array = np.array(features)
                if len(feature_array) < 512:
                    feature_array = np.pad(
                        feature_array,
                        (0, 512 - len(feature_array)),
                        mode='constant'
                    )
                
                embeddings.append(feature_array[:512])
                
            except Exception as e:
                logger.error(f"Failed to process image {path}: {e}")
                embeddings.append(np.zeros(512))
        
        return np.array(embeddings)
    
    def get_embedding_dimensions(self) -> Dict[str, int]:
        """Get the dimensionality of embeddings for each modality.
        
        Returns:
            Dictionary mapping modality to embedding dimension
        """
        return {
            'text': 384,  # Sentence-BERT all-MiniLM-L6-v2
            'image': 512,  # CLIP ViT-B/32
            'structured': 256  # Custom encoder
        }
</file>

<file path="src/analytics/real_llm_service.py">
"""Real LLM service using OpenAI or Anthropic APIs"""

import asyncio
import os
import json
import logging
from typing import List, Dict, Any, Optional
import openai
from anthropic import Anthropic

logger = logging.getLogger(__name__)


class RealLLMService:
    """Real LLM service using OpenAI or Anthropic APIs"""
    
    def __init__(self, provider: str = 'openai'):
        """Initialize LLM service with specified provider.
        
        Args:
            provider: LLM provider to use ('openai' or 'anthropic')
        """
        self.provider = provider
        
        if provider == 'openai':
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                logger.warning("OPENAI_API_KEY not found, LLM features will be limited")
                self.client = None
            else:
                self.client = openai.AsyncOpenAI(api_key=api_key)
                self.model = 'gpt-4-turbo-preview'
                
        elif provider == 'anthropic':
            api_key = os.getenv('ANTHROPIC_API_KEY')
            if not api_key:
                logger.warning("ANTHROPIC_API_KEY not found, LLM features will be limited")
                self.client = None
            else:
                self.client = Anthropic(api_key=api_key)
                self.model = 'claude-3-opus-20240229'
        else:
            raise ValueError(f"Unsupported provider: {provider}")
            
        logger.info(f"Initialized RealLLMService with provider: {provider}")
    
    async def generate_text(self, prompt: str, max_length: int = 500, 
                          temperature: float = 0.7) -> str:
        """Generate text using real LLM.
        
        Args:
            prompt: Input prompt for generation
            max_length: Maximum tokens to generate
            temperature: Sampling temperature (0-1)
            
        Returns:
            Generated text
        """
        if not self.client:
            logger.warning("LLM client not initialized, using fallback generation")
            return await self._fallback_generation(prompt, max_length)
            
        try:
            if self.provider == 'openai':
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a research hypothesis generator specializing in cross-modal analysis and knowledge synthesis."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_length,
                    temperature=temperature
                )
                return response.choices[0].message.content
                
            elif self.provider == 'anthropic':
                # Use synchronous call wrapped in executor
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: self.client.messages.create(
                        model=self.model,
                        messages=[
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=max_length,
                        temperature=temperature
                    )
                )
                return response.content[0].text
                
        except Exception as e:
            logger.error(f"Failed to generate text with {self.provider}: {e}")
            return await self._fallback_generation(prompt, max_length)
    
    async def generate_structured_hypotheses(self, prompt: str, max_hypotheses: int = 5,
                                           creativity_level: float = 0.7) -> List[Dict[str, Any]]:
        """Generate research hypotheses with structured output.
        
        Args:
            prompt: Context and requirements for hypothesis generation
            max_hypotheses: Maximum number of hypotheses to generate
            creativity_level: Temperature for generation (0-1)
            
        Returns:
            List of structured hypothesis dictionaries
        """
        # Enhance prompt to request structured output
        structured_prompt = f"""
{prompt}

Generate {max_hypotheses} research hypotheses in the following JSON format:
[
    {{
        "hypothesis": "The hypothesis statement",
        "confidence": 0.0-1.0,
        "novelty": 0.0-1.0,
        "testability": 0.0-1.0,
        "reasoning": "Brief explanation of the reasoning",
        "key_concepts": ["concept1", "concept2"],
        "evidence_requirements": ["required evidence type 1", "required evidence type 2"]
    }}
]

Ensure the output is valid JSON that can be parsed. Focus on novel, testable hypotheses that connect different modalities or reveal hidden patterns.
"""
        
        # Generate hypotheses
        raw_response = await self.generate_text(
            structured_prompt, 
            max_length=1500,  # More tokens for structured output
            temperature=creativity_level
        )
        
        # Parse structured response
        try:
            # Extract JSON from response
            json_start = raw_response.find('[')
            json_end = raw_response.rfind(']') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = raw_response[json_start:json_end]
                hypotheses_data = json.loads(json_str)
            else:
                # Try to parse the entire response
                hypotheses_data = json.loads(raw_response)
            
            # Structure hypotheses with IDs
            hypotheses = []
            for i, h_data in enumerate(hypotheses_data[:max_hypotheses]):
                hypothesis = {
                    'id': f'hypothesis_{i}',
                    'text': h_data.get('hypothesis', ''),
                    'confidence_score': float(h_data.get('confidence', 0.5)),
                    'novelty_score': float(h_data.get('novelty', 0.5)),
                    'testability_score': float(h_data.get('testability', 0.5)),
                    'evidence_support': [],
                    'reasoning_type': 'llm_generated',
                    'reasoning': h_data.get('reasoning', ''),
                    'key_concepts': h_data.get('key_concepts', []),
                    'evidence_requirements': h_data.get('evidence_requirements', [])
                }
                hypotheses.append(hypothesis)
                
            logger.info(f"Successfully generated {len(hypotheses)} structured hypotheses")
            return hypotheses
            
        except (json.JSONDecodeError, KeyError) as e:
            # Fallback to simple parsing if structured output fails
            logger.warning(f"Failed to parse structured LLM output: {e}")
            return await self._parse_unstructured_hypotheses(raw_response, max_hypotheses)
    
    async def _parse_unstructured_hypotheses(self, text: str, max_hypotheses: int) -> List[Dict[str, Any]]:
        """Parse hypotheses from unstructured text.
        
        Args:
            text: Unstructured text containing hypotheses
            max_hypotheses: Maximum number to extract
            
        Returns:
            List of structured hypothesis dictionaries
        """
        hypotheses = []
        
        # Split by common patterns
        lines = text.split('\n')
        hypothesis_lines = []
        
        for line in lines:
            line = line.strip()
            # Look for numbered lists or bullet points
            if (line and (
                line[0].isdigit() or 
                line.startswith('-') or 
                line.startswith('*') or
                line.startswith('•') or
                'hypothesis' in line.lower()
            )):
                # Clean up the line
                cleaned = line.lstrip('0123456789.-*•) ').strip()
                if cleaned and len(cleaned) > 20:  # Minimum length for valid hypothesis
                    hypothesis_lines.append(cleaned)
        
        # Create structured hypotheses from extracted lines
        for i, hyp_text in enumerate(hypothesis_lines[:max_hypotheses]):
            hypothesis = {
                'id': f'hypothesis_{i}',
                'text': hyp_text,
                'confidence_score': 0.7,  # Default scores
                'novelty_score': 0.6,
                'testability_score': 0.8,
                'evidence_support': [],
                'reasoning_type': 'llm_generated',
                'reasoning': 'Extracted from unstructured LLM output',
                'key_concepts': self._extract_concepts(hyp_text),
                'evidence_requirements': []
            }
            hypotheses.append(hypothesis)
        
        logger.info(f"Extracted {len(hypotheses)} hypotheses from unstructured text")
        return hypotheses
    
    def _extract_concepts(self, text: str) -> List[str]:
        """Extract key concepts from hypothesis text.
        
        Args:
            text: Hypothesis text
            
        Returns:
            List of key concepts
        """
        # Simple concept extraction based on capitalized words and domain terms
        concepts = []
        
        # Domain-specific terms
        domain_terms = {
            'cross-modal', 'entity', 'pattern', 'relationship', 'correlation',
            'analysis', 'synthesis', 'integration', 'network', 'cluster',
            'embedding', 'similarity', 'linkage', 'evidence', 'hypothesis'
        }
        
        words = text.lower().split()
        
        # Extract domain terms
        for word in words:
            cleaned = word.strip('.,;:!?()[]{}')
            if cleaned in domain_terms:
                concepts.append(cleaned)
        
        # Extract capitalized terms (potential entities)
        words = text.split()
        for word in words:
            if word[0].isupper() and len(word) > 3:
                cleaned = word.strip('.,;:!?()[]{}')
                if cleaned.lower() not in domain_terms:
                    concepts.append(cleaned)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_concepts = []
        for concept in concepts:
            if concept.lower() not in seen:
                seen.add(concept.lower())
                unique_concepts.append(concept)
        
        return unique_concepts[:10]  # Limit to 10 concepts
    
    async def _fallback_generation(self, prompt: str, max_length: int) -> str:
        """Fallback text generation when LLM is not available.
        
        Args:
            prompt: Input prompt
            max_length: Maximum length (ignored in fallback)
            
        Returns:
            Basic generated text
        """
        # Extract key information from prompt
        key_terms = []
        for word in prompt.lower().split():
            if len(word) > 5 and word not in ['research', 'hypothesis', 'generate', 'create']:
                key_terms.append(word)
        
        # Generate basic hypotheses
        templates = [
            f"Cross-modal analysis suggests a relationship between {key_terms[0] if key_terms else 'entities'} across different modalities.",
            f"The integration of multi-modal data reveals patterns in {key_terms[1] if len(key_terms) > 1 else 'knowledge networks'}.",
            f"Novel connections emerge when analyzing {key_terms[2] if len(key_terms) > 2 else 'cross-disciplinary relationships'}."
        ]
        
        return '\n'.join(f"{i+1}. {template}" for i, template in enumerate(templates[:3]))
</file>

<file path="src/analytics/real_percentile_ranker.py">
"""Real percentile ranking using statistical analysis and reference distributions"""

import logging
import numpy as np
from typing import Dict, List, Optional, Any
import asyncio
from scipy import stats
import networkx as nx

logger = logging.getLogger(__name__)


class RealPercentileRanker:
    """Calculate real percentile ranks using reference distributions and statistical methods"""
    
    def __init__(self, neo4j_manager):
        """Initialize with Neo4j manager for database access.
        
        Args:
            neo4j_manager: Neo4j manager instance for querying reference data
        """
        self.neo4j_manager = neo4j_manager
        self.reference_distributions = {}
        self.distribution_cache_valid = False
        
    async def load_reference_distributions(self):
        """Load reference distributions from database"""
        logger.info("Loading reference distributions from database")
        
        metrics = ['h_index', 'citation_velocity', 'cross_disciplinary_impact', 
                  'citation_count', 'impact_factor', 'collaboration_count']
        
        for metric in metrics:
            try:
                # Query database for metric distribution
                query = f"""
                MATCH (e:Entity)
                WHERE e.{metric} IS NOT NULL
                RETURN e.{metric} as value
                ORDER BY value
                """
                
                result = await self.neo4j_manager.execute_read_query(query)
                values = [r['value'] for r in result if r['value'] is not None]
                
                if values and len(values) >= 10:  # Need minimum sample size
                    self.reference_distributions[metric] = np.array(values)
                    logger.info(f"Loaded {len(values)} reference values for {metric}")
                else:
                    # If insufficient real data, create synthetic distribution
                    logger.warning(f"Insufficient data for {metric}, using synthetic distribution")
                    self.reference_distributions[metric] = self._create_synthetic_distribution(metric)
                    
            except Exception as e:
                logger.error(f"Failed to load distribution for {metric}: {e}")
                self.reference_distributions[metric] = self._create_synthetic_distribution(metric)
        
        self.distribution_cache_valid = True
        logger.info(f"Loaded reference distributions for {len(self.reference_distributions)} metrics")
    
    def _create_synthetic_distribution(self, metric: str, size: int = 10000) -> np.ndarray:
        """Create synthetic distribution based on typical patterns.
        
        Args:
            metric: Metric name
            size: Number of samples to generate
            
        Returns:
            Synthetic distribution array
        """
        np.random.seed(42)  # Reproducible distributions
        
        if metric == 'h_index':
            # H-index typically follows power law / exponential decay
            # Most researchers have low h-index, few have very high
            values = np.random.exponential(scale=8, size=size)
            values = np.clip(values, 0, 100)  # Cap at realistic maximum
            
        elif metric == 'citation_velocity':
            # Citation velocity often log-normal
            # Most papers get few citations per year, some get many
            values = np.random.lognormal(mean=2.0, sigma=1.5, size=size)
            values = np.clip(values, 0, 500)
            
        elif metric == 'cross_disciplinary_impact':
            # Usually beta distribution (bounded 0-1)
            # Most work is within discipline, some is highly cross-disciplinary
            values = np.random.beta(a=2, b=5, size=size)
            
        elif metric == 'citation_count':
            # Heavy-tailed distribution (power law)
            # Most papers have few citations, very few have many
            values = np.random.pareto(a=1.5, size=size) * 10
            values = np.clip(values, 0, 10000)
            
        elif metric == 'impact_factor':
            # Log-normal distribution
            # Most journals have moderate IF, few have very high
            values = np.random.lognormal(mean=1.0, sigma=0.8, size=size)
            values = np.clip(values, 0.1, 50)
            
        elif metric == 'collaboration_count':
            # Poisson or negative binomial
            # Most researchers have moderate collaborations
            values = np.random.negative_binomial(n=10, p=0.3, size=size)
            
        else:
            # Generic beta distribution for unknown metrics
            values = np.random.beta(a=2, b=5, size=size)
        
        return np.sort(values)
    
    async def calculate_percentile_rank(self, score: float, metric: str) -> float:
        """Calculate real percentile rank for a score.
        
        Args:
            score: The score to rank
            metric: The metric name
            
        Returns:
            Percentile rank (0-100)
        """
        # Load distributions if not cached
        if not self.distribution_cache_valid:
            await self.load_reference_distributions()
        
        if metric not in self.reference_distributions:
            logger.warning(f"No reference distribution for {metric}, using conservative estimate")
            return 50.0  # Conservative middle ranking
        
        distribution = self.reference_distributions[metric]
        
        # Calculate percentile using empirical CDF
        percentile = stats.percentileofscore(distribution, score, kind='rank')
        
        # Apply smoothing for extreme values
        if percentile > 99:
            # Log scale for top 1% to differentiate extreme outliers
            excess = score / np.percentile(distribution, 99)
            percentile = min(99 + np.log10(excess), 100)
        elif percentile < 1:
            # Similar treatment for bottom 1%
            ratio = score / (np.percentile(distribution, 1) + 1e-6)
            percentile = max(ratio, 0)
        
        return float(percentile)
    
    async def calculate_percentile_ranks_batch(self, scores: Dict[str, float]) -> Dict[str, float]:
        """Calculate percentile ranks for multiple metrics.
        
        Args:
            scores: Dictionary mapping metric names to scores
            
        Returns:
            Dictionary mapping metric names to percentile ranks
        """
        percentiles = {}
        
        for metric, score in scores.items():
            if score is not None:
                percentiles[metric] = await self.calculate_percentile_rank(score, metric)
            else:
                percentiles[metric] = 0.0
        
        return percentiles
    
    async def calculate_collaboration_network_centrality(self, entity_id: str) -> float:
        """Calculate real network centrality in collaboration network.
        
        Args:
            entity_id: Entity ID to analyze
            
        Returns:
            Centrality score (0-1)
        """
        try:
            # Query collaboration network
            query = """
            // Get direct collaborators
            MATCH (e:Entity {id: $entity_id})-[:COLLABORATES_WITH]-(collaborator:Entity)
            WITH e, collect(DISTINCT collaborator) as direct_collaborators
            
            // Get extended network (2 hops)
            MATCH (e)-[:COLLABORATES_WITH*1..2]-(extended:Entity)
            WITH e, direct_collaborators, collect(DISTINCT extended) as extended_network
            
            // Get all collaboration edges in the subgraph
            UNWIND extended_network as n1
            MATCH (n1)-[r:COLLABORATES_WITH]-(n2:Entity)
            WHERE n2 IN extended_network
            RETURN 
                e.id as center_id,
                [c IN direct_collaborators | c.id] as direct_ids,
                [n IN extended_network | n.id] as network_ids,
                collect(DISTINCT {source: n1.id, target: n2.id}) as edges
            """
            
            result = await self.neo4j_manager.execute_read_query(
                query, {'entity_id': entity_id}
            )
            
            if not result:
                return 0.0
            
            data = result[0]
            network_ids = data.get('network_ids', [])
            edges = data.get('edges', [])
            
            if len(network_ids) < 2:
                return 0.0
            
            # Build NetworkX graph
            G = nx.Graph()
            
            # Add nodes
            for node_id in network_ids:
                G.add_node(node_id)
            
            # Add edges
            for edge in edges:
                if edge['source'] in network_ids and edge['target'] in network_ids:
                    G.add_edge(edge['source'], edge['target'])
            
            # Ensure entity is in graph
            if entity_id not in G:
                G.add_node(entity_id)
            
            # Calculate various centrality measures
            centrality_scores = []
            
            # Degree centrality
            degree_cent = nx.degree_centrality(G)
            centrality_scores.append(degree_cent.get(entity_id, 0))
            
            # Betweenness centrality
            if G.number_of_nodes() > 2:
                between_cent = nx.betweenness_centrality(G)
                centrality_scores.append(between_cent.get(entity_id, 0))
            
            # Closeness centrality
            if nx.is_connected(G):
                close_cent = nx.closeness_centrality(G)
                centrality_scores.append(close_cent.get(entity_id, 0))
            else:
                # Use subgraph containing the entity
                for component in nx.connected_components(G):
                    if entity_id in component:
                        subgraph = G.subgraph(component)
                        close_cent = nx.closeness_centrality(subgraph)
                        centrality_scores.append(close_cent.get(entity_id, 0))
                        break
            
            # Eigenvector centrality (if possible)
            try:
                if G.number_of_nodes() > 2 and G.number_of_edges() > 0:
                    eigen_cent = nx.eigenvector_centrality_numpy(G, max_iter=100)
                    centrality_scores.append(eigen_cent.get(entity_id, 0))
            except:
                pass  # Skip if computation fails
            
            # Return average of available centrality measures
            if centrality_scores:
                return float(np.mean(centrality_scores))
            else:
                return 0.0
                
        except Exception as e:
            logger.error(f"Failed to calculate collaboration centrality: {e}")
            return 0.0
    
    async def get_field_statistics(self, field: str) -> Dict[str, float]:
        """Get statistical summary for a field.
        
        Args:
            field: Academic field/discipline
            
        Returns:
            Dictionary with statistical measures
        """
        try:
            query = """
            MATCH (e:Entity)
            WHERE e.field = $field AND e.h_index IS NOT NULL
            WITH e.h_index as h_index, e.citation_count as citations
            RETURN 
                avg(h_index) as mean_h_index,
                percentileCont(h_index, 0.5) as median_h_index,
                percentileCont(h_index, 0.25) as q1_h_index,
                percentileCont(h_index, 0.75) as q3_h_index,
                avg(citations) as mean_citations,
                percentileCont(citations, 0.5) as median_citations
            """
            
            result = await self.neo4j_manager.execute_read_query(
                query, {'field': field}
            )
            
            if result:
                return result[0]
            else:
                return {
                    'mean_h_index': 10.0,
                    'median_h_index': 8.0,
                    'q1_h_index': 4.0,
                    'q3_h_index': 15.0,
                    'mean_citations': 100.0,
                    'median_citations': 50.0
                }
                
        except Exception as e:
            logger.error(f"Failed to get field statistics: {e}")
            return {}
    
    def calculate_relative_impact(self, score: float, field_stats: Dict[str, float], 
                                metric: str = 'h_index') -> float:
        """Calculate impact relative to field norms.
        
        Args:
            score: Individual's score
            field_stats: Field statistics
            metric: Metric to compare
            
        Returns:
            Relative impact score
        """
        field_median = field_stats.get(f'median_{metric}', 10.0)
        field_mean = field_stats.get(f'mean_{metric}', 10.0)
        
        if field_median > 0:
            # Calculate z-score relative to field
            relative_score = score / field_median
            
            # Apply log scaling for extreme values
            if relative_score > 3:
                relative_score = 3 + np.log(relative_score - 2)
            
            return relative_score
        else:
            return 1.0
</file>

<file path="src/analytics/theory_knowledge_base.py">
"""Theory knowledge base for identifying and applying relevant theories"""

import logging
from typing import List, Dict, Any, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import asyncio

logger = logging.getLogger(__name__)


class TheoryKnowledgeBase:
    """Real theory identification using knowledge base and semantic search"""
    
    def __init__(self, neo4j_manager):
        """Initialize with Neo4j manager for database access.
        
        Args:
            neo4j_manager: Neo4j manager instance
        """
        self.neo4j_manager = neo4j_manager
        self.theory_embeddings = {}
        self.theory_model = SentenceTransformer('all-MiniLM-L6-v2')
        self._theories_cached = False
        
    async def identify_applicable_theories(self, evidence_base: Dict) -> List[Dict[str, Any]]:
        """Identify theories from knowledge base that apply to evidence.
        
        Args:
            evidence_base: Dictionary containing evidence entities and relationships
            
        Returns:
            List of applicable theories with scores
        """
        try:
            # Extract key concepts from evidence
            concepts = await self._extract_concepts(evidence_base)
            
            # First try: Query theory database
            theories = await self._query_theory_database(concepts)
            
            # If no theories found, try semantic search
            if not theories:
                theories = await self._search_theories_by_similarity(evidence_base)
            
            # If still no theories, use domain-specific fallbacks
            if not theories:
                theories = await self._get_domain_specific_theories(evidence_base)
            
            # Score applicability of each theory
            scored_theories = []
            for theory in theories:
                score = await self._score_theory_applicability(theory, evidence_base)
                theory_dict = {
                    'name': theory.get('name', 'Unknown Theory'),
                    'applicability': score,
                    'description': theory.get('description', ''),
                    'domain': theory.get('domain', 'general'),
                    'conditions': theory.get('conditions', []),
                    'key_concepts': theory.get('keywords', [])
                }
                scored_theories.append(theory_dict)
            
            # Sort by applicability score
            scored_theories.sort(key=lambda x: x['applicability'], reverse=True)
            
            # Return top theories
            return scored_theories[:5]
            
        except Exception as e:
            logger.error(f"Failed to identify applicable theories: {e}")
            return await self._get_default_theories()
    
    async def _extract_concepts(self, evidence_base: Dict) -> List[str]:
        """Extract key concepts from evidence base.
        
        Args:
            evidence_base: Evidence dictionary
            
        Returns:
            List of key concepts
        """
        concepts = set()
        
        # Extract from entities
        for entity in evidence_base.get('entities', []):
            # Entity labels
            if 'labels' in entity:
                concepts.update(entity['labels'])
            
            # Entity properties that might contain concepts
            for key in ['type', 'category', 'field', 'domain', 'topic']:
                if key in entity and entity[key]:
                    concepts.add(str(entity[key]))
        
        # Extract from relationships
        for rel in evidence_base.get('relationships', []):
            if 'type' in rel:
                # Convert relationship type to concept
                rel_type = rel['type'].replace('_', ' ').lower()
                concepts.add(rel_type)
        
        # Extract from modalities if present
        if 'modalities' in evidence_base:
            concepts.update(evidence_base['modalities'])
        
        return list(concepts)
    
    async def _query_theory_database(self, concepts: List[str]) -> List[Dict]:
        """Query theory database for matching theories.
        
        Args:
            concepts: List of concepts to match
            
        Returns:
            List of matching theories
        """
        try:
            # Query for theories matching concepts
            query = """
            MATCH (t:Theory)
            WHERE any(concept IN $concepts WHERE 
                toLower(t.name) CONTAINS toLower(concept) OR
                any(keyword IN t.keywords WHERE toLower(keyword) CONTAINS toLower(concept)) OR
                toLower(t.domain) CONTAINS toLower(concept) OR
                toLower(t.description) CONTAINS toLower(concept)
            )
            RETURN t.name as name, 
                   t.description as description,
                   t.keywords as keywords, 
                   t.domain as domain,
                   t.applicability_conditions as conditions,
                   t.confidence_score as base_score
            ORDER BY t.citation_count DESC
            LIMIT 20
            """
            
            result = await self.neo4j_manager.execute_read_query(
                query, {'concepts': concepts}
            )
            
            if result:
                return result
            
            # Try broader search without concept filtering
            fallback_query = """
            MATCH (t:Theory)
            WHERE t.domain IN ['knowledge synthesis', 'cross-modal analysis', 
                              'network science', 'information theory', 'systems theory']
            RETURN t.name as name, 
                   t.description as description,
                   t.keywords as keywords, 
                   t.domain as domain,
                   t.applicability_conditions as conditions,
                   t.confidence_score as base_score
            ORDER BY t.citation_count DESC
            LIMIT 10
            """
            
            return await self.neo4j_manager.execute_read_query(fallback_query)
            
        except Exception as e:
            logger.warning(f"Theory database query failed: {e}")
            return []
    
    async def _search_theories_by_similarity(self, evidence_base: Dict) -> List[Dict]:
        """Search for theories using semantic similarity.
        
        Args:
            evidence_base: Evidence to match against
            
        Returns:
            List of similar theories
        """
        try:
            # Create evidence description
            evidence_desc = await self._create_evidence_description(evidence_base)
            
            if not evidence_desc:
                return []
            
            # Get all theories with embeddings
            if not self._theories_cached:
                await self._cache_theory_embeddings()
            
            if not self.theory_embeddings:
                return []
            
            # Compute similarity
            evidence_embedding = self.theory_model.encode([evidence_desc])
            
            similarities = []
            for theory_id, theory_data in self.theory_embeddings.items():
                similarity = cosine_similarity(
                    evidence_embedding,
                    theory_data['embedding'].reshape(1, -1)
                )[0][0]
                
                similarities.append({
                    'theory': theory_data['theory'],
                    'similarity': similarity
                })
            
            # Sort by similarity
            similarities.sort(key=lambda x: x['similarity'], reverse=True)
            
            # Return top matches
            return [item['theory'] for item in similarities[:10]]
            
        except Exception as e:
            logger.error(f"Semantic theory search failed: {e}")
            return []
    
    async def _cache_theory_embeddings(self):
        """Cache embeddings for all theories in database."""
        try:
            query = """
            MATCH (t:Theory)
            RETURN t.id as id, 
                   t.name as name,
                   t.description as description,
                   t.keywords as keywords,
                   t.domain as domain
            """
            
            theories = await self.neo4j_manager.execute_read_query(query)
            
            for theory in theories:
                # Create theory text representation
                theory_text = f"{theory['name']} {theory.get('description', '')} {' '.join(theory.get('keywords', []))}"
                
                # Generate embedding
                embedding = self.theory_model.encode(theory_text)
                
                self.theory_embeddings[theory['id']] = {
                    'embedding': embedding,
                    'theory': theory
                }
            
            self._theories_cached = True
            logger.info(f"Cached embeddings for {len(self.theory_embeddings)} theories")
            
        except Exception as e:
            logger.error(f"Failed to cache theory embeddings: {e}")
    
    async def _create_evidence_description(self, evidence_base: Dict) -> str:
        """Create textual description of evidence for similarity matching.
        
        Args:
            evidence_base: Evidence dictionary
            
        Returns:
            Text description
        """
        parts = []
        
        # Describe entities
        entity_types = set()
        for entity in evidence_base.get('entities', [])[:10]:  # Limit to prevent too long descriptions
            if 'type' in entity:
                entity_types.add(entity['type'])
        
        if entity_types:
            parts.append(f"Entities of types: {', '.join(entity_types)}")
        
        # Describe relationships
        rel_types = set()
        for rel in evidence_base.get('relationships', [])[:10]:
            if 'type' in rel:
                rel_types.add(rel['type'].replace('_', ' '))
        
        if rel_types:
            parts.append(f"Relationships: {', '.join(rel_types)}")
        
        # Add modalities
        if 'modalities' in evidence_base:
            parts.append(f"Modalities: {', '.join(evidence_base['modalities'])}")
        
        # Add any patterns or anomalies
        if 'patterns' in evidence_base:
            parts.append(f"Patterns observed: {len(evidence_base['patterns'])}")
        
        return ' '.join(parts)
    
    async def _score_theory_applicability(self, theory: Dict, evidence_base: Dict) -> float:
        """Score how applicable a theory is to the evidence.
        
        Args:
            theory: Theory dictionary
            evidence_base: Evidence dictionary
            
        Returns:
            Applicability score (0-1)
        """
        score = 0.0
        
        # Base score from theory confidence
        base_score = theory.get('base_score', 0.5)
        score += base_score * 0.3
        
        # Check domain match
        theory_domain = theory.get('domain', '').lower()
        evidence_concepts = await self._extract_concepts(evidence_base)
        
        domain_match = any(concept.lower() in theory_domain for concept in evidence_concepts)
        if domain_match:
            score += 0.2
        
        # Check keyword overlap
        theory_keywords = [kw.lower() for kw in theory.get('keywords', [])]
        keyword_overlap = sum(1 for concept in evidence_concepts 
                            if any(kw in concept.lower() or concept.lower() in kw 
                                  for kw in theory_keywords))
        
        keyword_score = min(keyword_overlap / max(len(theory_keywords), 1), 1.0) * 0.3
        score += keyword_score
        
        # Check applicability conditions
        conditions = theory.get('conditions', [])
        if conditions:
            # Simple check - in practice, would evaluate conditions
            condition_score = 0.2  # Default partial match
            score += condition_score
        else:
            score += 0.2  # No conditions = generally applicable
        
        return min(score, 1.0)
    
    async def _get_domain_specific_theories(self, evidence_base: Dict) -> List[Dict]:
        """Get domain-specific theories based on evidence characteristics.
        
        Args:
            evidence_base: Evidence dictionary
            
        Returns:
            List of domain-specific theories
        """
        theories = []
        
        # Analyze evidence characteristics
        has_network = len(evidence_base.get('relationships', [])) > 0
        has_multi_modal = len(evidence_base.get('modalities', [])) > 1
        has_temporal = any('timestamp' in e or 'date' in e 
                          for e in evidence_base.get('entities', []))
        
        # Add relevant theories based on characteristics
        if has_network:
            theories.append({
                'name': 'Network Theory',
                'description': 'Analyzes patterns and dynamics in networked systems',
                'domain': 'network science',
                'keywords': ['network', 'graph', 'connectivity', 'centrality', 'clustering'],
                'conditions': ['presence of relational data'],
                'base_score': 0.8
            })
            
            theories.append({
                'name': 'Small World Theory',
                'description': 'Studies networks with high clustering and short path lengths',
                'domain': 'network science',
                'keywords': ['small world', 'six degrees', 'clustering coefficient'],
                'conditions': ['network with clustering patterns'],
                'base_score': 0.6
            })
        
        if has_multi_modal:
            theories.append({
                'name': 'Information Integration Theory',
                'description': 'Explains how information from multiple sources is combined',
                'domain': 'cognitive science',
                'keywords': ['integration', 'multi-modal', 'fusion', 'synthesis'],
                'conditions': ['multiple data modalities'],
                'base_score': 0.7
            })
            
            theories.append({
                'name': 'Multimodal Learning Theory',
                'description': 'Framework for learning from heterogeneous data sources',
                'domain': 'machine learning',
                'keywords': ['multimodal', 'cross-modal', 'heterogeneous', 'fusion'],
                'conditions': ['diverse data types'],
                'base_score': 0.65
            })
        
        if has_temporal:
            theories.append({
                'name': 'Temporal Network Theory',
                'description': 'Analyzes time-varying networks and dynamic processes',
                'domain': 'temporal networks',
                'keywords': ['temporal', 'dynamic', 'evolution', 'time-series'],
                'conditions': ['temporal data available'],
                'base_score': 0.7
            })
        
        # Always include general theories
        theories.extend([
            {
                'name': 'Systems Theory',
                'description': 'Holistic approach to analyzing complex interconnected systems',
                'domain': 'systems science',
                'keywords': ['system', 'emergence', 'complexity', 'holistic'],
                'conditions': [],
                'base_score': 0.5
            },
            {
                'name': 'Knowledge Graph Theory',
                'description': 'Framework for representing and reasoning with structured knowledge',
                'domain': 'knowledge representation',
                'keywords': ['knowledge graph', 'ontology', 'semantic', 'reasoning'],
                'conditions': ['structured entity data'],
                'base_score': 0.6
            }
        ])
        
        return theories
    
    async def _get_default_theories(self) -> List[Dict[str, Any]]:
        """Get default theories when other methods fail.
        
        Returns:
            List of basic applicable theories
        """
        return [
            {
                'name': 'General Systems Theory',
                'applicability': 0.6,
                'description': 'Provides a framework for analyzing complex systems',
                'domain': 'systems science',
                'conditions': [],
                'key_concepts': ['system', 'interaction', 'emergence']
            },
            {
                'name': 'Information Theory',
                'applicability': 0.5,
                'description': 'Mathematical framework for information processing and transmission',
                'domain': 'information science',
                'conditions': [],
                'key_concepts': ['information', 'entropy', 'communication']
            }
        ]
</file>

</files>
